"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[22747],{19395:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"ML General/Principle Component Analysis - PCA","title":"Principle Component Analysis - PCA","description":"Definition of PCA","source":"@site/docs/05. ML General/34.Principle Component Analysis - PCA.md","sourceDirName":"05. ML General","slug":"/p/75f6131c-e9cb-4073-b06a-407ca559b982","permalink":"/notes/docs/p/75f6131c-e9cb-4073-b06a-407ca559b982","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/05. ML General/34.Principle Component Analysis - PCA.md","tags":[],"version":"current","sidebarPosition":34,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/75f6131c-e9cb-4073-b06a-407ca559b982","slug":"/p/75f6131c-e9cb-4073-b06a-407ca559b982"},"sidebar":"tutorialSidebar","previous":{"title":"SoftMax and Cross-Entropy Loss Function","permalink":"/notes/docs/p/83bf409d-c62d-436f-8814-d1bcd2c739ff"},"next":{"title":"Learning to Rank","permalink":"/notes/docs/p/3950ea5b-4dcb-4513-804d-55d0162005c8"}}');var o=i(74848),a=i(28453);const s={created_at:"2025-11-02",page_link:"/p/75f6131c-e9cb-4073-b06a-407ca559b982",slug:"/p/75f6131c-e9cb-4073-b06a-407ca559b982"},r=void 0,c={},l=[{value:"Definition of PCA",id:"definition-of-pca",level:2},{value:"A Summary of the PCA Approach",id:"a-summary-of-the-pca-approach",level:2},{value:"Eigenvectors, Eigenvalues",id:"eigenvectors-eigenvalues",level:2},{value:"eigenvalue decomposition",id:"eigenvalue-decomposition",level:2},{value:"eigendecomposition v.s. SVD",id:"eigendecomposition-vs-svd",level:2}];function d(e){const n={a:"a",h2:"h2",img:"img",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"definition-of-pca",children:"Definition of PCA"}),"\n",(0,o.jsx)(n.p,{children:"A method to reduce dimension (reduce the amount of features) by finding the top few axis that capture the maximum amount of variance in the data.  The top few new axis are found by performing eigendecomposition of the covariance matrix, and find the eigenvectors that corresponds to the highest few eigenvalues."}),"\n",(0,o.jsx)(n.h2,{id:"a-summary-of-the-pca-approach",children:"A Summary of the PCA Approach"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Standardize the data."}),"\n",(0,o.jsx)(n.li,{children:"Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Value Decomposition."}),"\n",(0,o.jsx)(n.li,{children:"Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (k\u2264d)."}),"\n",(0,o.jsx)(n.li,{children:"Construct the projection matrix W from the selected k eigenvectors."}),"\n",(0,o.jsx)(n.li,{children:"Transform the original dataset X via W to obtain a k-dimensional feature subspace Y."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"eigenvectors-eigenvalues",children:"Eigenvectors, Eigenvalues"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://guzintamath.com/textsavvy/2018/05/26/eigenvalues-and-eigenvectors/",children:"source"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209300052071.png",alt:""})}),"\n",(0,o.jsx)(n.p,{children:"\u03bb \u5c31\u662f eigenvalue., [r1, r2] \u5c31\u662feigenvector"}),"\n",(0,o.jsx)(n.h2,{id:"eigenvalue-decomposition",children:"eigenvalue decomposition"}),"\n",(0,o.jsx)(n.p,{children:"eigendecomposition, or eigenvalue decomposition, is the factorization of a matrix A, using the function Ax = \u03bbx, where \u03bb is eigenvalue and x is eigen vector."}),"\n",(0,o.jsxs)(n.p,{children:["\u518d\u4e3e\u53e6\u5916\u4e00\u4e2a\u4f8b\u5b50\uff1a\n",(0,o.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209300057131.png",alt:""}),"\nwe calculate the eigenvalues to be \u03bb1=\u22122 and \u03bb2=\u22121. And the corresponding eigenvectors are of the form (1,\u22122) and (\u22121,1), respectively."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209300058950.png",alt:""})}),"\n",(0,o.jsx)(n.h2,{id:"eigendecomposition-vs-svd",children:"eigendecomposition v.s. SVD"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.a,{href:"https://math.stackexchange.com/questions/320220/intuitively-what-is-the-difference-between-eigendecomposition-and-singular-valu",children:"source"})}),"\n",(0,o.jsx)(n.p,{children:"Consider the eigendecomposition \ud835\udc34=\ud835\udc43\ud835\udc37\ud835\udc43\u22121 and SVD \ud835\udc34=\ud835\udc48\u03a3\ud835\udc49\u2217"}),"\n",(0,o.jsx)(n.p,{children:"Some key differences are as follows:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"The vectors in the eigendecomposition matrix \ud835\udc43 are not necessarily orthogonal, so the change of basis isn't a simple rotation. On the other hand, the vectors in the matrices \ud835\udc48 and \ud835\udc49-   in the SVD are orthonormal, so they do represent rotations (and possibly flips)."}),"\n",(0,o.jsx)(n.li,{children:"In the SVD, the nondiagonal matrices \ud835\udc48 and \ud835\udc49 are not necessairily the inverse of one another. They are usually not related to each other at all. In the eigendecomposition the nondiagonal matrices \ud835\udc43 and \ud835\udc43\u22121-   are inverses of each other."}),"\n",(0,o.jsx)(n.li,{children:"In the SVD the entries in the diagonal matrix \u03a3 are all real and nonnegative. In the eigendecomposition, the entries of \ud835\udc37-   can be any complex number - negative, positive, imaginary, whatever."}),"\n",(0,o.jsx)(n.li,{children:"The SVD always exists for any sort of rectangular or square matrix, whereas the eigendecomposition can only exists for square matrices, and even among square matrices sometimes it doesn't exist."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>r});var t=i(96540);const o={},a=t.createContext(o);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);