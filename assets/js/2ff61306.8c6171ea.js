"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[93989],{3798:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"Zero To Hero/Makemore 4 - Backpropagation Ninja","title":"5. Makemore 4 - Backpropagation Ninja","description":"We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd\'s loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.","source":"@site/docs/04. Zero To Hero/06. Makemore 4 - Backpropagation Ninja.md","sourceDirName":"04. Zero To Hero","slug":"/p/b52964bd-d3bf-4f0c-bce8-750a995dfb2f","permalink":"/notes/docs/p/b52964bd-d3bf-4f0c-bce8-750a995dfb2f","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/04. Zero To Hero/06. Makemore 4 - Backpropagation Ninja.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/b52964bd-d3bf-4f0c-bce8-750a995dfb2f","slug":"/p/b52964bd-d3bf-4f0c-bce8-750a995dfb2f"},"sidebar":"tutorialSidebar","previous":{"title":"Makemore 3 - Activations & Gradients, BatchNorm","permalink":"/notes/docs/p/7d5e92b9-6fc9-48a9-bc43-0ec41f622dc2"},"next":{"title":"Makemore 5 - WaveNet","permalink":"/notes/docs/p/f6874608-fc80-4c3b-98aa-4ac3eb261e37"}}');var r=t(74848),o=t(28453);const i={created_at:"2025-11-02",page_link:"/p/b52964bd-d3bf-4f0c-bce8-750a995dfb2f",slug:"/p/b52964bd-d3bf-4f0c-bce8-750a995dfb2f"},s="5. Makemore 4 - Backpropagation Ninja",d={},c=[];function l(e){const n={a:"a",code:"code",h1:"h1",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"5-makemore-4---backpropagation-ninja",children:"5. Makemore 4 - Backpropagation Ninja"})}),"\n",(0,r.jsx)(n.p,{children:"We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks."}),"\n",(0,r.jsx)(n.p,{children:"!!!!!!!!!!!!\nI recommend you work through the exercise yourself but work with it in tandem and whenever you are stuck unpause the video and see me give away the answer. This video is not super intended to be simply watched. The exercise is here:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbkxrMS1UdzJTX3c1bHlMZzFvaGE2a0dkUGlaQXxBQ3Jtc0tucVRxVDRobTREVVl4VlBnRVhUOWdELThQXzQ0V0ZrM1ptRWJ2TVlkSmpTSTZUQmt1MHRXSlRWZ0NMT3B0bTlVTmpNNkwwVjU5eHVYQnNWUWxSM19TU1dqcXQxVFlVX3pHMDVqb2U4c2ViWjdSOXdIUQ&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-%3Fusp%3Dsharing&v=q8SA3rM6ckI",children:"https://colab.research.google.com/dri..."})}),"\n",(0,r.jsx)(n.p,{children:"!!!!!!!!!!!!"}),"\n",(0,r.jsx)(n.p,{children:"Links:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"makemore on github:"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa0Y2VXowWGNNdzc1OHo1RlFRV2phbmZQM1JGUXxBQ3Jtc0ttaFRmdVFjU1JzaHhLUEl0QXpULWVyQTZDaEpnb3U0eXFsNk9sUFRveHAxRm9BZ1NDU2hWTXlmT2dveXpfTkpmZ2U1WFZQa1BUcVRZRHBkM1Vrd01jdGJWR3ZyNVJwcTJNOXJjX1N3WTFmZVdyNHhvMA&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fmakemore&v=q8SA3rM6ckI",children:"https://github.com/karpathy/makemore"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"jupyter notebook I built in this video:"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbnJ2eFRmdFdoRlA0M0VrNWx6YVdqMnR1dzdOUXxBQ3Jtc0trN3FJdF9FQ0VGSFYyREg2ZV9Sam1YRjdWZ2w1SE5hT2E4SF9tc0FITC10b2ZMV0JqeGM4dzBaNXhwVlJUSlhnc0ZQZlIzSE8ydWtpVjZqZGJKcEhNeEp2SkFHa0ZqOUdHNkFueGM2T184T2RSUXdqNA&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fnn-zero-to-hero%2Fblob%2Fmaster%2Flectures%2Fmakemore%2Fmakemore_part4_backprop.ipynb&v=q8SA3rM6ckI",children:"https://github.com/karpathy/nn-zero-t..."})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"collab notebook:"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbENPclpqNFpkQ082NUhsbUN0SGpLUGFOTE5id3xBQ3Jtc0trZHhPai1nNGFxNXFVeFF4T0tlbTE0RmRVTU9odERHTkVWQmFyMXZaNnpLSHhKLTVyY09uam9tOHJQVkRYWXhNRVZkb1BDdjl6OEdtS2hYT0FqQzh1QWF5aEVNbm1fUGJjRjZCYWwxYVYzQ09BOVM1QQ&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1WV2oi2fh9XXyldh02wupFQX0wh5ZC-z-%3Fusp%3Dsharing&v=q8SA3rM6ckI",children:"https://colab.research.google.com/dri..."})}),"\n",(0,r.jsx)(n.p,{children:"Supplementary links:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Yes you should understand backprop:"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["\xa0",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbndfbDMxeG5VVlBOb3pzdFUwZy1teEIzdEJ1d3xBQ3Jtc0tsaHh3TWM2anJXelZkWGI2M0hoeUNxSWJqRTRqemJ6WWpydEUxRk4yRGl0Qlh1VXVzbWdyS1AtSVl3VjlvVXc5dmt4Q01JRmd1RnZkUXU1OWt6RTFYUjJRcUhHbW5kdkR4M3E3WmNMZGRLOUFsdTJpSQ&q=https%3A%2F%2Fkarpathy.medium.com%2Fyes-you-should-understand-backprop-e2f06eab496b&v=q8SA3rM6ckI",children:"/\xa0yes-you-should-understand-backprop"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"BatchNorm paper:"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa3RJcmpGRnNaRjBublc5cWRqcFk0NUo5U2t1Z3xBQ3Jtc0trZGM0bmlYSjVIZEp5djdRYklBZ2xydVdWLXRMcGJnUHhnQjZUc29nZDAyc3FPSkJfRVdmNDh2ZVI1ckdNcU1WdFlURWhSMnlUNklwVGtLdVg1REl5NC1iNjJDNzNocDBHQ3pIc1Uxb0JHUXhmY3B6SQ&q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.03167&v=q8SA3rM6ckI",children:"https://arxiv.org/abs/1502.03167"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Bessel\u2019s Correction:"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWU2M1A0WG5QbnhYR3R0eWVZaFhXUjd5dTdXQXxBQ3Jtc0trWnF5QlZYRWloWmNqbVV3VHBVNmllWHNqd0N6Si1qQ2Rtc0V2R3k0eDBBbkV5Ymo0QkRkMjBVQVZianJ5akNiaE9fODdaZ044Rldoclc1ZWpvYjVOSnNxVlFMVlU0NVVHVmFwcnRoOFZRT29VVkd1QQ&q=http%3A%2F%2Fmath.oxford.emory.edu%2Fsite%2Fmath117%2FbesselCorrection%2F&v=q8SA3rM6ckI",children:"http://math.oxford.emory.edu/site/mat..."})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Bengio et al. 2003 MLP LM"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa3JrUEtoU1FublVjN1k4TFYzQXBNUnpjaWd5Z3xBQ3Jtc0tuNGdxa2pnUlV3c205ZUFHT0M0WllfOTdKUHV4V1ozODBIUExGUkxhVFBXaDhwQlFVWW9ZSmZ3ZWxjdC1zRUZJTmYyT1ZSVmdXY1BYOHhncDNocWc3LXpXQkpKQXl6V2hPLUs1Y0F0eTRjNGJ2dWluMA&q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&v=q8SA3rM6ckI",children:"https://www.jmlr.org/papers/volume3/b..."})}),"\n",(0,r.jsx)(n.h1,{id:"exercise-1-manually-go-through-back-propagation",children:"Exercise 1: Manually Go Through Back Propagation"}),"\n",(0,r.jsx)(n.p,{children:"Using the same problem as from previous, but instead of using loss.barckward(), run the backward pass manually."}),"\n",(0,r.jsx)(n.p,{children:"Below is the code from previous lecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n\nwords = open('makemore-master/names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n# build the dataset\nblock_size = 3  # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix]  # crop and append\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\n\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])  # 80%\nXdev, Ydev = build_dataset(words[n1:n2])  # 10%\nXte, Yte = build_dataset(words[n2:])  # 10%\n\n# utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n  ex = torch.all(dt == t.grad).item()\n  app = torch.allclose(dt, t.grad, atol=1e-7)\n  maxdiff = (dt - t.grad).abs().max().item()\n  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\nbatch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n  p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n  t.retain_grad()\nloss.backward()\n"})}),"\n",(0,r.jsx)(n.p,{children:"Exercise 1: Backprop through the whole thing manually, backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"\ndlogprobs = torch.zeros_like(logprobs)\ndlogprobs[range(n), Yb] = -1/n\ndprobs = dlogprobs * (1/probs)\ndcounts_sum_inv = (dprobs * counts).sum(dim=1, keepdim=True)\n\"\"\"\n[[1,2,3], * [[1],   =  [[1,2,3], * [[1,1,1],   = [[1,2,3],\n[4,5,6]]     [2]]     [4,5,6]]      [2,2,2]]     [8,10,12]\n\nso, dprobs/dcounts_sum_inv should = [[1+2+3], [4+5+6]], because if counts_sum_inv multiply by 2, \nall of them will be multiply by 2, introducing a 2*counts change in the loss. \n\"\"\"\ndcounts_sum = dcounts_sum_inv * (- counts_sum ** -2)\ndcounts = dprobs * counts_sum_inv\ndcounts += dcounts_sum * torch.ones_like(counts)\n\ndnorm_logits = dcounts * counts # i.e., it is dcounts * norm_logits.exp()\ndlogit_maxes = (dnorm_logits * (-1)).sum(dim=1,keepdim=True)\nlogit_max_indices = logits.max(dim=1, keepdim=True).indices\ndlogit_maxes_dlogits = torch.zeros_like(logits)\ndlogit_maxes_dlogits[torch.arange(logits.size(0)), logit_max_indices] = 1\ndlogits = dnorm_logits\ndlogits += dlogit_maxes * dlogit_maxes_dlogits\n\ndh = dlogits @ W2.T\ndW2 = h.T @ dlogits\n\"\"\"\ny = a @ b + c\n\n[[1,2]] @ [[1,2,3] = [[1+8, 2+10, 3+12]]\n          [4,5,6]]\n1*2, 2*3, 1*3\nda = dy @ b.T\n    1*3, 3*2\ndb = a.T @ dy  \n2*3  2*1 1*3\n\"\"\"\ndb2 = dlogits.sum(0) # Note, the dimension is different for b2\n\"\"\"\nlogits = h @ W2 + b2\n2*4  2*3, 3*4, 1*4 \n\"\"\"\ndhpreact = dh * (1 - h ** 2)\ndbngain = (dhpreact * bnraw).sum(0)\ndbnbias = dhpreact.sum(0)\ndbnraw = dhpreact * bngain\ndbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)\ndbnvar = dbnvar_inv * -0.5 * (bnvar + 1e-5) ** -1.5\ndbndiff2 = dbnvar * (1/(n-1))\ndbndiff = dbnraw * bnvar_inv\ndbndiff += dbndiff2 * 2 * bndiff\ndbnmeani = (dbndiff * (-1)).sum(0)\ndhprebn = dbndiff.clone()\ndhprebn += dbnmeani * (1/n) * torch.ones_like(hprebn)\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1 = dhprebn.sum(0)\ndemb = dembcat.view(C[Xb].shape)\ndC = torch.zeros_like(C)\nfor data_idx in range(Xb.shape[0]):\n    for loc_idx in range(Xb.shape[1]):\n        char_idx = Xb[data_idx, loc_idx]\n        dC[char_idx, :] += demb[data_idx, loc_idx, :]\n\ncmp('counts_sum', dcounts_sum, counts_sum)\ncmp('counts', dcounts, counts)\ncmp('norm_logits', dnorm_logits, norm_logits)\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\ncmp('logits', dlogits, logits)\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\ncmp('hpreact', dhpreact, hpreact)\ncmp('bngain', dbngain, bngain)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnraw', dbnraw, bnraw)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\ncmp('bnvar', dbnvar, bnvar)\ncmp('bndiff2', dbndiff2, bndiff2)\ncmp('bndiff', dbndiff, bndiff)\ncmp('bnmeani', dbnmeani, bnmeani)\ncmp('hprebn', dhprebn, hprebn)\ncmp('embcat', dembcat, embcat)\ncmp('W1', dW1, W1)\ncmp('b1', db1, b1)\ncmp('emb', demb, emb)\ncmp('C', dC, C)\n\n"})}),"\n",(0,r.jsx)(n.p,{children:"output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\nlogits          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\nh               | exact: False | approximate: True  | maxdiff: 7.741618901491165e-09\nW2              | exact: False | approximate: True  | maxdiff: 2.9802322387695312e-08\nb2              | exact: False | approximate: True  | maxdiff: 2.60770320892334e-08\nhpreact         | exact: False | approximate: True  | maxdiff: 7.741618901491165e-09\nbngain          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\nbnbias          | exact: False | approximate: True  | maxdiff: 1.7345882952213287e-08\nbnraw           | exact: False | approximate: True  | maxdiff: 7.159542292356491e-09\nbnvar_inv       | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-08\nbnvar           | exact: False | approximate: True  | maxdiff: 3.259629011154175e-09\nbndiff2         | exact: False | approximate: True  | maxdiff: 1.0186340659856796e-10\nbndiff          | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\nbnmeani         | exact: False | approximate: True  | maxdiff: 1.2456439435482025e-08\nhprebn          | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\nembcat          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\nW1              | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-08\nb1              | exact: False | approximate: True  | maxdiff: 4.190951585769653e-09\nemb             | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\nC               | exact: False | approximate: True  | maxdiff: 2.1420419216156006e-08\n"})}),"\n",(0,r.jsx)(n.h1,{id:"exercise-2-backprop-through-cross_entropy-but-all-in-one-go",children:"Exercise 2: Backprop through cross_entropy but all in one go."}),"\n",(0,r.jsx)(n.p,{children:"To complete this challenge, examine the mathematical expression of the loss function. Take its derivative, simplify the expression, and then express it in a concise form."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'\n# forward pass\n\n# before:\n# logit_maxes = logits.max(1, keepdim=True).values\n# norm_logits = logits - logit_maxes # subtract max for numerical stability\n# counts = norm_logits.exp()\n# counts_sum = counts.sum(1, keepdims=True)\n# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can\'t get backprop to be bit exact...\n# probs = counts * counts_sum_inv\n# logprobs = probs.log()\n# loss = -logprobs[range(n), Yb].mean()\n\n# now:\nloss_fast = F.cross_entropy(logits, Yb)\nprint(loss_fast.item(), \'diff:\', (loss_fast - loss).item())\n\n"""\nlogits = [[0, 0.5, 1]]\ncnt = [[1, 1.65, 2.72]]\nsoftmax = [[0.18, 0.3, 0.5]]\nnll = [[1.6803, 1.1803, 0.6803]]]\nassume the currect result is at index 1 (the one with logits = 0.5)\nloss = 1.18\n-------------------\nin formula, assume logits = [x1, x2, x3]\nnll = [-log(e^x1/sum(e^x1, e^x2, e^x3)), -log(e^x2/sum(e^x1, e^x2, e^x3)), -log(e^x2/sum(e^x1, e^x2, e^x3))]\nafter plugging in the value (details in the video https://www.youtube.com/watch?v=q8SA3rM6ckI&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ 1:30:47,\nwe found that \n\ndloss_dxi = e^xi/sum(e^x1, e^x2, e^x3) if xi is not the correct prediction,\ndloss_dxi = e^xi/sum(e^x1, e^x2, e^x3) - 1 if xi is the correct prediction.\n\nAnd then for more than one samples, the dlogits need to divide by n. \n"""\n\n# backward pass\n\n# YOUR CODE HERE :)\nminus_item = torch.zeros_like(logits)\nminus_item = 1\nsoftmax = torch.nn.functional.softmax(logits, dim=1)\nsoftmax[torch.arange(n), Yb] -= 1\ndlogits = softmax * (1/n)\ncmp(\'logits\', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n""\n'})}),"\n",(0,r.jsx)(n.p,{children:"output:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"logits          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var a=t(96540);const r={},o=a.createContext(r);function i(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);