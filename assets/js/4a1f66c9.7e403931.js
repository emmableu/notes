"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[35783],{28453:(s,e,a)=>{a.d(e,{R:()=>r,x:()=>l});var n=a(96540);const t={},i=n.createContext(t);function r(s){const e=n.useContext(i);return n.useMemo(function(){return"function"==typeof s?s(e):{...e,...s}},[e,s])}function l(s){let e;return e=s.disableParentContext?"function"==typeof s.components?s.components(t):s.components||t:r(s.components),n.createElement(i.Provider,{value:e},s.children)}},65375:(s,e,a)=>{a.r(e),a.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>o,frontMatter:()=>r,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"Zero To Hero/Micrograd","title":"Resources","description":"Youtube link building micrograd","source":"@site/docs/04. Zero To Hero/01.Micrograd.md","sourceDirName":"04. Zero To Hero","slug":"/p/e8a79d58-7f55-415e-a4b9-25657b36ea95","permalink":"/notes/docs/p/e8a79d58-7f55-415e-a4b9-25657b36ea95","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/04. Zero To Hero/01.Micrograd.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/e8a79d58-7f55-415e-a4b9-25657b36ea95","slug":"/p/e8a79d58-7f55-415e-a4b9-25657b36ea95"},"sidebar":"tutorialSidebar","previous":{"title":"04. Zero to Hero","permalink":"/notes/docs/zero2hero"},"next":{"title":"Makemore 1.1 - bigram - Train and Evaluate a Bigram Model","permalink":"/notes/docs/p/d9d7f60b-8e6c-4fcd-a40d-6e44deaca3c8"}}');var t=a(74848),i=a(28453);const r={created_at:"2025-11-02",page_link:"/p/e8a79d58-7f55-415e-a4b9-25657b36ea95",slug:"/p/e8a79d58-7f55-415e-a4b9-25657b36ea95"},l="Resources",d={},c=[{value:"Section 1: Derivatives",id:"section-1-derivatives",level:2},{value:"1.1 Analytical Gradient",id:"11-analytical-gradient",level:3},{value:"1.2 Numerical Gradient",id:"12-numerical-gradient",level:3},{value:"1.3 Numerical Gradient from Symmetric Derivative",id:"13-numerical-gradient-from-symmetric-derivative",level:3},{value:"Section 2: Support for SoftMax",id:"section-2-support-for-softmax",level:2},{value:"2.1 Implement _backward() for multiple operations",id:"21-implement-_backward-for-multiple-operations",level:3},{value:"2.2 Implement the above in Torch and verify if results are the same.",id:"22-implement-the-above-in-torch-and-verify-if-results-are-the-same",level:3},{value:"Personal Notes",id:"personal-notes",level:2},{value:"1. Why is chain rule multiplicative.",id:"1-why-is-chain-rule-multiplicative",level:3}];function m(s){const e={a:"a",annotation:"annotation",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",math:"math",mfrac:"mfrac",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",msup:"msup",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,i.R)(),...s.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"resources",children:"Resources"})}),"\n",(0,t.jsxs)(e.p,{children:["Youtube link: ",(0,t.jsx)(e.a,{href:"https://youtu.be/VMj-3S1tku0",children:"The spelled-out intro to neural networks and backpropagation: building micrograd"})]}),"\n",(0,t.jsx)(e.p,{children:"Links:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["micrograd on github: ",(0,t.jsx)(e.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXJabldvUVRZY0FtekduTmNuVGtQM2trVmdBUXxBQ3Jtc0ttV1pVU1V0QkNNREtUX2NkZ2JTWURPYXVhNGdwNnlLNXplYVJfdlNMZVJEaWpiV3lLRFFSZkI4S29lTDZudlpTSUlDRVdkZEw4azFVeFNxYjhoT1FRNXRlZk01X2IwZDR1RVZQN0xrV19EMy1xNXhCaw&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fmicrograd&v=VMj-3S1tku0"}),(0,t.jsx)(e.a,{href:"https://github.com/karpathy/micrograd",children:"https://github.com/karpathy/micrograd"})]}),"\n",(0,t.jsxs)(e.li,{children:["jupyter notebooks : ",(0,t.jsx)(e.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbTlMRC1INm5TclEzcW1kalNyanB5TTRINVhjUXxBQ3Jtc0tuNlJuNXhpVkxpenhweXJKdF9RdkR3Ml9IWEIzLXVodi1sbjVtQ2Qyak85ZDd5bW1LbkZxN0RJa21DWjJfdG9NTFdjQzF5YUM1cjVpS05fVVI5YnlLOHJBMjZqeHBQaFY5aHhyLXN0MHE4cVl5RlZhNA&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fnn-zero-to-hero%2Ftree%2Fmaster%2Flectures%2Fmicrograd&v=VMj-3S1tku0"}),(0,t.jsx)(e.a,{href:"https://github.com/karpathy/nn-zero-t",children:"https://github.com/karpathy/nn-zero-t"}),"..."]}),"\n",(0,t.jsxs)(e.li,{children:["micrograd exercises: ",(0,t.jsx)(e.a,{href:"https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing",children:"https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing"})]}),"\n"]}),"\n",(0,t.jsx)(e.h1,{id:"micrograd-exercise",children:"Micrograd Exercise"}),"\n",(0,t.jsxs)(e.p,{children:["question link: ",(0,t.jsx)(e.a,{href:"https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing#scrollTo=JnGHatCI51JP",children:"https://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing#scrollTo=JnGHatCI51JP"})]}),"\n",(0,t.jsx)(e.h2,{id:"section-1-derivatives",children:"Section 1: Derivatives"}),"\n",(0,t.jsx)(e.p,{children:"Here is a mathematical expression that takes 3 inputs and produces one output:"}),"\n",(0,t.jsx)(e.span,{className:"katex-display",children:(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsx)(e.mrow,{}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex"})]})})}),(0,t.jsx)(e.span,{className:"katex-html","aria-hidden":"true"})]})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from math import sin, cos\n\ndef f(a, b, c):\n  return -a**3 + sin(3*b) - 1.0/c + b**2.5 - a**0.5\n\nprint(f(2, 3, 4))\n# 6.336362190988558\n"})}),"\n",(0,t.jsx)(e.h3,{id:"11-analytical-gradient",children:"1.1 Analytical Gradient"}),"\n",(0,t.jsx)(e.p,{children:"Write the function df that returns the analytical gradient of f"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# i.e. use your skills from calculus to take the derivative, then implement the formula\n# if you do not calculus then feel free to ask wolframalpha, e.g.:\n# <https://www.wolframalpha.com/input?i=d%2Fda%28sin%283*a%29%29%29>\n\ndef gradf(a, b, c):\n  return [0, 0, 0] # todo, return [df/da, df/db, df/dc]\n\n# expected answer is the list of \nans = [-12.353553390593273, 10.25699027111255, 0.0625]\nyours = gradf(2, 3, 4)\nfor dim in range(3):\n  ok = 'OK' if abs(yours[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}\")\n"})}),"\n",(0,t.jsx)(e.p,{children:"Solution:"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsxs)(e.mrow,{children:[(0,t.jsxs)(e.mfrac,{children:[(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mi,{mathvariant:"normal",children:"\u2202"}),(0,t.jsx)(e.mi,{children:"f"})]}),(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mi,{mathvariant:"normal",children:"\u2202"}),(0,t.jsx)(e.mi,{children:"a"})]})]}),(0,t.jsx)(e.mo,{children:"="}),(0,t.jsx)(e.mo,{children:"\u2212"}),(0,t.jsx)(e.mn,{children:"3"}),(0,t.jsxs)(e.msup,{children:[(0,t.jsx)(e.mi,{children:"a"}),(0,t.jsx)(e.mn,{children:"2"})]}),(0,t.jsx)(e.mo,{children:"\u2212"}),(0,t.jsxs)(e.mfrac,{children:[(0,t.jsx)(e.mn,{children:"1"}),(0,t.jsx)(e.mn,{children:"2"})]}),(0,t.jsxs)(e.msup,{children:[(0,t.jsx)(e.mi,{children:"a"}),(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mo,{children:"\u2212"}),(0,t.jsx)(e.mn,{children:"0.5"})]})]})]}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"\\frac{\\partial f}{\\partial a} = -3a^2 - \\frac{1}{2}a^{-0.5}"})]})})}),(0,t.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"1.2772em",verticalAlign:"-0.345em"}}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mopen nulldelimiter"}),(0,t.jsx)(e.span,{className:"mfrac",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsxs)(e.span,{className:"vlist",style:{height:"0.9322em"},children:[(0,t.jsxs)(e.span,{style:{top:"-2.655em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(e.span,{className:"mord mtight",children:[(0,t.jsx)(e.span,{className:"mord mtight",style:{marginRight:"0.05556em"},children:"\u2202"}),(0,t.jsx)(e.span,{className:"mord mathnormal mtight",children:"a"})]})})]}),(0,t.jsxs)(e.span,{style:{top:"-3.23em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,t.jsxs)(e.span,{style:{top:"-3.4461em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(e.span,{className:"mord mtight",children:[(0,t.jsx)(e.span,{className:"mord mtight",style:{marginRight:"0.05556em"},children:"\u2202"}),(0,t.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.10764em"},children:"f"})]})})]})]}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.345em"},children:(0,t.jsx)(e.span,{})})})]})}),(0,t.jsx)(e.span,{className:"mclose nulldelimiter"})]}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(e.span,{className:"mrel",children:"="}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"0.8974em",verticalAlign:"-0.0833em"}}),(0,t.jsx)(e.span,{className:"mord",children:"\u2212"}),(0,t.jsx)(e.span,{className:"mord",children:"3"}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,t.jsx)(e.span,{className:"msupsub",children:(0,t.jsx)(e.span,{className:"vlist-t",children:(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.8141em"},children:(0,t.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:"2"})})]})})})})})]}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsx)(e.span,{className:"mbin",children:"\u2212"}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"1.1901em",verticalAlign:"-0.345em"}}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mopen nulldelimiter"}),(0,t.jsx)(e.span,{className:"mfrac",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsxs)(e.span,{className:"vlist",style:{height:"0.8451em"},children:[(0,t.jsxs)(e.span,{style:{top:"-2.655em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:"2"})})})]}),(0,t.jsxs)(e.span,{style:{top:"-3.23em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,t.jsxs)(e.span,{style:{top:"-3.394em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:"1"})})})]})]}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.345em"},children:(0,t.jsx)(e.span,{})})})]})}),(0,t.jsx)(e.span,{className:"mclose nulldelimiter"})]}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mord mathnormal",children:"a"}),(0,t.jsx)(e.span,{className:"msupsub",children:(0,t.jsx)(e.span,{className:"vlist-t",children:(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.8141em"},children:(0,t.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(e.span,{className:"mord mtight",children:[(0,t.jsx)(e.span,{className:"mord mtight",children:"\u2212"}),(0,t.jsx)(e.span,{className:"mord mtight",children:"0.5"})]})})]})})})})})]})]})]})]})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsxs)(e.mrow,{children:[(0,t.jsxs)(e.mfrac,{children:[(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mi,{mathvariant:"normal",children:"\u2202"}),(0,t.jsx)(e.mi,{children:"f"})]}),(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mi,{mathvariant:"normal",children:"\u2202"}),(0,t.jsx)(e.mi,{children:"b"})]})]}),(0,t.jsx)(e.mo,{children:"="}),(0,t.jsx)(e.mn,{children:"3"}),(0,t.jsx)(e.mi,{children:"cos"}),(0,t.jsx)(e.mo,{children:"\u2061"}),(0,t.jsx)(e.mo,{stretchy:"false",children:"("}),(0,t.jsx)(e.mn,{children:"3"}),(0,t.jsx)(e.mi,{children:"b"}),(0,t.jsx)(e.mo,{stretchy:"false",children:")"}),(0,t.jsx)(e.mo,{children:"+"}),(0,t.jsx)(e.mn,{children:"2.5"}),(0,t.jsxs)(e.msup,{children:[(0,t.jsx)(e.mi,{children:"b"}),(0,t.jsx)(e.mn,{children:"1.5"})]})]}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"\\frac{\\partial f}{\\partial b} = 3\\cos(3b) + 2.5b^{1.5}"})]})})}),(0,t.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"1.2772em",verticalAlign:"-0.345em"}}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mopen nulldelimiter"}),(0,t.jsx)(e.span,{className:"mfrac",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsxs)(e.span,{className:"vlist",style:{height:"0.9322em"},children:[(0,t.jsxs)(e.span,{style:{top:"-2.655em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(e.span,{className:"mord mtight",children:[(0,t.jsx)(e.span,{className:"mord mtight",style:{marginRight:"0.05556em"},children:"\u2202"}),(0,t.jsx)(e.span,{className:"mord mathnormal mtight",children:"b"})]})})]}),(0,t.jsxs)(e.span,{style:{top:"-3.23em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,t.jsxs)(e.span,{style:{top:"-3.4461em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(e.span,{className:"mord mtight",children:[(0,t.jsx)(e.span,{className:"mord mtight",style:{marginRight:"0.05556em"},children:"\u2202"}),(0,t.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.10764em"},children:"f"})]})})]})]}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.345em"},children:(0,t.jsx)(e.span,{})})})]})}),(0,t.jsx)(e.span,{className:"mclose nulldelimiter"})]}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(e.span,{className:"mrel",children:"="}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,t.jsx)(e.span,{className:"mord",children:"3"}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,t.jsx)(e.span,{className:"mop",children:"cos"}),(0,t.jsx)(e.span,{className:"mopen",children:"("}),(0,t.jsx)(e.span,{className:"mord",children:"3"}),(0,t.jsx)(e.span,{className:"mord mathnormal",children:"b"}),(0,t.jsx)(e.span,{className:"mclose",children:")"}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,t.jsx)(e.span,{className:"mbin",children:"+"}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"0.8141em"}}),(0,t.jsx)(e.span,{className:"mord",children:"2.5"}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mord mathnormal",children:"b"}),(0,t.jsx)(e.span,{className:"msupsub",children:(0,t.jsx)(e.span,{className:"vlist-t",children:(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.8141em"},children:(0,t.jsxs)(e.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"2.7em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:"1.5"})})})]})})})})})]})]})]})]})}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsxs)(e.span,{className:"katex",children:[(0,t.jsx)(e.span,{className:"katex-mathml",children:(0,t.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,t.jsxs)(e.semantics,{children:[(0,t.jsxs)(e.mrow,{children:[(0,t.jsxs)(e.mfrac,{children:[(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mi,{mathvariant:"normal",children:"\u2202"}),(0,t.jsx)(e.mi,{children:"f"})]}),(0,t.jsxs)(e.mrow,{children:[(0,t.jsx)(e.mi,{mathvariant:"normal",children:"\u2202"}),(0,t.jsx)(e.mi,{children:"c"})]})]}),(0,t.jsx)(e.mo,{children:"="}),(0,t.jsxs)(e.mfrac,{children:[(0,t.jsx)(e.mn,{children:"1.0"}),(0,t.jsxs)(e.msup,{children:[(0,t.jsx)(e.mi,{children:"c"}),(0,t.jsx)(e.mn,{children:"2"})]})]})]}),(0,t.jsx)(e.annotation,{encoding:"application/x-tex",children:"\\frac{\\partial f}{\\partial c} = \\frac{1.0}{c^2}"})]})})}),(0,t.jsxs)(e.span,{className:"katex-html","aria-hidden":"true",children:[(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"1.2772em",verticalAlign:"-0.345em"}}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mopen nulldelimiter"}),(0,t.jsx)(e.span,{className:"mfrac",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsxs)(e.span,{className:"vlist",style:{height:"0.9322em"},children:[(0,t.jsxs)(e.span,{style:{top:"-2.655em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(e.span,{className:"mord mtight",children:[(0,t.jsx)(e.span,{className:"mord mtight",style:{marginRight:"0.05556em"},children:"\u2202"}),(0,t.jsx)(e.span,{className:"mord mathnormal mtight",children:"c"})]})})]}),(0,t.jsxs)(e.span,{style:{top:"-3.23em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,t.jsxs)(e.span,{style:{top:"-3.4461em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsxs)(e.span,{className:"mord mtight",children:[(0,t.jsx)(e.span,{className:"mord mtight",style:{marginRight:"0.05556em"},children:"\u2202"}),(0,t.jsx)(e.span,{className:"mord mathnormal mtight",style:{marginRight:"0.10764em"},children:"f"})]})})]})]}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.345em"},children:(0,t.jsx)(e.span,{})})})]})}),(0,t.jsx)(e.span,{className:"mclose nulldelimiter"})]}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,t.jsx)(e.span,{className:"mrel",children:"="}),(0,t.jsx)(e.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,t.jsxs)(e.span,{className:"base",children:[(0,t.jsx)(e.span,{className:"strut",style:{height:"1.1901em",verticalAlign:"-0.345em"}}),(0,t.jsxs)(e.span,{className:"mord",children:[(0,t.jsx)(e.span,{className:"mopen nulldelimiter"}),(0,t.jsx)(e.span,{className:"mfrac",children:(0,t.jsxs)(e.span,{className:"vlist-t vlist-t2",children:[(0,t.jsxs)(e.span,{className:"vlist-r",children:[(0,t.jsxs)(e.span,{className:"vlist",style:{height:"0.8451em"},children:[(0,t.jsxs)(e.span,{style:{top:"-2.655em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:(0,t.jsxs)(e.span,{className:"mord mtight",children:[(0,t.jsx)(e.span,{className:"mord mathnormal mtight",children:"c"}),(0,t.jsx)(e.span,{className:"msupsub",children:(0,t.jsx)(e.span,{className:"vlist-t",children:(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.7463em"},children:(0,t.jsxs)(e.span,{style:{top:"-2.786em",marginRight:"0.0714em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"2.5em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size3 size1 mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:"2"})})]})})})})})]})})})]}),(0,t.jsxs)(e.span,{style:{top:"-3.23em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"frac-line",style:{borderBottomWidth:"0.04em"}})]}),(0,t.jsxs)(e.span,{style:{top:"-3.394em"},children:[(0,t.jsx)(e.span,{className:"pstrut",style:{height:"3em"}}),(0,t.jsx)(e.span,{className:"sizing reset-size6 size3 mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:(0,t.jsx)(e.span,{className:"mord mtight",children:"1.0"})})})]})]}),(0,t.jsx)(e.span,{className:"vlist-s",children:"\u200b"})]}),(0,t.jsx)(e.span,{className:"vlist-r",children:(0,t.jsx)(e.span,{className:"vlist",style:{height:"0.345em"},children:(0,t.jsx)(e.span,{})})})]})}),(0,t.jsx)(e.span,{className:"mclose nulldelimiter"})]})]})]})]})}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from math import sin, cos\ndef gradf(a, b, c):\n    df_da = -3 * a**2 - 0.5 * a ** (-0.5)\n    df_db = 3 * cos(3 * b) + 2.5 * b ** 1.5\n    df_dc = c ** (-2)\n    return [df_da, df_db, df_dc]\n\n# expected answer is the list of\nans = [-12.353553390593273, 10.25699027111255, 0.0625]\nyours = gradf(2, 3, 4)\nfor dim in range(3):\n    ok = \'OK\' if abs(yours[dim] - ans[dim]) < 1e-5 else \'WRONG!\'\n    print(f"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}")\n\n"""\noutput:\nOK for dim 0: expected -12.353553390593273, yours returns -12.353553390593273\nOK for dim 1: expected 10.25699027111255, yours returns 10.25699027111255\nOK for dim 2: expected 0.0625, yours returns 0.0625\n"""\n'})}),"\n",(0,t.jsx)(e.h3,{id:"12-numerical-gradient",children:"1.2 Numerical Gradient"}),"\n",(0,t.jsx)(e.p,{children:"now estimate the gradient numerically without any calculus, using the approximation we used in the video. you should not call the function df from the last cell."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# -----------\nnumerical_grad = [0, 0, 0] # TODO\n# -----------\n\nfor dim in range(3):\n  ok = 'OK' if abs(numerical_grad[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}\")\n"})}),"\n",(0,t.jsx)(e.p,{children:"Solution:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def grad():\n\tda, db, dc = 0.0000001, 0.0000001, 0.0000001\n\ta, b, c = 2, 3, 4\n\tdf_da = (f(a, b, c) - f(a - da, b, c)) / da\n\tdf_db = (f(a, b, c) - f(a, b - db, c)) / db\n\tdf_dc = (f(a, b, c) - f(a, b, c - dc)) / dc\n\treturn [df_da, df_db, df_dc]\n\nnumerical_grad = grad()\nfor dim in range(3):\n  ok = \'OK\' if abs(numerical_grad[dim] - ans[dim]) < 1e-5 else \'WRONG!\'\n  print(f"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}")\n \n"""\noutput:\nOK for dim 0: expected -12.353553390593273, yours returns -12.353552794053257\nOK for dim 1: expected 10.25699027111255, yours returns 10.256990119472675\nOK for dim 2: expected 0.0625, yours returns 0.06250000517127319\n"""\n'})}),"\n",(0,t.jsx)(e.h3,{id:"13-numerical-gradient-from-symmetric-derivative",children:"1.3 Numerical Gradient from Symmetric Derivative"}),"\n",(0,t.jsx)(e.p,{children:"There is an alternative formula that provides a much better numerical approximation to the derivative of a function."}),"\n",(0,t.jsxs)(e.p,{children:["Learn about it here: ",(0,t.jsx)(e.a,{href:"https://en.wikipedia.org/wiki/Symmetric_derivative",children:"https://en.wikipedia.org/wiki/Symmetric_derivative"}),", implement it. confirm that for the same step size h this version gives a better approximation."]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# -----------\nnumerical_grad2 = [0, 0, 0] # TODO\n# -----------\n\nfor dim in range(3):\n  ok = 'OK' if abs(numerical_grad2[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}\")\n"})}),"\n",(0,t.jsx)(e.p,{children:"Solution:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def grad2():\n    da, db, dc = 0.0000001, 0.0000001, 0.0000001\n    a, b, c = 2, 3, 4\n    df_da = (f(a + da, b, c) - f(a - da, b, c)) / (2 * da)\n    df_db = (f(a, b + db, c) - f(a, b - db, c)) / (2 * db)\n    df_dc = (f(a, b, c + dc) - f(a, b, c - dc)) / (2 * dc)\n    return [df_da, df_db, df_dc]\n\nnumerical_grad2 = grad2()\n\nfor dim in range(3):\n    ok = \'OK\' if abs(numerical_grad2[dim] - ans[dim]) < 1e-5 else \'WRONG!\'\n    print(f"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}")\n\n"""\noutput:\nOK for dim 0: expected -12.353553390593273, yours returns -12.353553380251014\nOK for dim 1: expected 10.25699027111255, yours returns 10.256990252699438\nOK for dim 2: expected 0.0625, yours returns 0.06250000073038109\n"""\n'})}),"\n",(0,t.jsx)(e.h2,{id:"section-2-support-for-softmax",children:"Section 2: Support for SoftMax"}),"\n",(0,t.jsx)(e.h3,{id:"21-implement-_backward-for-multiple-operations",children:"2.1 Implement _backward() for multiple operations"}),"\n",(0,t.jsxs)(e.p,{children:["Without referencing our code/video ",(0,t.jsx)(e.strong,{children:"too"})," much, make this cell work. You'll have to implement (in some cases re-implement) a number of functions of the Value object, similar to what we've seen in the video. Instead of the squared error loss, this implements the negative log likelihood loss, which is very often used in classification."]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Value class starter code, with many functions taken out\nfrom math import exp, log\n\nclass Value:\n  \n  def __init__(self, data, _children=(), _op='', label=''):\n    self.data = data\n    self.grad = 0.0\n    self._backward = lambda: None\n    self._prev = set(_children)\n    self._op = _op\n    self.label = label\n\n  def __repr__(self):\n    return f\"Value(data={self.data})\"\n  \n  def __add__(self, other): # exactly as in the video\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data + other.data, (self, other), '+')\n    \n    def _backward():\n      self.grad += 1.0 * out.grad\n      other.grad += 1.0 * out.grad\n    out._backward = _backward\n    \n    return out\n  \n  # ------\n  # re-implement all the other functions needed for the exercises below\n  # your code here\n  # TODO\n  # ------\n\n  def backward(self): # exactly as in video  \n    topo = []\n    visited = set()\n    def build_topo(v):\n      if v not in visited:\n        visited.add(v)\n        for child in v._prev:\n          build_topo(child)\n        topo.append(v)\n    build_topo(self)\n    \n    self.grad = 1.0\n    for node in reversed(topo):\n      node._backward()\n\n# this is the softmax function\n# <https://en.wikipedia.org/wiki/Softmax_function>\ndef softmax(logits):\n  counts = [logit.exp() for logit in logits]\n  denominator = sum(counts)\n  out = [c / denominator for c in counts]\n  return out\n\n# this is the negative log likelihood loss function, pervasive in classification\nlogits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\nprobs = softmax(logits)\nloss = -probs[3].log() # dim 3 acts as the label for this input example\nloss.backward()\nprint(loss.data)\n\nans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\nfor dim in range(4):\n  ok = 'OK' if abs(logits[dim].grad - ans[dim]) < 1e-5 else 'WRONG!'\n  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")\n"})}),"\n",(0,t.jsx)(e.p,{children:"Solution"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"# Value class starter code, with many functions taken out\nimport math\n\nclass Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):  # exactly as in the video\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += 1.0 * out.grad\n            other.grad += 1.0 * out.grad\n\n        out._backward = _backward\n\n        return out\n\n    def __radd__(self, other):  # other + self\n        return self + other\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n\n        out._backward = _backward\n        return out\n\n    def __rmul__(self, other):\n        return self * other\n\n    def exp(self):\n        out = Value(math.exp(self.data), (self,), 'exp')\n\n        def _backward():\n            self.grad += math.exp(self.data) * out.grad\n\n        out._backward = _backward\n        return out\n\n    def __truediv__(self, other):\n        out = Value(self.data/other.data, (self, other), '/')\n\n        def _backward():\n            self.grad += other.data ** (-1) * out.grad\n            other.grad += - self.data * other.data ** (-2) * out.grad\n\n        out._backward = _backward\n        return out\n\n    def __neg__(self):  # -self\n        return self * -1\n\n    def log(self):\n        out = Value(math.log(self.data), (self, ), 'log')\n\n        def _backward():\n            self.grad += 1 / self.data * out.grad\n\n        out._backward = _backward\n        return out\n\n    def backward(self):  # exactly as in video\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n# this is the softmax function\n# <https://en.wikipedia.org/wiki/Softmax_function>\ndef softmax(logits):\n    counts = [logit.exp() for logit in logits]\n    denominator = sum(counts)\n    out = [c / denominator for c in counts]\n    return out\n\n# this is the negative log likelihood loss function, pervasive in classification\nlogits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\nprobs = softmax(logits)\nloss = -probs[3].log()  # dim 3 acts as the label for this input example\nloss.backward()\nprint(loss.data)\n\nans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\nfor dim in range(4):\n    ok = 'OK' if abs(logits[dim].grad - ans[dim]) < 1e-5 else 'WRONG!'\n    print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")\n\n\"\"\"\noutput:\n-3.1755153626167147\nOK for dim 0: expected 0.041772570515350445, yours returns 0.041772570515350445\nOK for dim 1: expected 0.8390245074625319, yours returns 0.8390245074625319\nOK for dim 2: expected 0.005653302662216329, yours returns 0.005653302662216329\nOK for dim 3: expected -0.8864503806400986, yours returns -0.886450380640099\n\"\"\"\n"})}),"\n",(0,t.jsx)(e.h3,{id:"22-implement-the-above-in-torch-and-verify-if-results-are-the-same",children:"2.2 Implement the above in Torch and verify if results are the same."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import torch\nlogits = torch.Tensor([0, 3, -2, 1])\nlogits.requires_grad = True\nprobs = torch.softmax(logits, dim=-1)\nloss = -torch.log(probs[3])\nloss.backward()\nans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n\nfor dim in range(4):\n    ok = \'OK\' if abs(logits.grad[dim] - ans[dim]) < 1e-5 else \'WRONG!\'\n    print(f"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits.grad[dim]}")\n    \n"""\noutput:\nOK for dim 0: expected 0.041772570515350445, yours returns 0.041772566735744476\nOK for dim 1: expected 0.8390245074625319, yours returns 0.8390244841575623\nOK for dim 2: expected 0.005653302662216329, yours returns 0.005653302650898695\nOK for dim 3: expected -0.8864503806400986, yours returns -0.8864504098892212\n"""\n'})}),"\n",(0,t.jsx)(e.h2,{id:"personal-notes",children:"Personal Notes"}),"\n",(0,t.jsx)(e.h3,{id:"1-why-is-chain-rule-multiplicative",children:"1. Why is chain rule multiplicative."}),"\n",(0,t.jsx)(e.p,{children:"i.e.\uff0c"}),"\n",(0,t.jsx)(e.p,{children:"y = f(g(x))"}),"\n",(0,t.jsx)(e.p,{children:"dy/dx = dy/dg * dg/df"}),"\n",(0,t.jsx)(e.p,{children:"E.g.,"}),"\n",(0,t.jsxs)(e.p,{children:["in the implementation of ",(0,t.jsx)(e.code,{children:"__add__"})," , in ",(0,t.jsx)(e.code,{children:"out._backward()"})," self.grad = 1 * self.grad, other.grad = 1 * other.grad"]}),"\n",(0,t.jsx)(e.p,{children:"\u60f3\u8c61\uff1a\u535a\u5c14\u7279\u8dd1\u6b65\uff0c \u901f\u5ea6 * \uff08\u65f6\u95f41 + \u65f6\u95f42\uff09 = \u603b\u8def\u7a0b"}),"\n",(0,t.jsx)(e.p,{children:"e.g.\uff0c10\u7c73/\u79d2 * ( 9 \u79d2 + 11 \u79d2\uff09 = 200\u7c73"}),"\n",(0,t.jsx)(e.p,{children:"d_\u603b\u8def\u7a0b/d_\u901f\u5ea6 = \u65f6\u95f41 + \u65f6\u95f42 \uff08i.e., \u901f\u5ea6\u63d0\u53471\u7c73/\u5206\u949f\uff0c \u603b\u8def\u7a0b\u589e\u52a020\u7c73\uff09"}),"\n",(0,t.jsx)(e.p,{children:"d_\u603b\u8def\u7a0b/d_\u65f6\u95f41 = \u901f\u5ea6 * 1 \uff08i.e., \u7b2c\u4e00\u6bb5\u591a\u52a0\u4e0a1\u79d2\uff0c\u603b\u8def\u7a0b\u589e\u52a010\u7c73\uff09"})]})}function o(s={}){const{wrapper:e}={...(0,i.R)(),...s.components};return e?(0,t.jsx)(e,{...s,children:(0,t.jsx)(m,{...s})}):m(s)}}}]);