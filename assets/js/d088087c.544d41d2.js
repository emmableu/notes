"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[24905],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(96540);const s={},a=i.createContext(s);function o(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(a.Provider,{value:n},e.children)}},29405:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"Zero To Hero/Let\'s Build GPT-2 (124M)","title":"Introduction","description":"We reproduce the GPT-2 (124M) from scratch. This video covers the whole process: First we build the GPT-2 network, then we optimize its training to be really fast, then we set up the training run following the GPT-2 and GPT-3 paper and their hyperparameters, then we hit run, and come back the next morning to see our results, and enjoy some amusing model generations. Keep in mind that in some places this video builds on the knowledge from earlier videos in the Zero to Hero Playlist (see my channel). You could also see this video as building my nanoGPT repo, which by the end is about 90% similar.","source":"@site/docs/04. Zero To Hero/10. Let\'s Build GPT-2 (124M).md","sourceDirName":"04. Zero To Hero","slug":"/p/dcc7f43c-a541-49f9-b9a0-7b6c92d6ad5f","permalink":"/notes/docs/p/dcc7f43c-a541-49f9-b9a0-7b6c92d6ad5f","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/04. Zero To Hero/10. Let\'s Build GPT-2 (124M).md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/dcc7f43c-a541-49f9-b9a0-7b6c92d6ad5f","slug":"/p/dcc7f43c-a541-49f9-b9a0-7b6c92d6ad5f"},"sidebar":"tutorialSidebar","previous":{"title":"Let\'s Build GPT Tokenizer","permalink":"/notes/docs/p/97f699ed-2b77-40cf-a3a4-7b41b8f39f2f"},"next":{"title":"Reinforcement Learning","permalink":"/notes/docs/reinforcement-learning"}}');var s=t(74848),a=t(28453);const o={created_at:"2025-11-02",page_link:"/p/dcc7f43c-a541-49f9-b9a0-7b6c92d6ad5f",slug:"/p/dcc7f43c-a541-49f9-b9a0-7b6c92d6ad5f"},r="Introduction",l={},d=[{value:"1.1: Architecture Overview",id:"11-architecture-overview",level:2},{value:"2.1 Building the basic model so that we can import the weights.",id:"21-building-the-basic-model-so-that-we-can-import-the-weights",level:2},{value:"2.2. Add 3 important weight initialization:",id:"22-add-3-important-weight-initialization",level:2},{value:"3.1 add the loss calculation into forward",id:"31-add-the-loss-calculation-into-forward",level:2},{value:"3.2 Write Code for a Dataloader",id:"32-write-code-for-a-dataloader",level:2},{value:"3.3 Write Code for training",id:"33-write-code-for-training",level:2},{value:"3.4 Send all code to GPU",id:"34-send-all-code-to-gpu",level:2},{value:"4.1 FP32 to TF32 or BF16",id:"41-fp32-to-tf32-or-bf16",level:2},{value:"Implement fp32:",id:"implement-fp32",level:3},{value:"Implement bf16:",id:"implement-bf16",level:3},{value:"4.2 Torch.compile",id:"42-torchcompile",level:2},{value:"4.3 Use FlashAttention",id:"43-use-flashattention",level:2},{value:"4.4 Change wierd numbers to 2**x",id:"44-change-wierd-numbers-to-2x",level:2},{value:"4.5 Training Results",id:"45-training-results",level:2},{value:"5.1 Learning Rate Decay",id:"51-learning-rate-decay",level:2},{value:"5.2 Weight Decay",id:"52-weight-decay",level:2},{value:"5.3 Batch Accumulation",id:"53-batch-accumulation",level:2},{value:"6.1 Step 1: Find DDP <code>os.environ</code> variables",id:"61-step-1-find-ddp-osenviron-variables",level:2},{value:"6.2 Step 2: Use <code>DistributedDataSampler</code>",id:"62-step-2-use-distributeddatasampler",level:2},{value:"6.3 Step 3: Update training process",id:"63-step-3-update-training-process",level:2},{value:"6.4 Step 4: use <code>torchrun</code> to run",id:"64-step-4-use-torchrun-to-run",level:2},{value:"6.5 Step 5:",id:"65-step-5",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,s.jsx)(n.p,{children:"We reproduce the GPT-2 (124M) from scratch. This video covers the whole process: First we build the GPT-2 network, then we optimize its training to be really fast, then we set up the training run following the GPT-2 and GPT-3 paper and their hyperparameters, then we hit run, and come back the next morning to see our results, and enjoy some amusing model generations. Keep in mind that in some places this video builds on the knowledge from earlier videos in the Zero to Hero Playlist (see my channel). You could also see this video as building my nanoGPT repo, which by the end is about 90% similar."}),"\n",(0,s.jsx)(n.p,{children:"Links:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["build-nanogpt GitHub repo, with all the changes in this video as individual commits: ",(0,s.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqazFRRWhjY0NiMy1tQnhCQ0wzU256d1EtV3V6Z3xBQ3Jtc0ttVmxCeXpHMmpsODBxOUFMWE40SXRRM0ZOb0JNLWNPdnI2UFp2WGt4amtHX042Vkg0cWh5cTFMTlhyZkNIUW9fZkQzdVRPNmVpYjN5QWNyN1JRWFRnRDh2OEg2SVJ4SF9TUzlxRU9DTUwxYUJZLTFLRQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fbuild-nanogpt&v=l8pRSuU81PU",children:"https://github.com/karpathy/build-nan..."})]}),"\n",(0,s.jsxs)(n.li,{children:["nanoGPT repo: ",(0,s.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa0VNNndyRWlVcHFGdlUxWTRvSnJVV1QwcEdsQXxBQ3Jtc0tsSjVXbDl1d3VTT3ZXQXRrbEcxcG9nRHhBVXZ1cldPYVF0REFFcXJmd3VwcW5jcWE0aXJFU3FLakZTUmtLckV3UjRWN0VLNG42cGVfa3luQXRKYTJMLVJuVEF2U0podndsdXFySHppbldRZ294YjRhWQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2FnanoGPT&v=l8pRSuU81PU",children:"https://github.com/karpathy/nanoGPT"})]}),"\n",(0,s.jsxs)(n.li,{children:["llm.c repo: ",(0,s.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbG04OGN5S3VndHFCMDY0Y2JJWnFZbGkyWGhhZ3xBQ3Jtc0ttZWtiMzIxOVNBYVpwRFVqWGY3NjNYTWlhLVZVYXZfNllNWUxfS0Q1OWg0Y2hqbjNVUkpMekFxSVhwVUh2c2VkU1lMUGZCbk5ZLXZGclgxZVFWQUxIcm13VVFuSmpmdHF6QTc1TDZSY0JPMjA0bGFpcw&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fllm.c&v=l8pRSuU81PU",children:"https://github.com/karpathy/llm.c"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Supplementary links:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Attention is All You Need paper: ",(0,s.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFFMYW5Gb1RBTnhjSVBoUlFoZjVzZ0JSakxqd3xBQ3Jtc0tudXJQeTJ3Z3pEMlpUcmhRanJZWTg5dnlwV0stWE1EVERVdmxnMXdWR0VaLXl2Z3dQR0RpdDVzWldzM3U5QmNKZW9YRm9SQ1lYSWxRSHNPdmdFWUxucFJLWkV2SVhvUnJsQ0hMX1liNTJFUGkyTTJJcw&q=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762&v=l8pRSuU81PU",children:"https://arxiv.org/abs/1706.03762"})]}),"\n",(0,s.jsxs)(n.li,{children:["OpenAI GPT-3 paper: ",(0,s.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXk3RUN6U083RzZITlprM3ltM09ZNmhQQnk4Z3xBQ3Jtc0trNHRBSVl1bW5CQ0JlOFdyejc3UmpoLVZhZDktSjFhVDFKR2NXbFNrZG9veUtzbG9jSnVxVklKS01OMlJrWWRPM3pWb1lEUmlWRlhoS0NYSURiaXFYMUZKcDAta2lmRzB6c2pFbTY0aGY1NnhHTnlOSQ&q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165&v=l8pRSuU81PU",children:"https://arxiv.org/abs/2005.14165"})]}),"\n",(0,s.jsxs)(n.li,{children:["OpenAI GPT-2 paper: ",(0,s.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa1RoYWtwX0NyblM2SEpLRnpTTm9UT0N1eThld3xBQ3Jtc0tuTFk1SXNsNGRnc29UQ0owTjNad2pmWjM3dERUNG5zUW1MTkxGcmh5bjJ5c2xhOV9tUkh5RmlVRzlQM3FZQVJYT3Y1aG05Rnh0eENCWERwY3A5QnRDQzdmSENSdW9NNkU4dzhSVzhhZnJYakZoZVhodw&q=https%3A%2F%2Fd4mucfpksywv.cloudfront.net%2Fbetter-language-models%2Flanguage_models_are_unsupervised_multitask_learners.pdf-&v=l8pRSuU81PU",children:"https://d4mucfpksywv.cloudfront.net/b..."})]}),"\n",(0,s.jsxs)(n.li,{children:["The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: ",(0,s.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbmIxd0VBU1hjRHp0ZHl6bGN0b3RWTm1MQmY5QXxBQ3Jtc0ttUXBQYnBheVZPZmxlU1FNRzJ0aVpEal85VmVta1NSQTdicnlTMmpIVDdFYlpKd0dOVFlTZ2NVNEI4cG1BUkxtQ1BkZF9KbW5LZlJ2YVpyUFBId3NTWlA1cF95dHNjOVBzN1kxcDVGNWxPVHdxSjlYMA&q=https%3A%2F%2Flambdalabs.com%2F&v=l8pRSuU81PU",children:"https://lambdalabs.com"})]}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"step-1-build-the-architecture-to-port-the-huggingface-model",children:"Step 1: Build the Architecture to Port the HuggingFace Model"}),"\n",(0,s.jsx)(n.p,{children:"We build the architecture to port in the huggingface model."}),"\n",(0,s.jsx)(n.p,{children:"We use huggingface model instead of original GPT2, as that one is built in tensorflow."}),"\n",(0,s.jsx)(n.h2,{id:"11-architecture-overview",children:"1.1: Architecture Overview"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042338754.png",alt:""})}),"\n",(0,s.jsx)(n.p,{children:"Architecture is similar to GPT from previous section"}),"\n",(0,s.jsx)(n.p,{children:"Below are some tiny updates we need to make:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"last linear layer will use the same weights as the positional embedding layer."}),"\n",(0,s.jsx)(n.li,{children:"we will use 12 blocks (Block * 12)"}),"\n",(0,s.jsx)(n.li,{children:"We will implement the multi-head-self-attention using 4-dimension tensors, instead of running tensor concatenation at the end, this will save time."}),"\n",(0,s.jsx)(n.li,{children:"to keep the variance = 1, the residual connection will divide by (sqrt(2 * num_Blocks))"}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"step-2-build-the-model-and-so-that-we-can-import-the-existing-gpt2-weights",children:"Step 2: Build the Model and so that we can import the existing GPT2 weights."}),"\n",(0,s.jsx)(n.h2,{id:"21-building-the-basic-model-so-that-we-can-import-the-weights",children:"2.1 Building the basic model so that we can import the weights."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom dataclasses import dataclass\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 1024 # max sequence length\n    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n    n_layer: int = 12 # number of layers\n    n_head: int = 12 # number of heads\n    n_embd: int = 768 # embedding dimension\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.n_embd = config.n_embd\n        self.n_head = config.n_head\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(\n            1, 1, config.block_size, config.block_size\n        ))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_embd, dim=-1)\n        q = q.view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n        att = (q @ k.transpose(-2,-1)) / self.n_head ** 0.5\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, -torch.inf)\n        att = att.softmax(dim=-1)\n        y = att @ v\n        y = y.transpose(1,2).contiguous().view(B, T, C)\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict({\n            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n            'wpe': nn.Embedding(config.block_size, config.n_embd),\n            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            'ln_f': nn.LayerNorm(config.n_embd)\n        })\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n    def forward(self, x):\n        B, T = x.shape\n        tok_emb = self.transformer.wte(x)\n        pos_emb = self.transformer.wpe(torch.arange(T))\n        x = tok_emb + pos_emb\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        return logits\n\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n        }[model_type]\n        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\nmodel = GPT.from_pretrained('gpt2')\nmodel.eval()\n\nimport tiktoken\nenc = tiktoken.get_encoding('gpt2')\ntokens = enc.encode(\"Hello, I'm a language model, \")\ntokens = torch.tensor(tokens, dtype=torch.long)\ntokens = tokens.unsqueeze(0).repeat(4,1)\nx = tokens\ntorch.manual_seed(42)\nmax_length = 50\nwhile x.size(1) < max_length:\n    with torch.no_grad():\n        logits = model(x)\n        logits = logits[:, -1, :]\n        probs = F.softmax(logits, dim=-1)\n        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n        ix = torch.multinomial(topk_probs, 1)\n        xcol = torch.gather(topk_indices, -1, ix)\n        x = torch.cat((x, xcol), dim=1)\n\nfor i in range(4):\n    tokens = x[i, :max_length].tolist()\n    decoded = enc.decode(tokens)\n    print(\">\", decoded)\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"output:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"/opt/homebrew/bin/python3.9 /Users/wengranwang/Documents/applebot_page_classification/page-classification/local-script/zero-to-hero/gpt-test-bolt/gpt2.py \nloading weights from pretrained gpt: gpt2\n> Hello, I'm a language model, \xa0what kind of one?\xa0\nThis code was generated once during the project.com workshop (which I'm running under) and again during the first.com event (which I'm running on)\n> Hello, I'm a language model, _____. Don't get me wrong, I love using ____.\" It said. \"Do not. Get. It wasn't.\n\nI could say. He could. If you can tell.\n> Hello, I'm a language model, iced and learned, but this is not my first time reading anything about how our parents feel about our own culture. (TheBluePearlMouth.PNG fileName)\nAry:\n\n> Hello, I'm a language model, \xa0you want to play on Android. I am a very real player now as well as a kid. I never get them to play on iOS. You're a really real human. \xa0I'm\n\nProcess finished with exit code 0\n\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"There is only 1 difference from the GPT 1 model from previous section:"})}),"\n",(0,s.jsx)(n.p,{children:"we used 4 dimension for k, q, v, this avoids concatenating tensors, as concatenating tensors takes a lot of time."}),"\n",(0,s.jsx)(n.h2,{id:"22-add-3-important-weight-initialization",children:"2.2. Add 3 important weight initialization:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["all weights are initiated with 0.02 standard deviation\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"this is essentially similar to the Faning initialization, but we use 0.02 to be the same as the original paper."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Important:"})," when we keep add a normal distribution of 0 mean, 1 std to an existing distribution, the variance increase, we need to counter that effect in c_proj before residual connection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Most Important:"})," the initial embedding matrix is the same as the lm_head weights, this helps us to reduce the number of parameters, in addition, if we think about the usage of those 2 weights, they essentially do the same thing."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict({\n            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n            'wpe': nn.Embedding(config.block_size, config.n_embd),\n            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            'ln_f': nn.LayerNorm(config.n_embd)\n        })\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # weight sharing scheme\n        self.transformer.wte.weight = self.lm_head.weight\n        # init params\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, 'RESIDUAL_CONNECTION_SCALE_INIT'):\n                std *= (2 * self.config.n_layer) ** -0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["and remember that for the ",(0,s.jsx)(n.code,{children:"c_proj"})," in SelfAttention, and for the ",(0,s.jsx)(n.code,{children:"c_proj"})," in MLP, add"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"self.c_proj.RESIDUAL_CONNECTION_SCALE_INIT = 1\n"})}),"\n",(0,s.jsx)(n.h1,{id:"3-write-code-for-model-training-and-evaluation",children:"3. Write code for model training and evaluation"}),"\n",(0,s.jsx)(n.h2,{id:"31-add-the-loss-calculation-into-forward",children:"3.1 add the loss calculation into forward"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"    def forward(self, x, y=None):\n        B, T = x.shape\n        tok_emb = self.transformer.wte(x)\n        pos_emb = self.transformer.wpe(torch.arange(T))\n        x = tok_emb + pos_emb\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        if y is not None:\n            loss = F.cross_entropy(input=logits.view(-1, self.config.vocab_size),target=y.view(-1))\n        else:\n            loss = None\n        return logits, loss\n"})}),"\n",(0,s.jsx)(n.h2,{id:"32-write-code-for-a-dataloader",children:"3.2 Write Code for a Dataloader"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ShakespeareDataLoader:\n    def __init__(self, batch_size, block_size):\n        self.i = 0\n        self.B = batch_size\n        self.T = block_size\n\n    def next_batch(self, encoded):\n        data = encoded[self.i: self.i + self.B * self.T + 1]\n        x = torch.tensor(data[:-1]).view(self.B, self.T)\n        y = torch.tensor(data[1:]).view(self.B, self.T)\n        self.i += self.B * self.T\n        if self.i + self.B * self.T + 1 > len(encoded):\n            self.i = 0\n        return x, y\n"})}),"\n",(0,s.jsx)(n.h2,{id:"33-write-code-for-training",children:"3.3 Write Code for training"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'\ndef train_model(training_config, gpt_config):\n    model = GPT(GPTConfig())\n    model.train()\n    optimizer = torch.optim.AdamW(params=model.parameters(), lr=3e-4)\n    dataloader = ShakespeareDataLoader(training_config.batch_size, gpt_config.block_size)\n    enc = tiktoken.get_encoding("gpt2")\n    with open("input.txt", encoding=\'utf-8\', mode=\'r\') as f:\n        text = f.read()\n    encoded = enc.encode(text)\n\n    for i in range(training_config.num_steps):\n        start_time = time.time()\n        optimizer.zero_grad()\n        x, y = dataloader.next_batch(encoded)\n        # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x, y)\n        loss.backward()\n        optimizer.step()\n        if device_type == "cuda":\n            torch.cuda.synchronize()  # wait for the GPU to finish work\n        end_time = time.time()\n        dt = (end_time - start_time) / (dataloader.B * dataloader.T) * 1000\n        if i % training_config.train_interval == 0 or i == training_config.num_steps - 1:\n            print(f"iteration {i=:4d} | ", f"loss={loss.item():4f} | ", f"{dt:4f} ms per token | ")\n\n'})}),"\n",(0,s.jsx)(n.p,{children:"Now, if you adjust the block_size to e.g., 36, and n_embd to e.g., 36 (needs to be 12*), then you can train the model locally."}),"\n",(0,s.jsx)(n.p,{children:"To view the speed in CUDA, use the below setting. Note that training_config.num_steps does not matter for now, as we are not going to train towards the end, we are just going to set things up in CUDA."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"@dataclass\nclass GPTConfig:\n    block_size: int = 1024  # max sequence length\n    vocab_size: int = 50257  # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n    n_layer: int = 12  # number of layers\n    n_head: int = 12  # number of heads\n    n_embd: int = 768  # embedding dimension\n\n@dataclass\nclass TrainingConfig:\n    batch_size: int = 16\n    num_steps: int = 200\n    learning_rate: int = 3e-4\n    train_interval: int = 50\n"})}),"\n",(0,s.jsx)(n.h2,{id:"34-send-all-code-to-gpu",children:"3.4 Send all code to GPU"}),"\n",(0,s.jsx)(n.p,{children:"Below is the complete basic code I send to GPU"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom dataclasses import dataclass\nimport time\nimport tiktoken\nimport numpy as np\n\ndevice_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n@dataclass\nclass GPTConfig:\n    block_size: int = 512  # max sequence length\n    vocab_size: int = 50257  # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n    n_layer: int = 12  # number of layers\n    n_head: int = 12  # number of heads\n    n_embd: int = 768  # embedding dimension\n\n@dataclass\nclass TrainingConfig:\n    batch_size: int = 4\n    num_steps: int = 200\n    learning_rate: int = 3e-4\n    train_interval: int = 1\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.c_proj.RESIDUAL_CONNECTION_SCALE_INIT = 1\n        self.n_embd = config.n_embd\n        self.n_head = config.n_head\n        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(\n            1, 1, config.block_size, config.block_size\n        ))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        qkv = self.c_attn(x)\n        q, k, v = qkv.split(self.n_embd, dim=-1)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        att = (q @ k.transpose(-2, -1)) / self.n_head ** 0.5\n        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, -torch.inf)\n        att = att.softmax(dim=-1)\n        y = att @ v\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.c_proj(y)\n        return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.gelu = nn.GELU(approximate='tanh')\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.c_proj.RESIDUAL_CONNECTION_SCALE_INIT = 1\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict({\n            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n            'wpe': nn.Embedding(config.block_size, config.n_embd),\n            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            'ln_f': nn.LayerNorm(config.n_embd)\n        })\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        # weight sharing scheme\n        self.transformer.wte.weight = self.lm_head.weight\n        # init params\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            std = 0.02\n            if hasattr(module, 'RESIDUAL_CONNECTION_SCALE_INIT'):\n                std *= (2 * self.config.n_layer) ** -0.5\n            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, x, y=None):\n        B, T = x.shape\n        tok_emb = self.transformer.wte(x)\n        pos_emb = self.transformer.wpe(torch.arange(T, device=device_type))\n        x = tok_emb + pos_emb\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        if y is not None:\n            loss = F.cross_entropy(input=logits.view(-1, self.config.vocab_size), target=y.view(-1))\n        else:\n            loss = None\n        return logits, loss\n\n    @classmethod\n    def from_pretrained(cls, model_type):\n        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n        from transformers import GPT2LMHeadModel\n        print(\"loading weights from pretrained gpt: %s\" % model_type)\n\n        # n_layer, n_head and n_embd are determined from model_type\n        config_args = {\n            'gpt2': dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n            'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n            'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n            'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n        }[model_type]\n        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n        # create a from-scratch initialized minGPT model\n        config = GPTConfig(**config_args)\n        model = GPT(config)\n        sd = model.state_dict()\n        sd_keys = sd.keys()\n        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # discard this mask / buffer, not a param\n\n        # init a huggingface/transformers model\n        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n        sd_hf = model_hf.state_dict()\n\n        # copy while ensuring all of the parameters are aligned and match in names and shapes\n        sd_keys_hf = sd_hf.keys()\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these, just a buffer\n        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]  # same, just the mask (buffer)\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n        # this means that we have to transpose these weights when we import them\n        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n        for k in sd_keys_hf:\n            if any(k.endswith(w) for w in transposed):\n                # special treatment for the Conv1D weights we need to transpose\n                assert sd_hf[k].shape[::-1] == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k].t())\n            else:\n                # vanilla copy over the other parameters\n                assert sd_hf[k].shape == sd[k].shape\n                with torch.no_grad():\n                    sd[k].copy_(sd_hf[k])\n\n        return model\n\nclass ShakespeareDataLoader:\n    def __init__(self, batch_size, block_size):\n        self.i = 0\n        self.B = batch_size\n        self.T = block_size\n\n    def next_batch(self, encoded):\n        data = encoded[self.i: self.i + self.B * self.T + 1]\n        x = torch.tensor(data[:-1]).view(self.B, self.T)\n        y = torch.tensor(data[1:]).view(self.B, self.T)\n        self.i += self.B * self.T\n        if self.i + self.B * self.T + 1 > len(encoded):\n            self.i = 0\n        return x, y\n\ndef train_model(training_config, gpt_config):\n    model = GPT(GPTConfig())\n    model.train()\n    model = model.to(device_type)\n    optimizer = torch.optim.AdamW(params=model.parameters(), lr=3e-4)\n    dataloader = ShakespeareDataLoader(training_config.batch_size, gpt_config.block_size)\n    enc = tiktoken.get_encoding(\"gpt2\")\n    with open(\"input.txt\", encoding='utf-8', mode='r') as f:\n        text = f.read()\n    encoded = enc.encode(text)\n\n    for i in range(training_config.num_steps):\n        start_time = time.time()\n        optimizer.zero_grad()\n        x, y = dataloader.next_batch(encoded)\n        x, y = x.to(device_type), y.to(device_type)\n        # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n        logits, loss = model(x, y)\n        loss.backward()\n        optimizer.step()\n        if device_type == \"cuda\":\n            torch.cuda.synchronize()  # wait for the GPU to finish work\n        end_time = time.time()\n        dt = (end_time - start_time) / (dataloader.B * dataloader.T) * 1000\n        if i % training_config.train_interval == 0 or i == training_config.num_steps - 1:\n            print(f\"iteration {i=:4d} | \", f\"loss={loss.item():4f} | \", f\"{dt:4f} ms per token | \")\n    return model\n\ndef generate_tokens(model, config):\n    model.eval()\n    enc = tiktoken.get_encoding('gpt2')\n    tokens = enc.encode(\"Hello, I'm a language model, \")\n    tokens = torch.tensor(tokens, dtype=torch.long)\n    tokens = tokens.unsqueeze(0).repeat(4, 1)\n    x = tokens.to(device_type)\n    torch.manual_seed(42)\n    max_length = 50\n    while x.size(1) < max_length:\n        with torch.no_grad():\n            logits, loss = model(x[:,-config.block_size:])\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n            ix = torch.multinomial(topk_probs, 1)\n            xcol = torch.gather(topk_indices, -1, ix).to(device_type)\n            x = torch.cat((x, xcol), dim=1)\n\n    for i in range(4):\n        tokens = x[i, :max_length].tolist()\n        decoded = enc.decode(tokens)\n        print(\">\", decoded)\n        \n# model = GPT.from_pretrained('gpt2')\n# model.eval()\n\nmodel = train_model(TrainingConfig(), GPTConfig())\ngenerate_tokens(model, GPTConfig())\n"})}),"\n",(0,s.jsx)(n.h1,{id:"4-optimization-methods",children:"4 Optimization methods"}),"\n",(0,s.jsx)(n.h2,{id:"41-fp32-to-tf32-or-bf16",children:"4.1 FP32 to TF32 or BF16"}),"\n",(0,s.jsx)(n.p,{children:"both: only change mantissa, don\u2019t change exponent."}),"\n",(0,s.jsx)(n.p,{children:"TF32: only change to TF32 for numbers, don\u2019t change aggregation function"}),"\n",(0,s.jsx)(n.h3,{id:"implement-fp32",children:"Implement fp32:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"torch.set_float32_matmul_precision('high')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"implement-bf16",children:"Implement bf16:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"with model.autocast(devicetype=device, dtype=torch.bfloat16):\n\t\tlogits, loss = model(x, y)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"42-torchcompile",children:"4.2 Torch.compile"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"model = torch.compile(model)\n"})}),"\n",(0,s.jsx)(n.p,{children:"this runs torch compile and saves LOTS of time during read/write to HBM (High-bandwidth memory)"}),"\n",(0,s.jsx)(n.h2,{id:"43-use-flashattention",children:"4.3 Use FlashAttention"}),"\n",(0,s.jsx)(n.p,{children:"FlashAttention is an algorithm that uses time (more flops) to exchange for memory efficiency."}),"\n",(0,s.jsx)(n.p,{children:"Internally It uses an algorithm that calculates softmax online."}),"\n",(0,s.jsx)(n.p,{children:"What it actually does is a very fast replacement to the below 4 operations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"att = q @ k.transpose(-2,-1)"}),"\n",(0,s.jsx)(n.li,{children:"att.masked_fill(tril == 0, inf)"}),"\n",(0,s.jsx)(n.li,{children:"att = att.softmax"}),"\n",(0,s.jsx)(n.li,{children:"y = att @ v"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"i.e., the below rows"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"        weight = q @ k.transpose(-2, -1) / (C // num_heads) ** 0.5  # B, nh, T, T\n        weight = weight.masked_fill(self.tril[:, :, :T, :T] == 0, -torch.inf)\n        weight = weight.softmax(-1)\n        y = weight @ v\n"})}),"\n",(0,s.jsx)(n.p,{children:"to replace it to one line:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"44-change-wierd-numbers-to-2x",children:"4.4 Change wierd numbers to 2**x"}),"\n",(0,s.jsx)(n.p,{children:"we can improve speed a great deal by changing all wierd numbers to 2**x"}),"\n",(0,s.jsx)(n.p,{children:"this is because internally GPU allocate chips in kernels using 2**x (e.g., 16 chips in one unit, etc)"}),"\n",(0,s.jsx)(n.p,{children:"If we put e.g., a batch with number = 17 elements, then it is very inefficient."}),"\n",(0,s.jsx)(n.p,{children:"It may in the end set up space allocation as, e.., 8 + 1 + 1 + 1 + 1, \u2026, and this caused speed to be compromised."}),"\n",(0,s.jsx)(n.h2,{id:"45-training-results",children:"4.5 Training Results"}),"\n",(0,s.jsx)(n.p,{children:"speed from Original code from 3.4:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"iteration i= 197 |  loss=5.446133 |  0.083159 ms per token | \niteration i= 198 |  loss=5.345389 |  0.082882 ms per token | \niteration i= 199 |  loss=5.423370 |  0.082952 ms per token | \n"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"with torch compile (9% speed improvement)"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"iteration i= 197 |  loss=5.531619 |  0.078779 ms per token | \niteration i= 198 |  loss=5.454237 |  0.078889 ms per token | \niteration i= 199 |  loss=5.476031 |  0.078809 ms per token | \n"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"with tf32 and bf16:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"bf16 were ~0.11 ms per token,"}),"\n",(0,s.jsx)(n.li,{children:"tf32 were ~0.10 ms per token"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"not useful"}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"with flash attention: 1% speed improvemen"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\niteration i= 196 |  loss=5.910939 |  0.082536 ms per token | \niteration i= 197 |  loss=5.763492 |  0.082578 ms per token | \niteration i= 198 |  loss=5.649689 |  0.082114 ms per token | \niteration i= 199 |  loss=5.712480 |  0.081921 ms per token | \n"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"with 2 ** x vocab size: not useful"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"used vocab size = 57344 = 7 * 2**13"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"iteration i= 197 |  loss=5.604393 |  0.088167 ms per token | \niteration i= 198 |  loss=5.511627 |  0.087224 ms per token | \niteration i= 199 |  loss=5.543206 |  0.087999 ms per token | \n"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"use torch.compile + flashAttention (the two that were useful)"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"iteration i= 197 |  loss=5.775408 |  0.078649 ms per token | \niteration i= 198 |  loss=5.643482 |  0.078356 ms per token | \niteration i= 199 |  loss=5.723202 |  0.078583 ms per token | \n"})}),"\n",(0,s.jsx)(n.h1,{id:"5-three-major-optimization-algorithms",children:"5. three major optimization algorithms"}),"\n",(0,s.jsx)(n.h2,{id:"51-learning-rate-decay",children:"5.1 Learning Rate Decay"}),"\n",(0,s.jsx)(n.p,{children:"The reason why we use learning rate decay is because:"}),"\n",(0,s.jsx)(n.p,{children:"the reason for decay:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"at the beginning, it is not actually learning much, other than understanding the basics (e.g., what tokens are not being used in the data at all, etc), so the model can tolerate a bigger learning rate."}),"\n",(0,s.jsx)(n.li,{children:"We can later make learning rate smaller because that is when the model is actually learning."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"the reason for warmup steps:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"I am not sure, but I think it\u2019s because at the very beginning the gradients are bigger, so it\u2019s ok with a small learning rate because the weights will change to the right direction regardless."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"the scale of learning rate decay:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"max_steps: it should be the number of steps for 1 epoch."}),"\n",(0,s.jsxs)(n.li,{children:["warmup_steps: it should be~ ",(0,s.jsx)(n.code,{children:"max_steps//25"})]}),"\n",(0,s.jsxs)(n.li,{children:["max_lr: it should be by default ",(0,s.jsx)(n.code,{children:"3e-4"})]}),"\n",(0,s.jsx)(n.li,{children:"min_lr: max_lr * 0.1"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"parameters to set up in actual implementation,"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"warmup_steps = 715"}),"\n",(0,s.jsx)(n.li,{children:"max_steps = 19073, this is because 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import math\n\nmax_lr = 6e-4\nmin_lr = max_lr * 0.1\nwarmup_steps = 10\nmax_steps = 50 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_steps:\n        return max_lr * (it+1) / warmup_steps\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > max_steps:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n    return min_lr + coeff * (max_lr - min_lr)\n\nfrom matplotlib import pyplot as plt\nplt.plot([get_lr(ele) for ele in range(55)])\nplt.show()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042338271.png",alt:""})}),"\n",(0,s.jsx)(n.p,{children:"in actual training:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'lr = get_lr(it)\nfor parameter_group in optimizer.parameter_groups:\n\t\tparameter_group["lr"] = lr\n'})}),"\n",(0,s.jsx)(n.h2,{id:"52-weight-decay",children:"5.2 Weight Decay"}),"\n",(0,s.jsx)(n.p,{children:"intuition: as model gets trained, we need to pull the weights closer to 0 so that it become less affected by the extremes, and so that the gradient do not vanish."}),"\n",(0,s.jsx)(n.p,{children:"we set up"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"optimizer = m.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n"})}),"\n",(0,s.jsx)(n.p,{children:"then we define the function under class GPTModel"}),"\n",(0,s.jsx)(n.p,{children:"the key is that weights that \u22652  dimension should be weight_decayed."}),"\n",(0,s.jsxs)(n.p,{children:["Note that we also need to add a ",(0,s.jsx)(n.code,{children:"fused=use_fused"})," parameter in order for these parameters to be fused in one kernel, this increases the speed of the gradient update, and should be included as default whether or not we use weight decay."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n        decay_params = [p for _, p in param_dict.items() if p.dim > 1]\n        nodecay_params = [p for _, p in param_dict.items() if p.dim <= 1]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == \"cuda\"\n        print(f\"using fused AdamW: {use_fused}\")\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n        return optimizer\n"})}),"\n",(0,s.jsx)(n.h2,{id:"53-batch-accumulation",children:"5.3 Batch Accumulation"}),"\n",(0,s.jsx)(n.h1,{id:"6-ddp",children:"6. DDP"}),"\n",(0,s.jsx)(n.p,{children:"So far we are only doing 1 gpu."}),"\n",(0,s.jsx)(n.p,{children:"We need to support doing >1 gpus."}),"\n",(0,s.jsx)(n.p,{children:"What DDP does is that:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"for each GPU we do the forward pass the same"}),"\n",(0,s.jsxs)(n.li,{children:["for each backward, after the backward() is called, if we call ",(0,s.jsx)(n.code,{children:"dist.all_reduce()"}),", we will average the weight across all the GPUs."]}),"\n",(0,s.jsxs)(n.li,{children:["then we call step(), where the weight used in step() is the averaged weight from the ",(0,s.jsx)(n.code,{children:"all_reduce"}),"()."]}),"\n"]}),"\n",(0,s.jsxs)(n.h2,{id:"61-step-1-find-ddp-osenviron-variables",children:["6.1 Step 1: Find DDP ",(0,s.jsx)(n.code,{children:"os.environ"})," variables"]}),"\n",(0,s.jsxs)(n.p,{children:["Most important ",(0,s.jsx)(n.code,{children:"os.environ"})," parameters:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RANK: gpu0, gpu1, gpu2, gpu3"}),"\n",(0,s.jsx)(n.li,{children:"WORLD_SIZE: number of gpu (i.e., 4)"}),"\n",(0,s.jsxs)(n.li,{children:["LOCAL_RANK: Refers to the index of the GPU on a\xa0",(0,s.jsx)(n.strong,{children:"specific machine"}),". This is used to tell the process which GPU to use on its machine. For example:\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["On a machine with 4 GPUs,\xa0",(0,s.jsx)(n.code,{children:"LOCAL_RANK"}),"\xa0will take values from\xa0",(0,s.jsx)(n.code,{children:"0"}),"\xa0to\xa0",(0,s.jsx)(n.code,{children:"3"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"This variable ensures that each process is assigned to the correct GPU on the local machine."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Let's say you're running a distributed training job with 2 machines, and each has 4 GPUs:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Machine 1"}),":\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RANK"}),": 0, 1, 2, 3 (global ranks for the GPUs on this machine)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"WORLD_SIZE"}),": 8 (since there are 8 GPUs total across both machines)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LOCAL_RANK"}),": 0, 1, 2, 3 (the specific GPUs on this machine)"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Machine 2"}),":\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RANK"}),": 4, 5, 6, 7 (global ranks for the GPUs on this machine)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"WORLD_SIZE"}),": 8"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LOCAL_RANK"}),": 0, 1, 2, 3 (the specific GPUs on this machine, similar to Machine 1)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"ddp = os.environ.get('RANK', -1) != -1\nif ddp:\n    init_process_group(backend='nccl')\n    ddp_rank = int(os.environ['RANK'])\n    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n    ddp_world_size = int(os.environ['WORLD_SIZE'])\n    device = f'cuda:{ddp_local_rank}'\n    torch.cuda.set_device(device)\n    master_process = ddp_rank == 0  # master_process is for logging\nelse:\n    ddp_rank, ddp_local_rank, ddp_world_size, master_process = 0, 0, 1, True\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"})}),"\n",(0,s.jsx)(n.p,{children:"make sure to use int(), as otherwise, it is a string and cannot do the rest of the stuff."}),"\n",(0,s.jsxs)(n.h2,{id:"62-step-2-use-distributeddatasampler",children:["6.2 Step 2: Use ",(0,s.jsx)(n.code,{children:"DistributedDataSampler"})]}),"\n",(0,s.jsxs)(n.p,{children:["Note: you must use ",(0,s.jsx)(n.code,{children:"DistributedDataSampler"}),", instead of doing something like"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"        for j, data in enumerate(dataloader):\n            if j % ddp_world_size != ddp_rank: continue\n"})}),"\n",(0,s.jsx)(n.p,{children:"why:"}),"\n",(0,s.jsx)(n.p,{children:"this will cause UNEVEN distribution, and at the last round of the training, GPU7, for example, will not have any input gradient to send to other machines, and the rest of GPU will TIMEOUT while waiting for the gradient to be sent."}),"\n",(0,s.jsxs)(n.p,{children:["also, if using ",(0,s.jsx)(n.code,{children:"torchrun"}),", then it will distribute the rest of the load to the rest of the GPUs, causing GPU overload."]}),"\n",(0,s.jsx)(n.h2,{id:"63-step-3-update-training-process",children:"6.3 Step 3: Update training process"}),"\n",(0,s.jsx)(n.p,{children:"we need to use the model as:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"model.to(device)\nmodel = torch.compile(model)\nif ddp:\n\t\tmodel = DDP(model, device_ids = [ddp_local_rank])\n"})}),"\n",(0,s.jsx)(n.p,{children:"Note that now we need to save the model as"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"raw_model = model.module if ddp else model\n"})}),"\n",(0,s.jsx)(n.p,{children:"this raw_model needs to be used for:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"saving the model"}),"\n",(0,s.jsxs)(n.li,{children:["also set up optimizers need to use ",(0,s.jsx)(n.code,{children:"raw_model.parameters()"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"During training:"}),"\n",(0,s.jsx)(n.p,{children:"for data loading:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"it\u2019s better for each data for each machine to be different,"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"e.g., we can do"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"for j, data in enumerate(dataloader):\n\t\tif j % ddp_world_size != ddp_rank: continue\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If we disable DDP in the middle (e.g., like in gradient accumulation), then we need to reenable it."}),"\n",(0,s.jsxs)(n.p,{children:["To do that, after calling ",(0,s.jsx)(n.code,{children:"loss.backward()"})]}),"\n",(0,s.jsx)(n.p,{children:"we need to call"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"if ddp:\n\t\tdist.all_reduce(loss, op=dist.ReduceOp.AVG)\n"})}),"\n",(0,s.jsx)(n.p,{children:"But we don\u2019t need to call it otherwise."}),"\n",(0,s.jsxs)(n.h2,{id:"64-step-4-use-torchrun-to-run",children:["6.4 Step 4: use ",(0,s.jsx)(n.code,{children:"torchrun"})," to run"]}),"\n",(0,s.jsxs)(n.p,{children:["to run this, we need to use ",(0,s.jsx)(n.code,{children:"torchrun"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"torchrun --nproc_per_node=4 train.py\n"})}),"\n",(0,s.jsx)(n.h2,{id:"65-step-5",children:"6.5 Step 5:"}),"\n",(0,s.jsx)(n.p,{children:"call"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"if ddp: init_process_group(backend='nccl')\n"})}),"\n",(0,s.jsx)(n.p,{children:"at the beginning of the program."}),"\n",(0,s.jsx)(n.p,{children:"call"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"if ddp: destroy_process_group() \n"})}),"\n",(0,s.jsx)(n.p,{children:"at the very end of the program"})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);