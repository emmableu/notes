"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[37689],{8827:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"Zero To Hero/Let\'s Build GPT Tokenizer","title":"Let\'s Build GPT Tokenizer","description":"The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We\'ll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely.","source":"@site/docs/04. Zero To Hero/09. Let\'s Build GPT Tokenizer.md","sourceDirName":"04. Zero To Hero","slug":"/p/97f699ed-2b77-40cf-a3a4-7b41b8f39f2f","permalink":"/notes/docs/p/97f699ed-2b77-40cf-a3a4-7b41b8f39f2f","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/04. Zero To Hero/09. Let\'s Build GPT Tokenizer.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/97f699ed-2b77-40cf-a3a4-7b41b8f39f2f","slug":"/p/97f699ed-2b77-40cf-a3a4-7b41b8f39f2f"},"sidebar":"tutorialSidebar","previous":{"title":"Let\u2019s Build GPT","permalink":"/notes/docs/p/8c0b35bd-57b9-4149-b6fe-f6a60b69e248"},"next":{"title":"Introduction","permalink":"/notes/docs/p/dcc7f43c-a541-49f9-b9a0-7b6c92d6ad5f"}}');var r=t(74848),i=t(28453);const o={created_at:"2025-11-02",page_link:"/p/97f699ed-2b77-40cf-a3a4-7b41b8f39f2f",slug:"/p/97f699ed-2b77-40cf-a3a4-7b41b8f39f2f"},a=void 0,l={},c=[{value:"<strong>Exercises:</strong>",id:"exercises",level:3},{value:"<strong>Links:</strong>",id:"links",level:3},{value:"<strong>Supplementary links:</strong>",id:"supplementary-links",level:3}];function h(e){const n={a:"a",h3:"h3",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely."}),"\n",(0,r.jsx)(n.h3,{id:"exercises",children:(0,r.jsx)(n.strong,{children:"Exercises:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advised flow"}),": Reference this document and try to implement the steps before watching the partial solutions in the video. If you get stuck, full solutions are available in the minbpe code:"]}),"\n",(0,r.jsxs)(n.p,{children:["\ud83d\udc49 ",(0,r.jsx)(n.a,{href:"https://github.com/karpathy/minbpe/blob/master/exercise.md",children:"https://github.com/karpathy/minbpe/blob/master/exercise.md"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"links",children:(0,r.jsx)(n.strong,{children:"Links:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Google Colab for the video"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing",children:"https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"GitHub repo for the video (minBPE)"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://github.com/karpathy/minbpe",children:"https://github.com/karpathy/minbpe"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Playlist for the Zero to Hero series"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ",children:"https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Discord channel"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://discord.gg/3zy8kqD9Cp",children:"https://discord.gg/3zy8kqD9Cp"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Karpathy\u2019s Twitter"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://twitter.com/karpathy",children:"https://twitter.com/karpathy"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"supplementary-links",children:(0,r.jsx)(n.strong,{children:"Supplementary links:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tiktokenizer"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://tiktokenizer.vercel.app",children:"https://tiktokenizer.vercel.app"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tiktoken (OpenAI)"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://github.com/openai/tiktoken",children:"https://github.com/openai/tiktoken"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"SentencePiece (Google)"}),":"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://github.com/google/sentencepiece",children:"https://github.com/google/sentencepiece"})}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(h,{...e})}):h(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var s=t(96540);const r={},i=s.createContext(r);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);