"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[39664],{14362:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"Zero To Hero/Let\u2019s Build GPT","title":"Let\u2019s Build GPT","description":"We build a Generatively Pretrained Transformer (GPT), following the paper \\"Attention is All You Need\\" and OpenAI\'s GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.","source":"@site/docs/04. Zero To Hero/08. Let\u2019s Build GPT.md","sourceDirName":"04. Zero To Hero","slug":"/p/8c0b35bd-57b9-4149-b6fe-f6a60b69e248","permalink":"/notes/docs/p/8c0b35bd-57b9-4149-b6fe-f6a60b69e248","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/04. Zero To Hero/08. Let\u2019s Build GPT.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/8c0b35bd-57b9-4149-b6fe-f6a60b69e248","slug":"/p/8c0b35bd-57b9-4149-b6fe-f6a60b69e248"},"sidebar":"tutorialSidebar","previous":{"title":"Makemore 5 - WaveNet","permalink":"/notes/docs/p/f6874608-fc80-4c3b-98aa-4ac3eb261e37"},"next":{"title":"Let\'s Build GPT Tokenizer","permalink":"/notes/docs/p/97f699ed-2b77-40cf-a3a4-7b41b8f39f2f"}}');var a=t(74848),s=t(28453);const r={created_at:"2025-11-02",page_link:"/p/8c0b35bd-57b9-4149-b6fe-f6a60b69e248",slug:"/p/8c0b35bd-57b9-4149-b6fe-f6a60b69e248"},l="Introduction",o={},d=[{value:"1.1 Function for Getting Input and Output",id:"11-function-for-getting-input-and-output",level:2},{value:"Step 1.2: Get BiGram Language Model That can Generate Text",id:"step-12-get-bigram-language-model-that-can-generate-text",level:2},{value:"Step 1.3 Create Train and Validation Set, and train the model",id:"step-13-create-train-and-validation-set-and-train-the-model",level:2},{value:"Step 1.4: Update the code to see train and validation loss during training",id:"step-14-update-the-code-to-see-train-and-validation-loss-during-training",level:2},{value:"2.1: self-attention trick: matrix multiplication",id:"21-self-attention-trick-matrix-multiplication",level:2},{value:"7.1 Compare BatchNorm and LayerNorm",id:"71-compare-batchnorm-and-layernorm",level:2},{value:"Summary:",id:"summary",level:3},{value:"BatchNorm",id:"batchnorm",level:3},{value:"7.2 LayerNorm \u52a0\u5728\u54ea\u91cc",id:"72-layernorm-\u52a0\u5728\u54ea\u91cc",level:2},{value:"Other tokenizers:",id:"other-tokenizers",level:2},{value:"nn.Flatten &amp; nn.Unflatten",id:"nnflatten--nnunflatten",level:2},{value:"nn.Flatten: put a &gt;2 dimension data to 2 dimension.",id:"nnflatten-put-a-2-dimension-data-to-2-dimension",level:3},{value:"nn.Unflatten: put a 2 dimension data to &gt; 2 dimension",id:"nnunflatten-put-a-2-dimension-data-to--2-dimension",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042322002.png",alt:""})}),"\n",(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"introduction",children:"Introduction"})}),"\n",(0,a.jsx)(n.p,{children:'We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI\'s GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.'}),"\n",(0,a.jsx)(n.p,{children:"Links:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["youtube: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8",children:"https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8"})]}),"\n",(0,a.jsxs)(n.li,{children:["Google colab for the video: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2tTOHpVU25NUXZQelBabmJBOEpPUU9RT3hVd3xBQ3Jtc0ttN1JTOW9leUFnRENiUzhoNnR6cHlnQ2lUMDN6a1lSYzJpZzNKZTYzdGVpWEduOEV4WlVKSnEzYTdJeGlIZWxEVkxrT3p3MW9lNnh6TFk3LWVKUGF0MnhDUE5xaGwxS1pJV1haRlE4Vml3MWxneUkwdw&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-%3Fusp%3Dsharing&v=kCc8FmEb1nY"}),(0,a.jsx)(n.a,{href:"https://colab.research.google.com/dri",children:"https://colab.research.google.com/dri"}),"..."]}),"\n",(0,a.jsxs)(n.li,{children:["GitHub repo for the video: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbFkyVXFBMWtOWnlGR3lZNkpreWxrcTNSS1gzZ3xBQ3Jtc0trMGIyN1FtdmtoY05PajZ3Y3hFMFh5Y1BoWUNxdTJlTDNTQ1ZvVWNPdUtKeXRLZGdWNHFWZkJiTldqMldMQmp6VFowQ2VpM2oxU2pUT2FWSHkySFJOVVJCNVJ4MjZvN2JEZzE5QmlYT2J1X2tyMjNkZw&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fng-video-lecture&v=kCc8FmEb1nY"}),(0,a.jsx)(n.a,{href:"https://github.com/karpathy/ng-video-",children:"https://github.com/karpathy/ng-video-"}),"..."]}),"\n",(0,a.jsxs)(n.li,{children:["Playlist of the whole Zero to Hero series so far: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1&t=0s",children:"\u2022\xa0The\xa0spelled-out\xa0intro\xa0to\xa0neural\xa0netwo..."})]}),"\n",(0,a.jsxs)(n.li,{children:["nanoGPT repo: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbGRVc29uUmsyR1BpR3c4bXR3cllvMnFXZ0lIUXxBQ3Jtc0ttalBuQTA5Ny1KeHp0TDAwLXJqNTRJTFdka0k5YjV6NVUyVjh2NHNRbE9KZDdubm45X2J0eW1fbGVNM1o2bktha3FYbHZ1M3VlcHJIeGFNQkkybjYwOWNCcmdVVl94OHNldmcyTTFfZ2FDVzdlYXJPVQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2FnanoGPT&v=kCc8FmEb1nY"}),(0,a.jsx)(n.a,{href:"https://github.com/karpathy/nanoGPT",children:"https://github.com/karpathy/nanoGPT"})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Supplementary links:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Attention is All You Need paper: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWphR2h6SDRXXzhYZ19GaEVGSDZHRnVGREJ5Z3xBQ3Jtc0tuT0JpVWhzWHNodWRQNUQzdUE5X1VmRVc2NWlRYVVZcF9RYzhmQjlnWjNFdTVFQl9sUzFvVlpnRHkwWFdSZFJTUFI5anVwZ0swSjN1dDFvQWp3T0drSE1QYk9PVnJEazNqeDMxa3RxMl9Mc1ktUjlPTQ&q=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762&v=kCc8FmEb1nY"}),(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/1706.03762",children:"https://arxiv.org/abs/1706.03762"})]}),"\n",(0,a.jsxs)(n.li,{children:["OpenAI GPT-3 paper: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbEUzWFJ1a0Q5RDNId1BpM1RXanZGeDR4S2JHQXxBQ3Jtc0tuM1RsTXRWb2xjWjM1RkdQOEpKYXBGYUNfM0RyMmVobXhyRGJ0SF9QOHZMUTdsOVRWM2hObnhRa2NHNkRTWFNxM1lobGl6dGsxUVFUY092VjdXVEdQS1owMV9BZzltMU5ET2FOenNjN3hmVGY0cWRsbw&q=https%3A%2F%2Farxiv.org%2Fabs%2F2005.14165&v=kCc8FmEb1nY"}),(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2005.14165",children:"https://arxiv.org/abs/2005.14165"})]}),"\n",(0,a.jsxs)(n.li,{children:["OpenAI ChatGPT blog post: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjlhS0dRNlBNNlRIb1FDVkM2N2lTcHAxbFVTQXxBQ3Jtc0ttUHlGZXBjR1dBQWdGVkdpTmp0Nzg4cnZqdmRzazBVdFRPZjkwczhtVFNiWjl4QWhYMGZuVzc1VU9Qd0tBMGtCdlZJMnZnYzZSdk5USGtmQ0xWd3ByX29fYTc2UEo3LUxyMmthVU9kWTFZbXdPQlhBcw&q=https%3A%2F%2Fopenai.com%2Fblog%2Fchatgpt%2F&v=kCc8FmEb1nY"}),(0,a.jsx)(n.a,{href:"https://openai.com/blog/chatgpt/",children:"https://openai.com/blog/chatgpt/"})]}),"\n",(0,a.jsxs)(n.li,{children:["The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbkEzNGVtTEgyT3o4SFlKMkpxRE0tR0Fqc0hHZ3xBQ3Jtc0trLTV4ckswTkZNLXoxeHd6YWY2ZVM1bjg4M2wzY1BuYXpzZ3hFSUpaOXB2bHFBMzdFYVhjRHJsRjhrdERLQnYwczZBZkhyNFplci00cWhYV0FaXzNiOGVyeEtzaEZPNVFGQ014dmlaS0xDQUFWQTBMdw&q=https%3A%2F%2Flambdalabs.com%2F&v=kCc8FmEb1nY"}),(0,a.jsx)(n.a,{href:"https://lambdalabs.com",children:"https://lambdalabs.com"})]}),"\n",(0,a.jsx)(n.li,{children:"If you prefer to work in notebooks, I think the easiest path today is Google Colab."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Suggested exercises:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["EX1: The n-dimensional tensor mastery challenge: Combine the ",(0,a.jsx)(n.code,{children:"Head"})," and ",(0,a.jsx)(n.code,{children:"MultiHeadAttention"})," into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT)."]}),"\n",(0,a.jsx)(n.li,{children:"EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)"}),"\n",(0,a.jsx)(n.li,{children:"EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?"}),"\n",(0,a.jsx)(n.li,{children:"EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"}),"\n"]}),"\n",(0,a.jsx)(n.h1,{id:"step-1-define-input-and-output-and-generate-output-using-a-bigram-language-model",children:"Step 1: Define Input and Output, and Generate Output using a BiGram Language Model"}),"\n",(0,a.jsx)(n.p,{children:"\u6211\u4eectrain\u7684\u76ee\u6807\u662f\uff1a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'for the sentence "First Citizen:"\ntarget input and output for block_size = 8 (meaning max input length = 8):\nF -> i\nFi -> r\nFir -> s\nFirs -> t\nFirst -> ` `\nFirst -> C\nFirst C -> i\nFirst Ci -> t\n'})}),"\n",(0,a.jsx)(n.p,{children:"i.e., \u6211\u4eec\u4f18\u5316parameters\u7684\u65b9\u6cd5\u662f"}),"\n",(0,a.jsx)(n.p,{children:"\u5982\u679cinput\u662f First Cit"}),"\n",(0,a.jsx)(n.p,{children:"\u6211\u4eec\u4f18\u5316parameters\uff0c\u8ba9"}),"\n",(0,a.jsx)(n.p,{children:"x = F \u65f6 y = i"}),"\n",(0,a.jsx)(n.p,{children:"x = Fi \u65f6 y = r"}),"\n",(0,a.jsx)(n.p,{children:"\u2026"}),"\n",(0,a.jsx)(n.p,{children:"x = First Ci \u65f6 y = t"}),"\n",(0,a.jsx)(n.p,{children:"\u5bf9\u4e8estep 1\uff0c\u6211\u4eec\u8fd8\u6ca1\u6709\u638c\u63e1\u8fd9\u79cd\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u7684input\u7684\u65b9\u6cd5\uff0c"}),"\n",(0,a.jsx)(n.p,{children:"\u6240\u4ee5\uff0c\u6211\u4eec\nx = F \u65f6  y = i"}),"\n",(0,a.jsx)(n.p,{children:"x = Fi \u65f6 \u53ea\u5904\u7406 x=i\uff0c y = r"}),"\n",(0,a.jsx)(n.p,{children:"\u2026"}),"\n",(0,a.jsx)(n.p,{children:"x = First C \u65f6\u53ea\u5904\u7406x=C y = i"}),"\n",(0,a.jsx)(n.p,{children:"x = First Ci \u65f6\u53ea\u5904\u7406x=i y = t"}),"\n",(0,a.jsx)(n.h2,{id:"11-function-for-getting-input-and-output",children:"1.1 Function for Getting Input and Output"}),"\n",(0,a.jsx)(n.p,{children:"For batch size = 2, this is what input and output looks like"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\n\nwith open("ng-video-lecture/input.txt", encoding=\'utf-8\', mode="r") as f:\n    text = f.read()\nprint(text[:100])\n\nitos = sorted(list(set(text)))\nstoi = {c:i for i, c in enumerate(itos)}\n\nencode = lambda text:[stoi[c] for c in text]\ndecode = lambda lst:"".join([itos[i] for i in lst])\nblock_size = 8\n\ndef get_x_y(ix):\n    """\n    for the sentence "First Citizen:"\n    target input and output for block_size = 8 (meaning max input length = 8):\n    F -> i\n    Fi -> r\n    Fir -> s\n    Firs -> t\n    First -> ` `\n    First -> C\n    First C -> i\n    First Ci -> t\n    """\n    encoded = encode(text[ix:ix + block_size + 2])\n    x_data, y_data = [], []\n    for i in range(block_size):\n        x_data.append(encoded[:i+1])\n        y_data.append(encoded[i+1])\n    print(f\'{x_data=}\')\n    print(f\'{y_data=}\')\n    return x_data, y_data\n\ndef get_batch(batch_size):\n    """\n    e.g., if batch_size = 32, and block_size = 8,\n    then we select 32 places, where we take 9 character_sequence each.\n    """\n    ixs = torch.randint(len(text), (batch_size,))\n    for ix in ixs:\n        get_x_y(ix)\n\nget_batch(2)\n\n'})}),"\n",(0,a.jsx)(n.p,{children:"output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"x_data=[[6], [6, 1], [6, 1, 39], [6, 1, 39, 58], [6, 1, 39, 58, 1], [6, 1, 39, 58, 1, 58], [6, 1, 39, 58, 1, 58, 46], [6, 1, 39, 58, 1, 58, 46, 43]]\ny_data=[1, 39, 58, 1, 58, 46, 43, 1]\nx_data=[[53], [53, 1], [53, 1, 61], [53, 1, 61, 47], [53, 1, 61, 47, 58], [53, 1, 61, 47, 58, 46], [53, 1, 61, 47, 58, 46, 1], [53, 1, 61, 47, 58, 46, 1, 42]]\ny_data=[1, 61, 47, 58, 46, 1, 42, 43]\n"})}),"\n",(0,a.jsx)(n.h2,{id:"step-12-get-bigram-language-model-that-can-generate-text",children:"Step 1.2: Get BiGram Language Model That can Generate Text"}),"\n",(0,a.jsx)(n.p,{children:"It only has one layer, which is the embedding layer."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class BiGramModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Embedding(vocab_size, vocab_size),\n            nn.Flatten(0, 1)\n        )\n        for layer in self.layers:\n            layer.register_forward_hook(lambda layer, input, output:\n                                        print(f"{layer.__class__.__name__:20} | in: {input[0].shape} | out: {output.shape}"))\n\n    def forward(self, X):\n        logits = self.layers.forward(X)\n        return logits\n\n    def generate_next(self, X, iter_i, iter_limit):\n        "input is a 1-dimensional tensor, but the first dimension is always 1. e.g., 1 * 4"\n        if iter_i == iter_limit:\n            return X\n        iter_i += 1\n        logits = self.forward(X[:, -1:])  # we only need 1 input char i.e., the last one\n        probs = torch.softmax(logits, dim=-1)\n        ix = torch.multinomial(probs, num_samples=1)\n        X = torch.concat([X, ix.view(1, 1)], dim=-1)\n        return self.generate_next(X, iter_i + 1, iter_limit)\n\n    def generate(self, X=torch.tensor([[0]]), iter_limit=100):\n        x_gen = self.generate_next(X, 0, iter_limit)\n        print(f"{x_gen=}")\n        print(decode(x_gen.squeeze().tolist()))\n\nm = BiGramModel(vocab_size=vocab_size)\nm.generate()\n\n'})}),"\n",(0,a.jsx)(n.p,{children:"output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"x_gen=tensor([[ 0, 4, 35, 5, 43, 29, 46, 62, 1, 27, 45, 50, 3, 23, 56, 27, 47, 57, 47, 24, 37, 35, 5, 28, 7, 41, 49, 7, 42, 6, 36, 26, 15, 44, 0, 34, 57, 37, 22, 23, 48, 58, 9, 29, 33, 58, 2, 49, 36, 31, 32]]) &W'eQhx Ogl$KrOisiLYW'P-ck-d,XNCf VsYJKjt3QUt!kXST\n"})}),"\n",(0,a.jsx)(n.h2,{id:"step-13-create-train-and-validation-set-and-train-the-model",children:"Step 1.3 Create Train and Validation Set, and train the model"}),"\n",(0,a.jsx)(n.p,{children:"Below is the full code, note that I updated the get_x_y function for this use case."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\nfrom torch import nn\n\nwith open("ng-video-lecture/input.txt", encoding=\'utf-8\', mode="r") as f:\n    text = f.read()\nprint(text[:100])\n\nitos = sorted(list(set(text)))\nstoi = {c: i for i, c in enumerate(itos)}\nvocab_size = len(itos)\nprint(f"{vocab_size=}")\n\nencode = lambda text: [stoi[c] for c in text]\ndecode = lambda lst: "".join([itos[i] for i in lst])\nblock_size = 8\n\ndef get_x_y(ix, text):\n    """\n    for the sentence "First Citizen:" withix = 0\n    target input and output for block_size = 8 (meaning max input length = 8):\n    First Ci -> irst Cit\n    """\n    encoded = encode(text[ix:ix + block_size + 2])\n    x_data, y_data = encoded[:block_size], encoded[1:block_size + 1]\n    return torch.tensor(x_data, dtype=torch.int32), torch.tensor(y_data, dtype=torch.int64)\n\ndef get_batch(batch_size, text):\n    """\n    e.g., if batch_size = 32, and block_size = 8,\n    then we select 32 places, where we take 9 character_sequence each.\n    """\n    ixs = torch.randint(len(text) - block_size - 2, (batch_size,))\n    x_data_lst, y_data_lst = [], []\n    for ix in ixs:\n        x_data, y_data = get_x_y(ix, text)\n        x_data_lst.append(x_data)\n        y_data_lst.append(y_data)\n    return torch.stack(x_data_lst), torch.stack(y_data_lst)\n\nclass BiGramModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Embedding(vocab_size, vocab_size),\n            nn.Flatten(0, 1)\n        )\n        # for layer in self.layers:\n        #     layer.register_forward_hook(lambda layer, input, output:\n        #                                 print(f"{layer.__class__.__name__:20} | in: {input[0].shape} | out: {output.shape}"))\n\n    def forward(self, X):\n        logits = self.layers.forward(X)\n        return logits\n\n    def generate_next(self, X, iter_i, iter_limit):\n        "input is a 1-dimensional tensor, but the first dimension is always 1. e.g., 1 * 4"\n        if iter_i == iter_limit:\n            return X\n        iter_i += 1\n        logits = self.forward(X[:, -1:])  # we only need 1 input char i.e., the last one\n        probs = torch.softmax(logits, dim=-1)\n        ix = torch.multinomial(probs, num_samples=1)\n        X = torch.concat([X, ix.view(1, 1)], dim=-1)\n        return self.generate_next(X, iter_i + 1, iter_limit)\n\n    def generate(self, X=torch.tensor([[0]]), iter_limit=1000):\n        x_gen = self.generate_next(X, 0, iter_limit)\n        print(decode(x_gen.squeeze().tolist()))\n\nm = BiGramModel(vocab_size=vocab_size)\n# m.generate()\n\ntrain_sep_n = int(len(text) * 0.9)\ntrain_text = text[:train_sep_n]\nval_text = text[train_sep_n:]\n\nbatch_size = 32\nNUM_EPOCH = 1000\noptimizer = torch.optim.Adam(params=m.parameters(),lr=0.01)\nloss_fn = nn.CrossEntropyLoss()\nlossi = []\ndef train():\n    m.train()\n    for _ in range(NUM_EPOCH):\n        optimizer.zero_grad()\n        X, Y = get_batch(batch_size=batch_size, text=train_text)\n        logits = m.forward(X)\n        y_flatten = Y.flatten(0,1)\n        loss = loss_fn(input=logits,target=y_flatten)\n        loss.backward()\n        print(loss.log10().item())\n        lossi.append(loss.log10().item())\n        optimizer.step()\n\ntrain()\nimport matplotlib.pyplot as plt\nplt.plot(lossi)\nplt.show()\nm.generate()\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Below is the plotted lossi\n",(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042325153.png",alt:""})]}),"\n",(0,a.jsx)(n.p,{children:"Below is the generated text, notice that it\u2019s making more sense now."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"KEOf, to w3upigginde sturs.\nWilo mu K$LABu ngrrousKIncom\nZ'tofothotra the Yert ca g, tho&Grengon my, s, bke aimer mure ten d'vea t a n\nBar; sh?\n\n\nD absull ast les rs s gin wid Who f thinoll mig,\nWhir g ffued my.\nICKES:\n"})}),"\n",(0,a.jsx)(n.h2,{id:"step-14-update-the-code-to-see-train-and-validation-loss-during-training",children:"Step 1.4: Update the code to see train and validation loss during training"}),"\n",(0,a.jsx)(n.p,{children:"The key code is this one:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"@torch.no_grad()\ndef estimate_loss(m, eval_iter=100):\n    m.eval()\n    train_loss_lst, val_loss_lst = [], []\n    for _ in range(eval_iter):\n        train_loss = estimate_batch_loss(m, train_text).log10().item()\n        val_loss = estimate_batch_loss(m, val_text).log10().item()\n        train_loss_lst.append(train_loss)\n        val_loss_lst.append(val_loss)\n    m.train()\n    return sum(train_loss_lst)/eval_iter, sum(val_loss_lst)/eval_iter\n\ndef estimate_batch_loss(m, text):\n    X, Y = get_batch(batch_size=batch_size, text=text)\n    logits = m.forward(X)\n    y_flatten = Y.flatten(0, 1)\n    loss = loss_fn(input=logits, target=y_flatten)\n    return loss\n\n"})}),"\n",(0,a.jsx)(n.p,{children:"Below is the full code:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\nfrom torch import nn\n\nwith open("ng-video-lecture/input.txt", encoding=\'utf-8\', mode="r") as f:\n    text = f.read()\nprint(text[:100])\n\nitos = sorted(list(set(text)))\nstoi = {c: i for i, c in enumerate(itos)}\nvocab_size = len(itos)\nprint(f"{vocab_size=}")\n\nencode = lambda text: [stoi[c] for c in text]\ndecode = lambda lst: "".join([itos[i] for i in lst])\nblock_size = 8\n\ndef get_x_y(ix, text):\n    """\n    for the sentence "First Citizen:" withix = 0\n    target input and output for block_size = 8 (meaning max input length = 8):\n    First Ci -> irst Cit\n    """\n    encoded = encode(text[ix:ix + block_size + 2])\n    x_data, y_data = encoded[:block_size], encoded[1:block_size + 1]\n    return torch.tensor(x_data, dtype=torch.int32), torch.tensor(y_data, dtype=torch.int64)\n\ndef get_batch(batch_size, text):\n    """\n    e.g., if batch_size = 32, and block_size = 8,\n    then we select 32 places, where we take 9 character_sequence each.\n    """\n    ixs = torch.randint(len(text) - block_size - 2, (batch_size,))\n    x_data_lst, y_data_lst = [], []\n    for ix in ixs:\n        x_data, y_data = get_x_y(ix, text)\n        x_data_lst.append(x_data)\n        y_data_lst.append(y_data)\n    return torch.stack(x_data_lst), torch.stack(y_data_lst)\n\nclass BiGramModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Embedding(vocab_size, vocab_size),\n            nn.Flatten(0, 1)\n        )\n        # for layer in self.layers:\n        #     layer.register_forward_hook(lambda layer, input, output:\n        #                                 print(f"{layer.__class__.__name__:20} | in: {input[0].shape} | out: {output.shape}"))\n\n    def forward(self, X):\n        logits = self.layers.forward(X)\n        return logits\n\n    def generate_next(self, X, iter_i, iter_limit):\n        "input is a 1-dimensional tensor, but the first dimension is always 1. e.g., 1 * 4"\n        if iter_i == iter_limit:\n            return X\n        iter_i += 1\n        logits = self.forward(X[:, -1:])  # we only need 1 input char i.e., the last one\n        probs = torch.softmax(logits, dim=-1)\n        ix = torch.multinomial(probs, num_samples=1)\n        X = torch.concat([X, ix.view(1, 1)], dim=-1)\n        return self.generate_next(X, iter_i + 1, iter_limit)\n\n    def generate(self, X=torch.tensor([[0]]), iter_limit=1000):\n        x_gen = self.generate_next(X, 0, iter_limit)\n        print(decode(x_gen.squeeze().tolist()))\n\nm = BiGramModel(vocab_size=vocab_size)\n# m.generate()\n\ntrain_sep_n = int(len(text) * 0.9)\ntrain_text = text[:train_sep_n]\nval_text = text[train_sep_n:]\n\nbatch_size = 32\nNUM_EPOCH = 10000\noptimizer = torch.optim.Adam(params=m.parameters(),lr=0.01)\nloss_fn = nn.CrossEntropyLoss()\nlossi = []\n\n@torch.no_grad()\ndef estimate_loss(m, eval_iter=100):\n    m.eval()\n    train_loss_lst, val_loss_lst = [], []\n    for _ in range(eval_iter):\n        train_loss = estimate_batch_loss(m, train_text).log10().item()\n        val_loss = estimate_batch_loss(m, val_text).log10().item()\n        train_loss_lst.append(train_loss)\n        val_loss_lst.append(val_loss)\n    m.train()\n    return sum(train_loss_lst)/eval_iter, sum(val_loss_lst)/eval_iter\n\ndef estimate_batch_loss(m, text):\n    X, Y = get_batch(batch_size=batch_size, text=text)\n    logits = m.forward(X)\n    y_flatten = Y.flatten(0, 1)\n    loss = loss_fn(input=logits, target=y_flatten)\n    return loss\n\ntrain_lossi, val_lossi = [], []\ndef train():\n    m.train()\n    for i in range(NUM_EPOCH):\n        optimizer.zero_grad()\n        loss = estimate_batch_loss(m, train_text)\n        loss.backward()\n        if i % 100 == 0:\n            cur_train_loss, cur_val_loss = estimate_loss(m)\n            train_lossi.append(cur_train_loss)\n            val_lossi.append(cur_val_loss)\n        optimizer.step()\n\ntrain()\nimport matplotlib.pyplot as plt\nplt.plot(train_lossi)\nplt.show()\n\nplt.plot(val_lossi)\nplt.show()\nm.generate()\n'})}),"\n",(0,a.jsx)(n.p,{children:"Now it gives 2 much smoother plots"}),"\n",(0,a.jsxs)(n.p,{children:["train loss:\n",(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042326147.png",alt:""})]}),"\n",(0,a.jsxs)(n.p,{children:["Validation loss:\n",(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042326319.png",alt:""})]}),"\n",(0,a.jsx)(n.h1,{id:"step-2-build-self-attention",children:"Step 2: Build Self-Attention"}),"\n",(0,a.jsx)(n.h2,{id:"21-self-attention-trick-matrix-multiplication",children:"2.1: self-attention trick: matrix multiplication"}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.em,{children:["Question 1: let\u2019s say we have an X that\u2019s ",(0,a.jsx)(n.code,{children:"[1,2,3,4]"}),", how do we update it to ",(0,a.jsx)(n.code,{children:"[1,3,6,10]"}),"?"]})}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Motivation: we want tokens to communicate to their past.\n",(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507171857638.png",alt:""})]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["Follow-up question 2, let\u2019s say X is ",(0,a.jsx)(n.code,{children:"[[[1,2,3,4]] * 10]"}),", how do we simultaneously update all of them to ",(0,a.jsx)(n.code,{children:"[[[1,3,6,10]] * 10]"})," ?"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["\u76f4\u63a5\u7528 ",(0,a.jsx)(n.code,{children:"[[[1,2,3,4] ]* 10]"}),"  \u521a\u521a\u90a3\u4e2a\u4e2d\u95f4\u90a3\u4e2amatrix"]}),"\n",(0,a.jsx)(n.p,{children:"\u7528torch\u8868\u793a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"t = torch.tensor([[1,2,3,4] for _ in range(10)], dtype=torch.float32)\nmul = torch.tril(torch.ones((4,4),dtype=torch.float32)).T\n\nres = t @ mul\nprint(res)\nprint(res.shape)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507171903944.png",alt:""})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507171904629.png",alt:""})}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"Question 3: What if X is of dimension: 10 * 4 * 3, and we want to get the sum of the 1th dimension, i,e, the dimension of 1,2,3,4?"}),"\n",(0,a.jsxs)(n.p,{children:["10 * 4 * 3 ",(0,a.jsx)(n.code,{children:"@"})," 4* 4, this does not work, even if torch add a batch dimension so that 4 * 4 becomes 10 * 4 * 4"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Let\u2019s think about how to do it the other way around."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"[[1,0,0,0],   * [1,2,3,4]  = [[1],[3],[6],[10]]\n[1,1,0,0],      \n[1,1,1,0],      \n[1,1,1,0]]    \n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"torch.tril(torch.ones(4,4))"})," * ",(0,a.jsx)(n.code,{children:"10\u4e2a[1,2,3,4]"})," \u8f6c\u7f6e\uff0c\u5c31\u7b49\u4e8e10\u4e2a ",(0,a.jsx)(n.code,{children:"[1,3,6,10]"})]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"4 * 4 @ 10 * 4 *3"})}),"\n",(0,a.jsxs)(n.p,{children:["torch\u4f1a\u7ed9",(0,a.jsx)(n.code,{children:"4 * 4"}),"\u518d\u52a0\u4e00\u4e2abatch dimension\uff0c \u53d8\u6210 10 * 4 * 4"]}),"\n",(0,a.jsx)(n.p,{children:"\u5b9e\u9645\u4e0a\u7684\u8ba1\u7b97\u662f\u6bcf\u4e2abatch\u5185\u8fdb\u884c4_4 \u7684tril\u548c4_3\u7684matrix\u7684\u70b9\u4e58"}),"\n",(0,a.jsx)(n.p,{children:"\u6240\u4ee5\uff0c\u6700\u7ec8\u7684torch implementation\u662f\u8fd9\u6837"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"t = torch.randn(10,4,3)\nmul = torch.tril(torch.ones((4,4),dtype=torch.float32))\n\nres = mul @ t\nprint(res.shape) # torch.Size([10, 4, 3])\n"})}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"Question 4: \u6211\u4eec\u73b0\u5728\u9700\u8981\u7684\u662f\u4e00\u4e2aweight matrix\u7528\u6765\u6700\u7ec8\u548c \u5904\u7406\u8fc7\u7684x\u8fdb\u884c\u70b9\u4e58\uff0c \u8fd9\u4e2aweight matrix\u9700\u8981\u6ee1\u8db3\u8fd9\u4e9b\u6761\u4ef6\uff1a"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\u6bd4\u5982 I like green vegetable, \u73b0\u5728\u7684\u8868\u793a\u662f ",(0,a.jsx)(n.code,{children:"[[I0.3,I0.5], [like0.5,like0.5],[green0.6,green0.1], [vegetable0.9,vegetable0.1]]"})," \u8981\u53d8\u6210"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"  [[I-I1, 0,0,0],\n  [like-I0.3, like-like0.7,0,0,0]],\n  [green-I0.1, green-like0.1,green-green0.8,0,0]],\n  [vegetable-I0.05, vegetable-like0.15,vegetable-green0.6,vegetable-vegetable0.2]],\n"})}),"\n",(0,a.jsx)(n.p,{children:"\u6211\u4eec\u9700\u8981\u505a\u7684\u5c31\u662fquery \u548c key \u70b9\u4e58\uff0c"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"query\uff1a what I want"}),"\n",(0,a.jsx)(n.li,{children:"key: what I have."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"\u5bf9\u4e8e\u4ee5\u4e0a\u7684\u4f8b\u5b50\uff0c\u6211\u4eec\u9700\u8981"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"[[I0.3, like0.5, green0.6, vegetable0.9],   @  [[I0.3,I0.5], \n[I0.5, like0.6, green0.1, vegetable0.1]]        [like0.5,like0.5],\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[green0.6,green0.1], \n\t\t\t\t\t\t\t\t\t\t\t\t[vegetable0.9,vegetable0.1]]\n"})}),"\n",(0,a.jsx)(n.p,{children:"\u5bf9\u4e8e\u8fd9\u4e2a\u7ed3\u679c\uff0c\u628a\u672a\u6765\u672a\u77e5\u7684\u90e8\u5206mask\u6210inf\uff0c\u7136\u540e\u505asoftmax"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507171905835.png",alt:""})}),"\n",(0,a.jsx)(n.p,{children:"\u6ce8\u610f \u4e0b\u56fe\u7684-\u221e\u5e94\u8be5\u662f0\uff0c \u4e0b\u56fe\u5176\u5b9e\u662fsoftmax\u4e4b\u540e\u7684\u7ed3\u679c softmax\u6bcf\u4e00\u884c\u52a0\u8d77\u6765\u5e94\u8be5= 1"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507171906148.png",alt:""})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Why did we use softmax at the last step for ",(0,a.jsx)(n.code,{children:"q*k"}),"?\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\u8bb0\u5f97\u6211\u4eec\u5728backprop-ninja\u90a3\u4e2asession\u63a8\u5bfc\u8fc7negative log likelihood loss\u5bf9\u4e8e\u6bcf\u4e2a\u5143\u7d20\u7684\u5bfc\u6570\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\u5bf9\u4e8e\u6b63\u786e\u7684prediction\u662f1-p"}),"\n",(0,a.jsx)(n.li,{children:"\u5bf9\u4e8e\u9519\u8bef\u7684prediction\u662fp"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"\u6240\u4ee5\u6709\u4e86softmax\uff0c\u5b83\u53ef\u4ee5\u628aq*k\u5f80 \u8d8a\u6709\u5173\u7cfb\uff0c\u5c31\u8d8a\u9760\u8fd11\uff0c\u8d8a\u6ca1\u6709\u5173\u7cfb\uff0c\u5c31\u8d8a\u9760\u8fd10 \u8fd9\u4e2a\u65b9\u5411\u8fdb\u884cbackpropagation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"\u4ee5\u4e0a\u5f97\u5230\u662f\u65b0\u7684weight\uff0c \u518d\u4e58\u4ee5\u5904\u7406\u8fc7\u7684x"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\u6211\u4eec\u662f\u5426\u9700\u8981\u4e00\u76f4mask\u672a\u6765\u672a\u77e5\u7684\u90e8\u5206\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["mask\u6389\u672a\u6765\u672a\u77e5\u7684\u90e8\u5206\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4e0d\u518d\u5173\u5fc3 e.g., I-green\u7684\u5173\u7cfb\uff0c\u800c\u662f\u53ea\u5173\u5fc3green-I \u7684\u5173\u7cfb\uff0c\u5728transformer\u4e2d\u8fd9\u4e2a\u5904\u7406\u662f\u6709\u7528\u7684\uff0c\u4f46\u662f\u5982\u679c\u6bd4\u5982\u6211\u4eec\u662f\u505atext analysis\uff0ce.g.\uff0ctext classification\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0dmask\u6389\u672a\u6765\u7684\u672a\u77e5\u90e8\u5206\uff0c\u56e0\u4e3a\u6211\u4eec\u662f\u5173\u5fc3\u90a3\u4e9b\u5185\u5bb9\u7684\u3002\u8fd9\u4e2a\u5728torch\u7684\u5904\u7406\u4e0a\u5c31\u662f\u628a",(0,a.jsx)(n.code,{children:"decoder"})," \u53d8\u6210 ",(0,a.jsx)(n.code,{children:"encoder"}),", \u5c31\u4f1a\u53bb\u6389mask\u7684\u90a3\u4e00\u884c\u3002"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"torch implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"x = torch.randn(10,4,32)\nquery_layer = nn.Linear(32, 2, bias=False)\nkey_layer = nn.Linear(32, 2, bias=False)\nq = query_layer(x)\nk = key_layer(x)\nweight = q @ k.transpose(1,2)\ntril = torch.tril(torch.ones(4,4))\nweight = weight.masked_fill(tril == 0, -torch.inf) \nweight = weight.softmax(dim=-1)\nvalue_layer = nn.Linear(32, 2, bias=False)\nv = value_layer(x)\nout = weight @ v\nprint(weight[0])\nprint(out.shape)\n"})}),"\n",(0,a.jsx)(n.p,{children:"output"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n        [0.5654, 0.4346, 0.0000, 0.0000],\n        [0.3271, 0.2887, 0.3841, 0.0000],\n        [0.2639, 0.2359, 0.2198, 0.2805]], grad_fn=<SelectBackward0>)\ntorch.Size([10, 4, 2])\n"})}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"Question 5: during weight initialization, how do we make the weight less to the extreme? (remember, softmax makes bigger values even closer to 1)"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"answer:"}),"\n",(0,a.jsx)(n.p,{children:"we modify"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"weight = q @ k.transpose(1,2)\n"})}),"\n",(0,a.jsx)(n.p,{children:"to"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"weight = q @ k.transpose(1,2) / head_size ** -0.5\n"})}),"\n",(0,a.jsx)(n.p,{children:"in our case, head_size = 2."}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:"Summary"}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"I am a sentence."}),"\n",(0,a.jsx)(n.li,{children:"Here\u2019s what I am interested in (q)"}),"\n",(0,a.jsx)(n.li,{children:"Here\u2019s what I have (k)"}),"\n",(0,a.jsxs)(n.li,{children:["if I find some info interesting, here\u2019s what I present to you ",(0,a.jsx)(n.code,{children:"(weight @ v)"})]}),"\n"]}),"\n",(0,a.jsx)(n.h1,{id:"step-3-update-network-to-include-self-attention",children:"Step 3: Update Network to Include Self-Attention"}),"\n",(0,a.jsx)(n.p,{children:"\u8bb0\u5f97\u6211\u4eectrain\u7684\u76ee\u6807\u662f\uff1a"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'for the sentence "First Citizen:"\ntarget input and output for block_size = 8 (meaning max input length = 8):\nF -> i\nFi -> r\nFir -> s\nFirs -> t\nFirst -> ` `\nFirst -> C\nFirst C -> i\nFirst Ci -> t\n'})}),"\n",(0,a.jsxs)(n.p,{children:["\u800c\u4e14\uff0c\u901a\u8fc7\u4e0a\u9762 masked_fill \u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u5df2\u7ecf\u5177\u5907\u4e86\u53ea\u7ed9\u4e00\u4e2ainput ",(0,a.jsx)(n.code,{children:"First Ci"})," \u65f6\u4ea7\u751f\u4ee5\u4e0a\u5185\u5bb9\u7684\u6761\u4ef6\u3002"]}),"\n",(0,a.jsx)(n.p,{children:"\u53e6\u5916\uff0c\u8fd9\u6b21\u7684implementation\u8981\u8865\u5145\u4ee5\u4e0b\u5185\u5bb9"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"positional embedding: \u6211\u4eec\u8981\u7528\u8fd9\u4e2a\u6765\u8bb0\u5f55\u4f4d\u7f6e\u4fe1\u606f"}),"\n",(0,a.jsx)(n.li,{children:"predict\u7684\u65f6\u5019\uff0c\u53ea\u9700\u8981\u4fdd\u7559\u6700\u540e\u4e00\u884cpredict\u7684\u7ed3\u679c (First Ci -> t)"}),"\n",(0,a.jsxs)(n.li,{children:["\u8981\u7528 ",(0,a.jsx)(n.code,{children:"self.register_buffer(name=\u201dtril\u201d, tensor=torch.tril(torch.ones(4,4)))"})," \u6765\u8bb0\u5f55tril\u8fd9\u4e2aconstant\uff0c\u5b83\u4e0d\u662fparameter\uff0c\u4e0d\u9700\u8981backpropagate"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\n\nwith open("ng-video-lecture/input.txt", encoding=\'utf-8\', mode="r") as f:\n    text = f.read()\n\nitos = sorted(list(set(text)))\nstoi = {c: i for i, c in enumerate(itos)}\nvocab_size = len(itos)\nn_emb = 32\nprint(f"{vocab_size=}")\n\nencode = lambda text: [stoi[c] for c in text]\ndecode = lambda lst: "".join([itos[i] for i in lst])\nblock_size = 256\n\n\ndef get_x_y(ix, text):\n    """\n    for the sentence "First Citizen:" withix = 0\n    target input and output for block_size = 8 (meaning max input length = 8):\n    First Ci -> irst Cit\n    """\n    encoded = encode(text[ix:ix + block_size + 2])\n    x_data, y_data = encoded[:block_size], encoded[1:block_size + 1]\n    return torch.tensor(x_data, dtype=torch.int32), torch.tensor(y_data, dtype=torch.int64)\n\n\ndef get_batch(batch_size, text):\n    """\n    e.g., if batch_size = 32, and block_size = 8,\n    then we select 32 places, where we take 9 character_sequence each.\n    """\n    ixs = torch.randint(len(text) - block_size - 2, (batch_size,))\n    x_data_lst, y_data_lst = [], []\n    for ix in ixs:\n        x_data, y_data = get_x_y(ix, text)\n        x_data_lst.append(x_data)\n        y_data_lst.append(y_data)\n    return torch.stack(x_data_lst), torch.stack(y_data_lst)\n\n\n\n\nclass SelfAttentionHead(nn.Module):\n\n    def __init__(self, block_size, head_size):\n        # block_size is T, head_size is C\n        super().__init__()\n        self.block_size = block_size\n        self.head_size = head_size\n        self.register_buffer(name=\'tril\', tensor=torch.tril(torch.ones(block_size, block_size)))\n        # tril is constant, not parameter, should register here.\n\n\n    def forward(self, X):\n        # x = torch.randn(10, 4(block_size), 32)\n        B, T, C = X.shape\n        query_layer = nn.Linear(C, self.head_size, bias=False)\n        key_layer = nn.Linear(C, self.head_size, bias=False)\n        q = query_layer(X)\n        k = key_layer(X)\n        weight = q @ k.transpose(1, 2)\n        weight = weight.masked_fill(self.tril == 0, -torch.inf)\n        weight = weight.softmax(dim=-1)\n        value_layer = nn.Linear(C, self.head_size, bias=False)\n        v = value_layer(X)\n        out = weight @ v\n        return out\n\n\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size, block_size, n_emb):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n\n    def forward(self, X):\n        # X dim: 10, 4, n_emb=2\n        B, T = X.shape\n        token_emb = self.token_embedding_table(X) # 10, 4, 2\n        position_emb = self.position_embedding_table(torch.arange(0, T)) # 4, 2\n        return token_emb + position_emb # 10, 4, 2\n\nhead_size = 16\n\nclass NNModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            (\'embedding\', EmbeddingLayer(vocab_size, block_size, n_emb)),\n            (\'self_attention\', SelfAttentionHead(block_size, head_size)),\n            (\'lm_head\', nn.Linear(head_size, vocab_size)),\n            ])\n        )\n        # for layer in self.layers:\n        #     layer.register_forward_hook(lambda layer, input, output:\n        #                                 print(f"{layer.__class__.__name__:20} | in: {input[0].shape} | out: {output.shape}"))\n\n    def forward(self, X):\n        logits = self.layers.forward(X)\n        return logits\n\n    def generate_next(self, X, iter_i, iter_limit):\n        "input is a 1-dimensional tensor, but the first dimension is always 1. e.g., 1 * 4"\n        if iter_i == iter_limit:\n            return X\n        iter_i += 1\n        # X: [1, 8]\n        logits = self.forward(X[:, -8:]) # [1, 8, 16]\n        probs = torch.softmax(logits[:, -1, :].squeeze(), dim=-1)# [16]\n        ix = torch.multinomial(probs, num_samples=1)\n        X = torch.concat([X, ix.view(1, 1)], dim=-1)\n        return self.generate_next(X, iter_i + 1, iter_limit)\n\n    def generate(self, X=torch.tensor([[0 for _ in range(block_size)]]), iter_limit=1000):\n        x_gen = self.generate_next(X, 0, iter_limit)\n        print(decode(x_gen.squeeze().tolist()))\n\n\nm = NNModel()\n# m.generate()\ntrain_sep_n = int(len(text) * 0.9)\ntrain_text = text[:train_sep_n]\nval_text = text[train_sep_n:]\n\nbatch_size = 64\nNUM_EPOCH = 5000\noptimizer = torch.optim.Adam(params=m.parameters(),lr=0.0001)\nloss_fn = nn.CrossEntropyLoss()\nlossi = []\n\n@torch.no_grad()\ndef estimate_total_loss(m, eval_iter=100):\n    m.eval()\n    train_loss_lst, val_loss_lst = [], []\n    for _ in range(eval_iter):\n        train_loss = estimate_batch_loss(m, train_text).log10().item()\n        # print(f"{train_loss=}")\n        val_loss = estimate_batch_loss(m, val_text).log10().item()\n        # print(f"{val_loss=}")\n        train_loss_lst.append(train_loss)\n        val_loss_lst.append(val_loss)\n    m.train()\n    return sum(train_loss_lst)/eval_iter, sum(val_loss_lst)/eval_iter\n\ndef estimate_batch_loss(m, text):\n    X, Y = get_batch(batch_size=batch_size, text=text)\n    logits = m.forward(X).flatten(0,1)\n    loss = loss_fn(input=logits, target=Y.flatten(0,1))\n    return loss\n\n\ntrain_lossi, val_lossi = [], []\n\ndef train():\n    m.train()\n    for i in tqdm(range(NUM_EPOCH)):\n        optimizer.zero_grad()\n        loss = estimate_batch_loss(m, train_text)\n        loss.backward()\n        if i % 100 == 0:\n            cur_train_loss, cur_val_loss = estimate_total_loss(m)\n            train_lossi.append(cur_train_loss)\n            val_lossi.append(cur_val_loss)\n        optimizer.step()\n\n\ntrain()\nimport matplotlib.pyplot as plt\nplt.plot(train_lossi)\nplt.show()\n\nplt.plot(val_lossi)\nplt.show()\n\nprint("------ Final Train Loss: ---------\\n", 10**train_lossi[-1])\nprint("------ Final Validation Loss: ----\\n", 10**val_lossi[-1])\nm.generate()\n'})}),"\n",(0,a.jsx)(n.h1,{id:"step-4-change-to-multi-head-attention",children:"Step 4: Change to Multi-Head-Attention"}),"\n",(0,a.jsx)(n.p,{children:"For the same X, go through several independent attention layers to learn their inter-connections, and in the end concatenate the results at the last dimension."}),"\n",(0,a.jsxs)(n.p,{children:["for example, if the self-attention learns a ",(0,a.jsx)(n.code,{children:"1 * 4 * 16"})," matrix for [[i like green vegetables]],"]}),"\n",(0,a.jsxs)(n.p,{children:["if we use 4 heads, each then will learn a ",(0,a.jsx)(n.code,{children:"1 * 4 * 4"})," matrix for [[i like green vegetables]],"]}),"\n",(0,a.jsxs)(n.p,{children:["then we concatenate the results to ",(0,a.jsx)(n.code,{children:"1 * 4 * 16"})," ."]}),"\n",(0,a.jsx)(n.p,{children:"update the code to add the multi-Attention module"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.heads = nn.ModuleList([SelfAttentionHead(self.head_size) for _ in range(n_head)])\n\n    def forward(self, X):\n        return torch.cat([h(X) for h in self.heads], dim=-1)\n"})}),"\n",(0,a.jsx)(n.p,{children:"also change the self.layers"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"        self.layers = nn.Sequential(OrderedDict([\n            ('embedding', EmbeddingLayer(vocab_size, n_emb)),\n            # ('self_attention', SelfAttentionHead(block_size, head_size)),\n            ('multi_head_attention', MultiHeadAttention(n_head=4, head_size=head_size // 4)),\n            ('lm_head', nn.Linear(head_size, vocab_size)),\n        ])\n"})}),"\n",(0,a.jsx)(n.h1,{id:"step-5-feedforward-layer",children:"Step 5: FeedForward Layer"}),"\n",(0,a.jsx)(n.p,{children:"Between the multihead layer and the final linear layer to fix the output dimension (i.e., from 16 to the vocab size), we add a feedforward layer, which is basically just a linear + a relu layer"}),"\n",(0,a.jsx)(n.p,{children:"to do that, we define a feedforward layer:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"\nclass FeedForward(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('linear', nn.Linear(in_features, out_features)),\n            ('relu', nn.ReLU(out_features)),\n        ]))\n    def forward(self, X):\n        return self.layers.forward(X)\n"})}),"\n",(0,a.jsx)(n.p,{children:"and also change the self.layers"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"        self.layers = nn.Sequential(OrderedDict([\n            ('embedding', EmbeddingLayer(vocab_size, n_emb)),\n            # ('self_attention', SelfAttentionHead(block_size, head_size)),\n            ('multi_head_attention', MultiHeadAttention(n_head=4, head_size=head_size // 4)),\n            ('feed_forward', FeedForward(head_size, head_size)),\n            ('lm_head', nn.Linear(head_size, vocab_size)),\n        ])\n"})}),"\n",(0,a.jsx)(n.h1,{id:"step-6-use-a-repeated-block-for-multi-head-attention--feedforward-layer",children:"Step 6: Use a repeated Block for multi head attention + feedforward layer"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class Block(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('multi_head_attention', MultiHeadAttention(n_head, head_size=head_size // n_head)),\n            ('feed_forward', FeedForward(head_size, head_size)),\n        ]))\n    def forward(self, X):\n        return self.layers.forward(X)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"        self.layers = nn.Sequential(OrderedDict([\n            ('embedding', EmbeddingLayer(vocab_size, n_emb)),\n            ('block1', Block(n_head=4,head_size=head_size)),\n            ('block2', Block(n_head=4,head_size=head_size)),\n            ('block3', Block(n_head=4,head_size=head_size)),\n            ('lm_head', nn.Linear(head_size, vocab_size)),\n        ])\n"})}),"\n",(0,a.jsxs)(n.h1,{id:"step-6-resnet-very-important",children:["Step 6: ResNet (",(0,a.jsx)(n.em,{children:"VERY IMPORTANT"}),")"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"This step reduced my validation loss from 3.5 to 2.5 in ONE PASS!!"})}),"\n",(0,a.jsx)(n.p,{children:"Use residual connection to offset the issue of network becomes too deep and the direct, linear information from X to Y get lost through the deep network."}),"\n",(0,a.jsx)(n.p,{children:"in short, what it does is"}),"\n",(0,a.jsx)(n.p,{children:"Y = aX + b"}),"\n",(0,a.jsx)(n.p,{children:"Z = cY + d"}),"\n",(0,a.jsx)(n.p,{children:"we change that to:"}),"\n",(0,a.jsx)(n.p,{children:"Y = aX + b + X"}),"\n",(0,a.jsx)(n.p,{children:"Z = cY + d + Y"}),"\n",(0,a.jsx)(n.p,{children:"why is this helpful:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["addition allows gradients to distribute equally across branches\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"e.g., y = ab + ac, dy/da = b + c, addition allows the b and c to both be preserved during backward propagation."}),"\n",(0,a.jsxs)(n.li,{children:["This is helpful in the above case because it allows ",(0,a.jsx)(n.code,{children:"a"})," and ",(0,a.jsx)(n.code,{children:"b"})," to have a direct linear influence on Z, i.e., Z actually = cY + d + aX + b + X, it allows those information to flow through"]}),"\n",(0,a.jsxs)(n.li,{children:["even if we make it deeper and deeper, e.g., Y = aX + b + X, Z = cY + d + Y, Z2 = eZ + f + Z, etc, the direct relationship from final output Z2 to ",(0,a.jsx)(n.code,{children:"a"})," and ",(0,a.jsx)(n.code,{children:"b"})," does not get lost."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"implementation"}),"\n",(0,a.jsxs)(n.p,{children:["First, we need to make sure the MultiHeadAttention and the FeedForward layers indeed ",(0,a.jsx)(n.em,{children:(0,a.jsx)(n.strong,{children:"has a linear layer as the final layer"})})," flow out and preserve the linear information."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.heads = nn.ModuleList([\n            SelfAttentionHead(head_size=head_size) for _ in range(n_head)\n        ])\n        **self.proj = nn.Linear(head_size * n_head, head_size * n_head)**\n\n    def forward(self, X):\n        X = torch.cat([h(X) for h in self.heads], dim=-1)\n        X = self.proj(X)\n        return X\n\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class FeedForward(nn.Module):\n    def __init__(self, in_features, inner_layer_features, out_features):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('linear', nn.Linear(in_features, inner_layer_features)),\n            ('relu', nn.ReLU(inner_layer_features)),\n            **('proj', nn.Linear(inner_layer_features, out_features)),**\n\n        ]))\n    def forward(self, X):\n        return self.layers.forward(X)\n\n"})}),"\n",(0,a.jsx)(n.p,{children:"We also need to make sure in the Block part there is indeed a X that flows through the information"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class Block(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('multi_head_attention', MultiHeadAttention(n_head, head_size=head_size // n_head)),\n            ('feed_forward', FeedForward(head_size, head_size*4, head_size)),\n        ]))\n    def forward(self, X):\n        for layer in self.layers:\n            **X = X + layer(X)**\n        return X\n"})}),"\n",(0,a.jsx)(n.p,{children:"We don\u2019t need to change anything in the actual model part"}),"\n",(0,a.jsx)(n.p,{children:"Using this method, my loss dramatically reduced from"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"------ Final Train Loss: ---------\n 3.3046764334475265\n------ Final Validation Loss: ----\n 3.350014769068994\n"})}),"\n",(0,a.jsx)(n.p,{children:"to"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"------ Final Train Loss: ---------\n 2.493276333727324\n------ Final Validation Loss: ----\n 2.5093250995415595\n"})}),"\n",(0,a.jsx)(n.h1,{id:"step-7-layer-normalization",children:"Step 7: Layer Normalization"}),"\n",(0,a.jsx)(n.p,{children:"in batch normalization, over (B, T, C), we normalized on the B dimension"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\u5bf9\u4e8e\u6bcf\u4e00\u4e2abatch (e.g., B, T, C: 32 * 4 * 2)"}),"\n",(0,a.jsx)(n.li,{children:"\u6211\u4eec \u7528 4 * 2 \u4e2agamma\uff0c 4*2 \u4e2abeta\u4f5c\u4e3aparameter"}),"\n",(0,a.jsx)(n.li,{children:"\u8fd8\u8bb0\u5f97\u5417\uff1f"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"in layer normalization, over (B, T, C), we normalize on the T dimension."}),"\n",(0,a.jsx)(n.h2,{id:"71-compare-batchnorm-and-layernorm",children:"7.1 Compare BatchNorm and LayerNorm"}),"\n",(0,a.jsx)(n.h3,{id:"summary",children:"Summary:"}),"\n",(0,a.jsx)(n.p,{children:"\u4ee5\u4e0b\u7528\u8ba1\u7b97\u6765\u8868\u8fbe\uff1a\u5bf9\u4e8einput B,T,C,"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["batchnorm\u53d6\u7684\u662f B,T \u7684\u7ef4\u5ea6\u4e0a\u7684\u5e73\u5747\uff0c\u6700\u540e\u7ed3\u679c\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"X[:,:,0].mean() = 0"}),", ",(0,a.jsx)(n.code,{children:"X[:, :, 0].var() = 1"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["layernorm\u53d6\u7684\u662f C \u7684\u7ef4\u5ea6\u4e0a\u7684\u5e73\u5747\uff0c\u6700\u540e\u7ed3\u679c\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"X[0,0,:].mean() = 0"}),", ",(0,a.jsx)(n.code,{children:"X[0, 0, :].var() = 1"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"batchnorm",children:"BatchNorm"}),"\n",(0,a.jsx)(n.p,{children:"nn.BatchNorm1d \u662f\u9ed8\u8ba4\u5904\u7406input \u683c\u5f0f\u662f (B,C,T),"}),"\n",(0,a.jsx)(n.p,{children:"e.g.,"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"\nt = torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]], dtype=torch.float32)\nm = nn.BatchNorm1d(2)\nout = m(t)\nprint(out)\nprint(out[:,0,:].mean(), out[:,0,:].var())\nprint(out[:,1,:].mean(), out[:,1,:].var())\nprint([p for p in m.parameters()])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"tensor([[[-1.2865, -0.9649, -0.6433],\n         [-1.2865, -0.9649, -0.6433]],\n\n        [[ 0.6433,  0.9649,  1.2865],\n         [ 0.6433,  0.9649,  1.2865]]], grad_fn=<NativeBatchNormBackward0>)\ntensor(0., grad_fn=<MeanBackward0>) tensor(1.2000, grad_fn=<VarBackward0>)\ntensor(0., grad_fn=<MeanBackward0>) tensor(1.2000, grad_fn=<VarBackward0>)\n[Parameter containing:\ntensor([1., 1.], requires_grad=True), Parameter containing:\ntensor([0., 0.], requires_grad=True)]\n\n"})}),"\n",(0,a.jsx)(n.p,{children:"\u4e5f\u5c31\u662f\u8bf4\uff0c\u771f\u6b63\u5728optimize\u7684parameter\u53ea\u67092\uff0c\u4e5f\u5c31\u662f\u5728dim=1\u7684\u5730\u65b9\u6709\u4e24\u4e2a\u6570\u5b57"}),"\n",(0,a.jsx)(n.p,{children:"\u4e5f\u5c31\u662f\u8bf4\uff0c gamma\u7684shape \u662f 2\uff0cbeta\u7684shape\u4e5f\u662f2"}),"\n",(0,a.jsx)(n.p,{children:"\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48\uff0c\u5982\u679c\u5b9e\u9645\u4e0a\u6211\u4eec\u9700\u8981\u7684\u7684input\u662f\u6b63\u5e38\u7684\u683c\u5f0f B, T, C \uff08\u800c\u4e0d\u662f B, C, T), \u6211\u4eec\u9700\u8981\u505a\u4ee5\u4e0b"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"x = torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]], dtype=torch.float32)\nB, T, C = x.shape\nx = x.flatten(0,1)\nm = nn.BatchNorm1d(C)\nout = m(x)\nout = out.unflatten(0, (B, T))\nprint(out)\nprint(out[:,:,0].mean())\nprint(out[:,:,0].var())\nprint(out[:,:,1].mean())\nprint(out[:,:,1].var())\nprint([p for p in m.parameters()])\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"\u6211\u4eec\u4ee5\u4e0a\u8ff0\u5904\u7406\u4e3abatchnorm\u7684\u6807\u51c6\u5904\u7406\uff0c\u53ef\u4ee5\u770b\u51fa\u6765\uff0cbatchnorm\u53d6\u7684\u662f B \u548c T \u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5e73\u5747"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"tensor([[[-1.3416, -1.3416, -1.3416],\n         [-0.4472, -0.4472, -0.4472]],\n\n        [[ 0.4472,  0.4472,  0.4472],\n         [ 1.3416,  1.3416,  1.3416]]], grad_fn=<ViewBackward0>)\ntensor(-5.9605e-08, grad_fn=<MeanBackward0>)\ntensor(1.3333, grad_fn=<VarBackward0>)\ntensor(2.9802e-08, grad_fn=<MeanBackward0>)\ntensor(1.3333, grad_fn=<VarBackward0>)\n[Parameter containing:\ntensor([1., 1., 1.], requires_grad=True), Parameter containing:\ntensor([0., 0., 0.], requires_grad=True)]\n"})}),"\n",(0,a.jsx)(n.p,{children:"\u8fd9\u6837\u5904\u7406\u5f97\u5230\u7684\u624d\u662f\u4e00\u822c\u610f\u4e49\u4e0a\u7684batchnorm\u7684\u7ed3\u679c"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"\u4f46\u5982\u679c\u6211\u4eec\u505alayernorm, \u53ef\u4ee5\u770b\u5230layernorm\u53d6\u7684\u662f C \u8fd9\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u5e73\u5747"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"x = torch.tensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]], dtype=torch.float32)\nB, T, C = x.shape\nm = nn.LayerNorm(C)\nout = m(x)\nprint(out)\nprint(out[0,0,:].mean())\nprint(out[0,0,:].var())\nprint(out[0,1,:].mean())\nprint(out[0,1,:].var())\nprint([p for p in m.parameters()])\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"tensor([[[-1.2247,  0.0000,  1.2247],\n         [-1.2247,  0.0000,  1.2247]],\n\n        [[-1.2247,  0.0000,  1.2247],\n         [-1.2247,  0.0000,  1.2247]]], grad_fn=<NativeLayerNormBackward0>)\ntensor(0., grad_fn=<MeanBackward0>)\ntensor(1.5000, grad_fn=<VarBackward0>)\ntensor(0., grad_fn=<MeanBackward0>)\ntensor(1.5000, grad_fn=<VarBackward0>)\n[Parameter containing:\ntensor([1., 1., 1.], requires_grad=True), Parameter containing:\ntensor([0., 0., 0.], requires_grad=True)]\n"})}),"\n",(0,a.jsx)(n.h2,{id:"72-layernorm-\u52a0\u5728\u54ea\u91cc",children:"7.2 LayerNorm \u52a0\u5728\u54ea\u91cc"}),"\n",(0,a.jsx)(n.p,{children:"\u548cattention is all you need paper\u4ecb\u7ecd\u7684\u7a0d\u6709\u4e0d\u540c\uff0c\u6211\u4eeclayernorm\u52a0\u5728\uff1a"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"\u6bcf\u4e00\u4e2aBlock \u7684 multiHeadAttention \u4e4b\u524d"}),"\n",(0,a.jsx)(n.li,{children:"\u6bcf\u4e2aBlock\u7684multiheadattention\u548cfeedforward\u4e4b\u95f4"}),"\n",(0,a.jsx)(n.li,{children:"\u8fd8\u6709\u6700\u540e\u4e00\u4e2ablock\u7ed3\u675f\u4e4b\u540e\uff0c\u5728output linear \u4e4b\u524d \uff08right before the final linear layer that decodes into vocabulary)."}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class Block(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('layer_norm1', nn.LayerNorm(head_size)),\n            ('multi_head_attention', MultiHeadAttention(n_head, head_size=head_size // n_head)),\n            ('layer_norm2', nn.LayerNorm(head_size)),\n            ('feed_forward', FeedForward(head_size, head_size*4, head_size)),\n        ]))\n\n    def forward(self, X):\n        for layer in self.layers:\n            X = X + layer(X)\n        return X\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class GPTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('embedding', EmbeddingLayer(vocab_size, n_emb)),\n            ('block1', Block(n_head=4,head_size=head_size)),\n            ('block2', Block(n_head=4,head_size=head_size)),\n            ('block3', Block(n_head=4,head_size=head_size)),\n            ('layer_norm', nn.LayerNorm(head_size)),\n            ('lm_head', nn.Linear(head_size, vocab_size)),\n        ])\n        )\n"})}),"\n",(0,a.jsx)(n.h1,{id:"step-8-add-dropout",children:"Step 8: Add DropOut"}),"\n",(0,a.jsx)(n.p,{children:"Add dropout"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["right before the connection to the residual pathway\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"i.e., right before residual pathway at the end of multi-head-attention"}),"\n",(0,a.jsx)(n.li,{children:"right before residual pathway at the end of feedforward"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"and right after q * k (i.e., weight) is calculated in self-attention layer"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"in MultiHeadAttention"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.heads = nn.ModuleList([\n            SelfAttentionHead(head_size=head_size) for _ in range(n_head)\n        ])\n        self.proj = nn.Linear(head_size * n_head, head_size * n_head)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, X):\n        X = torch.cat([h(X) for h in self.heads], dim=-1)\n        X = self.proj(X)\n        X = self.dropout(X)\n        return X\n"})}),"\n",(0,a.jsx)(n.p,{children:"in FeedForward"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"class FeedForward(nn.Module):\n    def __init__(self, in_features, inner_layer_features, out_features):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('linear', nn.Linear(in_features, inner_layer_features)),\n            ('relu', nn.ReLU(inner_layer_features)),\n            ('proj', nn.Linear(inner_layer_features, out_features)),\n            ('dropout', nn.Dropout(0.2)),\n\n        ]))\n    def forward(self, X):\n        return self.layers.forward(X)\n\n"})}),"\n",(0,a.jsx)(n.p,{children:"in SelfAttentionHead"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"        weight = q @ k.transpose(1, 2)\n        weight = weight.masked_fill(self.tril == 0, -torch.inf)\n        weight = weight.softmax(dim=-1)\n        weight = self.dropout(weight)\n"})}),"\n",(0,a.jsx)(n.h1,{id:"9-final---network-architecture",children:"9. Final - Network Architecture"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042335409.png",alt:""})}),"\n",(0,a.jsx)(n.p,{children:"Note: LayerNorm should not be included into the residual connection pathway"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"we want the residual connection to skip layernorm+multi-head-self-attention and, only on layernorm+feedforward only."}),"\n",(0,a.jsx)(n.li,{children:"This is because we want a clean residual pathway so that X that did not get normalized get feed into the neural network."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"We didn\u2019t use the encoder part:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"that is for calculating cross attention, which is used for machine translation."}),"\n",(0,a.jsx)(n.li,{children:"We used the tril mask to run the self-attention, which is a decoder architecture."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"short introduction of cross attention in machine translation:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"e.g., if we translate from a french sentence to an english sentence"}),"\n",(0,a.jsx)(n.li,{children:"encoder takes the key, value from the french sentence, with the full sentence (not the tril masked one)"}),"\n",(0,a.jsx)(n.li,{children:"and then the cross attention uses the query to interact with the k, v."}),"\n"]}),"\n",(0,a.jsx)(n.h1,{id:"10-final-code",children:"10. Final Code"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"1from collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\n\nwith open(\"ng-video-lecture/input.txt\", encoding='utf-8', mode=\"r\") as f:\n    text = f.read()\n\nitos = sorted(list(set(text)))\nstoi = {c: i for i, c in enumerate(itos)}\nvocab_size = len(itos)\nn_emb = 64\nprint(f\"{vocab_size=}\")\n\nencode = lambda text: [stoi[c] for c in text]\ndecode = lambda lst: \"\".join([itos[i] for i in lst])\nblock_size = 16\nhead_size = n_emb\n\ndef get_x_y(ix, text):\n    \"\"\"\n    for the sentence \"First Citizen:\" withix = 0\n    target input and output for block_size = 8 (meaning max input length = 8):\n    First Ci -> irst Cit\n    \"\"\"\n    encoded = encode(text[ix:ix + block_size + 2])\n    x_data, y_data = encoded[:block_size], encoded[1:block_size + 1]\n    return torch.tensor(x_data, dtype=torch.int32), torch.tensor(y_data, dtype=torch.int64)\n\ndef get_batch(batch_size, text):\n    \"\"\"\n    e.g., if batch_size = 32, and block_size = 8,\n    then we select 32 places, where we take 9 character_sequence each.\n    \"\"\"\n    ixs = torch.randint(len(text) - block_size - 2, (batch_size,))\n    x_data_lst, y_data_lst = [], []\n    for ix in ixs:\n        x_data, y_data = get_x_y(ix, text)\n        x_data_lst.append(x_data)\n        y_data_lst.append(y_data)\n    return torch.stack(x_data_lst), torch.stack(y_data_lst)\n\nclass SelfAttentionHead(nn.Module):\n\n    def __init__(self, head_size):\n        # block_size is T, head_size is C\n        super().__init__()\n        self.head_size = head_size\n        self.register_buffer(name='tril', tensor=torch.tril(torch.ones(block_size, block_size)))\n        # tril is constant, not parameter, should register here.\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, X):\n        # x = torch.randn(10, 4(block_size), 32)\n        B, T, C = X.shape\n        query_layer = nn.Linear(C, self.head_size, bias=False)\n        key_layer = nn.Linear(C, self.head_size, bias=False)\n        q = query_layer(X)\n        k = key_layer(X)\n        weight = q @ k.transpose(1, 2)\n        weight = weight.masked_fill(self.tril == 0, -torch.inf)\n        weight = weight.softmax(dim=-1)\n        weight = self.dropout(weight)\n        value_layer = nn.Linear(C, self.head_size, bias=False)\n        v = value_layer(X)\n        out = weight @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.heads = nn.ModuleList([\n            SelfAttentionHead(head_size=head_size) for _ in range(n_head)\n        ])\n        self.proj = nn.Linear(head_size * n_head, head_size * n_head)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, X):\n        X = torch.cat([h(X) for h in self.heads], dim=-1)\n        X = self.proj(X)\n        X = self.dropout(X)\n        return X\n\nclass FeedForward(nn.Module):\n    def __init__(self, in_features, inner_layer_features, out_features):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('linear', nn.Linear(in_features, inner_layer_features)),\n            ('relu', nn.ReLU(inner_layer_features)),\n            ('proj', nn.Linear(inner_layer_features, out_features)),\n            ('dropout', nn.Dropout(0.2)),\n\n        ]))\n    def forward(self, X):\n        return self.layers.forward(X)\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size, n_emb):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n\n    def forward(self, X):\n        # X dim: 10, 4, n_emb=2\n        B, T = X.shape\n        token_emb = self.token_embedding_table(X)  # 10, 4, 2\n        position_emb = self.position_embedding_table(torch.arange(0, T))  # 4, 2\n        return token_emb + position_emb  # 10, 4, 2\n\nclass Block(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('layer_norm1', nn.LayerNorm(head_size)),\n            ('multi_head_attention', MultiHeadAttention(n_head, head_size=head_size // n_head)),\n            ('layer_norm2', nn.LayerNorm(head_size)),\n            ('feed_forward', FeedForward(head_size, head_size*4, head_size)),\n        ]))\n\n    def forward(self, X):\n        for layer in self.layers:\n            if layer.__class__.__name__ not in ['LayerNorm']:\n                X = X + layer(X)\n        return X\n\nclass GPTModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(OrderedDict([\n            ('embedding', EmbeddingLayer(vocab_size, n_emb)),\n            ('block1', Block(n_head=4,head_size=head_size)),\n            ('block2', Block(n_head=4,head_size=head_size)),\n            ('block3', Block(n_head=4,head_size=head_size)),\n            ('layer_norm', nn.LayerNorm(head_size)),\n            ('lm_head', nn.Linear(head_size, vocab_size)),\n        ])\n        )\n        # for layer in self.layers:\n        #     layer.register_forward_hook(lambda layer, input, output:\n        #                                 print(f\"{layer.__class__.__name__:20} | in: {input[0].shape} | out: {output.shape}\"))\n\n    def forward(self, X):\n        logits = self.layers.forward(X)\n        return logits\n\n    def generate_next(self, X, iter_i, iter_limit):\n        \"input is a 1-dimensional tensor, but the first dimension is always 1. e.g., 1 * 4\"\n        if iter_i == iter_limit:\n            return X\n        iter_i += 1\n        # X: [1, 8]\n        logits = self.forward(X[:, -block_size:])  # [1, 8, 16]\n        probs = torch.softmax(logits[:, -1, :].squeeze(), dim=-1)  # [16]\n        ix = torch.multinomial(probs, num_samples=1)\n        X = torch.concat([X, ix.view(1, 1)], dim=-1)\n        return self.generate_next(X, iter_i + 1, iter_limit)\n\n    def generate(self, X=torch.tensor([[0 for _ in range(block_size)]]), iter_limit=1000):\n        x_gen = self.generate_next(X, 0, iter_limit)\n        print(decode(x_gen.squeeze().tolist()))\n\nm = GPTModel()\ntrain_sep_n = int(len(text) * 0.9)\ntrain_text = text[:train_sep_n]\nval_text = text[train_sep_n:]\n\nbatch_size = 64\nNUM_EPOCH = 1000\noptimizer = torch.optim.Adam(params=m.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()\nlossi = []\n\n@torch.no_grad()\ndef estimate_total_loss(m, eval_iter=100):\n    m.eval()\n    train_loss_lst, val_loss_lst = [], []\n    for _ in range(eval_iter):\n        train_loss = estimate_batch_loss(m, train_text).log10().item()\n        # print(f\"{train_loss=}\")\n        val_loss = estimate_batch_loss(m, val_text).log10().item()\n        # print(f\"{val_loss=}\")\n        train_loss_lst.append(train_loss)\n        val_loss_lst.append(val_loss)\n    m.train()\n    return sum(train_loss_lst) / eval_iter, sum(val_loss_lst) / eval_iter\n\ndef estimate_batch_loss(m, text):\n    X, Y = get_batch(batch_size=batch_size, text=text)\n    logits = m.forward(X).flatten(0, 1)\n    loss = loss_fn(input=logits, target=Y.flatten(0, 1))\n    return loss\n\ntrain_lossi, val_lossi = [], []\n\ndef train():\n    m.train()\n    for i in tqdm(range(NUM_EPOCH)):\n        optimizer.zero_grad()\n        loss = estimate_batch_loss(m, train_text)\n        loss.backward()\n        if i % 100 == 0:\n            cur_train_loss, cur_val_loss = estimate_total_loss(m)\n            train_lossi.append(cur_train_loss)\n            val_lossi.append(cur_val_loss)\n        optimizer.step()\n\ntrain()\nimport matplotlib.pyplot as plt\n\nplt.plot(train_lossi)\nplt.show()\n\nplt.plot(val_lossi)\nplt.show()\n\nprint(\"------ Final Train Loss: ---------\\\\n\", 10 ** train_lossi[-1])\nprint(\"------ Final Validation Loss: ----\\\\n\", 10 ** val_lossi[-1])\nm.generate()\n\n"})}),"\n",(0,a.jsx)(n.h1,{id:"appendix",children:"Appendix"}),"\n",(0,a.jsx)(n.h2,{id:"other-tokenizers",children:"Other tokenizers:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Google uses SentencePiece. It is a sub-word tokenizer (between 1 char and 1 word)"}),"\n",(0,a.jsxs)(n.li,{children:["OpenAI uses ",(0,a.jsx)(n.code,{children:"tiktoken"}),", it uses BPE (byte-pair encoding) that\u2019s similar to sub-word tokenizer."]}),"\n",(0,a.jsxs)(n.li,{children:["e.g., ",(0,a.jsx)(n.code,{children:"hii there"})," has 3 tokens."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"nnflatten--nnunflatten",children:"nn.Flatten & nn.Unflatten"}),"\n",(0,a.jsx)(n.h3,{id:"nnflatten-put-a-2-dimension-data-to-2-dimension",children:"nn.Flatten: put a >2 dimension data to 2 dimension."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["default\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["is ",(0,a.jsx)(n.code,{children:"m = nn.Flatten(start_dim=1,end_dim=-1)"}),","]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"t = torch.randn((1,2,3,4))\nm = nn.Flatten()\nout = m(t)\nprint(out.shape) # (1, 24)\n"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"if I do"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"m = nn.Flatten(0,1)"})}),"\n",(0,a.jsx)(n.li,{children:"then out shape will be (2, 12) \u2192 (explicitly) flatten first dimension, and flatten whatever is rest as the second dimension"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"if I do"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"m = nn.Flaten(2,3)"})}),"\n",(0,a.jsx)(n.li,{children:"then out will be (2, 12) too \u2192(explicitly) flatten second dimension, and flatten whatever is rest as the first dimension"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"nnunflatten-put-a-2-dimension-data-to--2-dimension",children:"nn.Unflatten: put a 2 dimension data to > 2 dimension"}),"\n",(0,a.jsx)(n.p,{children:"need to specify: which dimension, make it into what"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"t = torch.randn(24, 63)\nm = nn.Unflatten(dim=0, unflattened_size=(1, 4, 6))\nout = m(t)\nprint(out.shape) # (1, 4, 6, 63)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"t = torch.randn(24, 63)\nm = nn.Unflatten(dim=1, unflattened_size=(9, 7))\nout = m(t) \nprint(out.shape) # (24, 9, 7)\n"})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>l});var i=t(96540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);