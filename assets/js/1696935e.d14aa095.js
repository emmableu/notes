"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[85273],{28453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>s});var r=t(96540);const a={},o=r.createContext(a);function i(n){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),r.createElement(o.Provider,{value:e},n.children)}},58334:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>i,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"DL Theory 100/Explain Transformer architecture components and overall mechanism","title":"1. Explain Transformer architecture (components and overall mechanism)","description":"step 1: \u57fa\u4e8emulti-head attention, \u52a0\u4e0a layernorm \u548c resnet \u548c FFN","source":"@site/docs/02. DL Theory 100/001. Explain Transformer architecture components and overall mechanism.md","sourceDirName":"02. DL Theory 100","slug":"/p/73e70ad8-d53b-4524-a52f-a0f338135a3b","permalink":"/notes/docs/p/73e70ad8-d53b-4524-a52f-a0f338135a3b","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/02. DL Theory 100/001. Explain Transformer architecture components and overall mechanism.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"created_at":"2025-11-01","page_link":"/p/73e70ad8-d53b-4524-a52f-a0f338135a3b","slug":"/p/73e70ad8-d53b-4524-a52f-a0f338135a3b"},"sidebar":"tutorialSidebar","previous":{"title":"02. DL Theory 100","permalink":"/notes/docs/dl100"},"next":{"title":"Self-Attention step by step implementation","permalink":"/notes/docs/p/43a96f3c-a9c2-4990-9538-c7601da9d719"}}');var a=t(74848),o=t(28453);const i={created_at:"2025-11-01",page_link:"/p/73e70ad8-d53b-4524-a52f-a0f338135a3b",slug:"/p/73e70ad8-d53b-4524-a52f-a0f338135a3b"},s="1. Explain Transformer architecture (components and overall mechanism)",l={},d=[{value:"step 1: \u57fa\u4e8emulti-head attention, \u52a0\u4e0a layernorm \u548c resnet \u548c FFN",id:"step-1-\u57fa\u4e8emulti-head-attention-\u52a0\u4e0a-layernorm-\u548c-resnet-\u548c-ffn",level:2},{value:"Step 2: \u52a0\u4e0adropout",id:"step-2-\u52a0\u4e0adropout",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"1-explain-transformer-architecture-components-and-overall-mechanism",children:"1. Explain Transformer architecture (components and overall mechanism)"})}),"\n",(0,a.jsx)(e.h2,{id:"step-1-\u57fa\u4e8emulti-head-attention-\u52a0\u4e0a-layernorm-\u548c-resnet-\u548c-ffn",children:"step 1: \u57fa\u4e8emulti-head attention, \u52a0\u4e0a layernorm \u548c resnet \u548c FFN"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n"""\nx => ln1 => mha => +  => ln2 => ffn => + \n    -------------\x3e      -------------\x3e\n"""\nclass FFN(nn.Module):\n    def __init__(self, C: int):\n        super().__init__()\n        self.linear1 = nn.Linear(C, 4*C)\n        self.linear2 = nn.Linear(4*C, C)\n\n    def forward(self, X):\n        X = self.linear1(X)\n        X = F.relu(X)\n        X = self.linear2(X)\n        return X\n\n\nclass Block(nn.Module):\n    def __init__(self, C:int, H:int):\n        super().__init__()\n        self.mha = MultiHeadAttention(C, H)\n        self.ln1 = nn.LayerNorm(C)\n        self.ln2 = nn.LayerNorm(C)\n        self.ffn = FFN(C) # apply non-linear relationship to each token\n\n    def forward(self, X):\n        X = self.mha(self.ln1(X)) + X\n        X = self.ffn(self.ln2(X)) + X\n        return X\n'})}),"\n",(0,a.jsx)(e.p,{children:"\u4f5c\u7528\uff1a"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"resnet: \u9632\u6b62 degredation problem; gradient vanishment/explosion; saddle point"}),"\n",(0,a.jsx)(e.li,{children:"layernorm\uff1a\u9632\u6b62gradient vanishment"}),"\n",(0,a.jsx)(e.li,{children:"ffn: add nonlinear relationship between individual tokens to output"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"step-2-\u52a0\u4e0adropout",children:"Step 2: \u52a0\u4e0adropout"}),"\n",(0,a.jsx)(e.p,{children:"\u76ee\u7684\uff1a"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"randomly zero out different units on every forward pass"}),"\n",(0,a.jsx)(e.li,{children:"reduce overly relying on a single path, prevent overfit\n\u5b9e\u9645\u505a\u7684\u662f"}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"mask ~ Bernoulli(p = 1 - dropout_rate)\ny = (x * mask) / (1 - dropout_rate)\n"})}),"\n",(0,a.jsx)(e.p,{children:"\u5b9a\uff1aTransformer \u662f\u57fa\u4e8e Self-Attention \u81ea\u6ce8\u610f\u529b \u548c  FFN \u524d\u9988\u7f51\u7edc \u7684\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\uff0c\u5229\u7528 \u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09 \u5e76\u884c\u5efa\u6a21\u5168\u5c40\u4f9d\u8d56\uff0c\u53ef\u505a Encoder-Decoder\uff08\u5982 T5\uff09\u6216 Decoder-Only\uff08\u5982 GPT\uff09\u3002"}),"\n",(0,a.jsx)(e.p,{children:"\u56e0\uff1aRNN/CNN \u53d7\u9650\u4e8e\u987a\u5e8f/\u5c40\u90e8\u611f\u53d7\u91ce\uff1b\u957f\u4f9d\u8d56\u96be\u3001\u5e76\u884c\u5ea6\u4f4e\u3002Attention \u5141\u8bb8\u4efb\u610f token \u4e24\u4e24\u4ea4\u4e92\uff0c\u68af\u5ea6\u8def\u5f84\u77ed\u3001\u53ef\u5e76\u884c\u3002"}),"\n",(0,a.jsx)(e.p,{children:"\u6cd5\uff1a\u5c42\u7ea7\u5806\u53e0 [LN \u2192 MHA \u2192 \u6b8b\u5dee] + [LN \u2192 FFN \u2192 \u6b8b\u5dee]\uff1b\u52a0\u5165\u4f4d\u7f6e\u7f16\u7801\uff08sinusoidal \u6216 RoPE/learned\uff09\uff1b\u8bad\u7ec3\u7528 CE/\u56e0\u679c\u63a9\u7801\uff1b\u63a8\u7406\u7528\u81ea\u56de\u5f52\u89e3\u7801+KV Cache\u3002"}),"\n",(0,a.jsx)(e.p,{children:"\u679c\uff1a\u5728\u7ffb\u8bd1\u3001\u5bf9\u8bdd\u3001\u4ee3\u7801\u3001\u68c0\u7d22\u95ee\u7b54\u7b49\u4efb\u52a1\u4e0a SOTA\uff1b\u63a8\u7406\u65f6\u53ef\u6d41\u5f0f/\u7f13\u5b58\uff0c\u541e\u5410\u4e0e\u8d28\u91cf\u517c\u987e\u3002"}),"\n",(0,a.jsx)(e.p,{children:"\u4f18\uff1a\u5e76\u884c\u53cb\u597d\u3001\u957f\u7a0b\u5efa\u6a21\u5f3a\u3001\u6a21\u5757\u5316\u6613\u6269\u5c55\uff08MoE/GQA/LoRA \u7b49\uff09\uff0c\u751f\u6001\u6210\u719f\u3002"}),"\n",(0,a.jsx)(e.p,{children:"\u7ec6\u8282 & \u4f8b\u5b50\uff08\u4e00\u6b65\u4e00\u6b65\uff09\n\u4f8b\uff1a\u53e5\u5b50 \u201cTime flies fast\u201d\u3002\n1.\tEmbedding\uff1a\u5c06 token \u6620\u5c04\u5230 d \u6a21\u7ef4\u7a7a\u95f4\uff08\u5982 d=768\uff09\u3002\n2.\t\u4f4d\u7f6e\u7f16\u7801\uff1a\u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u52a0\u4e0a\u4f4d\u7f6e\u5411\u91cf\uff0c\u4fdd\u7559\u987a\u5e8f\u3002\n3.\tMHA\uff1a\u5bf9\u6bcf\u4e2a token \u8ba1\u7b97 Q= XW_Q, K= XW_K, V= XW_V\uff1b\u6ce8\u610f\u529b = softmax(QK\u1d40/\u221ad_k)V\uff1b\u591a\u5934\u5e76\u884c\u540e concat + W_O\u3002\n4.\tFFN\uff1a\u9010\u4f4d\u7f6e MLP\uff08\u542b\u6fc0\u6d3b GeLU/Swish\uff0c\u7ef4\u5ea6\u6269\u5c55\u56e0\u5b50\u22484\uff09\u3002\n5.\t\u6b8b\u5dee+LN\uff1a\u7a33\u5b9a\u8bad\u7ec3\u3001\u4fdd\u7559\u68af\u5ea6\u8def\u5f84\u3002\n6.\tDecoder-Only\uff1a\u52a0 \u56e0\u679c mask\uff0c\u7981\u6b62\u770b\u672a\u6765\u4f4d\u3002\n7.\t\u8bad\u7ec3\uff1a\u6700\u5927\u4f3c\u7136/\u4ea4\u53c9\u71b5\uff1b\u63a8\u7406\uff1a\u81ea\u56de\u5f52\u751f\u6210 + Top-p/\u6e29\u5ea6\u7b49\u3002"}),"\n",(0,a.jsx)(e.p,{children:"3 \u4e2a\u91cd\u70b9 Follow-up\nA. \u201c\u4e3a\u4ec0\u4e48\u4e00\u5b9a\u8981\u6b8b\u5dee\u8fde\u63a5\uff1f\u201d\n\u2022\t\u5b9a\uff1a\u6b8b\u5dee\u8ba9\u5c42\u5b66\u201c\u6270\u52a8\u201d\uff0c\u4fdd\u4e3b\u5e72\u68af\u5ea6\u901a\u8def\u3002\n\u2022\t\u56e0\uff1a\u6df1\u5c42\u7f51\u7edc\u68af\u5ea6\u6d88\u5931/\u9000\u5316\u3002\n\u2022\t\u6cd5\uff1ay = x + F(LN(x))\uff1b\u53cd\u5411\u68af\u5ea6\u6709\u201c\u76f4\u901a\u9053\u201d\u3002\n\u2022\t\u679c\uff1a\u66f4\u6df1\u5c42\u4ecd\u53ef\u7a33\u5b9a\u6536\u655b\u3002\n\u2022\t\u4f18\uff1a\u7b80\u5355\u9ad8\u6548\u3001\u901a\u7528\u6027\u5f3a\u3002"}),"\n",(0,a.jsx)(e.p,{children:"B. \u201cEncoder-Decoder \u4e0e Decoder-Only \u4f55\u65f6\u9009\uff1f\u201d\n\u2022\t\u5b9a\uff1a\u524d\u8005\u9002\u5408\u6761\u4ef6\u751f\u6210\uff08\u7ffb\u8bd1\uff09\uff0c\u540e\u8005\u9002\u5408\u7eed\u5199/\u901a\u7528\u751f\u6210\u3002\n\u2022\t\u56e0\uff1a\u662f\u5426\u9700\u8981\u663e\u5f0f\u5bf9\u6e90\u5e8f\u5217\u505a cross-attention\u3002\n\u2022\t\u6cd5\uff1a\u4efb\u52a1\u82e5\u5f3a\u4f9d\u8d56\u8f93\u5165\u7ed3\u6784\u5316\u6761\u4ef6\u2192Encoder-Decoder\uff1b\u5f00\u653e\u751f\u6210\u2192Decoder-Only\u3002\n\u2022\t\u679c\uff1a\u66f4\u8d34\u4efb\u52a1\u5f52\u7eb3\u504f\u7f6e\u3002\n\u2022\t\u4f18\uff1a\u8ba1\u7b97/\u8d28\u91cf\u53d6\u820d\u53ef\u63a7\u3002"}),"\n",(0,a.jsx)(e.p,{children:"C. \u201c\u4f4d\u7f6e\u7f16\u7801\u5fc5\u987b\u662f\u6b63\u5f26\u5417\uff1f\u201d\n\u2022\t\u5b9a\uff1a\u4e0d\u5fc5\u987b\uff0c\u53ef learned/\u76f8\u5bf9/\u65cb\u8f6c\u3002\n\u2022\t\u56e0\uff1a\u4e0d\u540c\u65b9\u6848\u5bf9\u5916\u63a8\u3001\u957f\u4e0a\u4e0b\u6587\u8868\u73b0\u4e0d\u540c\u3002\n\u2022\t\u6cd5\uff1aSinusoidal\uff08\u53ef\u5916\u63a8\uff09\u3001Learned\uff08\u8d34\u6570\u636e\uff09\u3001RoPE/ALiBi\uff08\u957f\u7a0b\u53cb\u597d\uff09\u3002\n\u2022\t\u679c\uff1a\u4e0a\u4e0b\u6587\u957f\u5ea6\u6269\u5c55\u80fd\u529b\u5dee\u5f02\u663e\u8457\u3002\n\u2022\t\u4f18\uff1a\u53ef\u6309\u9700\u6c42\u66ff\u6362\u5347\u7ea7\u3002"}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202511022057851.png",alt:""})})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}}}]);