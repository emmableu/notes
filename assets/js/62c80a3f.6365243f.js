"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[24524],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(96540);const r={},a=i.createContext(r);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}},50147:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>l,frontMatter:()=>s,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"Zero To Hero/Makemore 3 - Activations & Gradients, BatchNorm","title":"Makemore 3 - Activations & Gradients, BatchNorm","description":"Links:","source":"@site/docs/04. Zero To Hero/05. Makemore 3 - Activations & Gradients, BatchNorm.md","sourceDirName":"04. Zero To Hero","slug":"/p/7d5e92b9-6fc9-48a9-bc43-0ec41f622dc2","permalink":"/notes/docs/p/7d5e92b9-6fc9-48a9-bc43-0ec41f622dc2","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/04. Zero To Hero/05. Makemore 3 - Activations & Gradients, BatchNorm.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/7d5e92b9-6fc9-48a9-bc43-0ec41f622dc2","slug":"/p/7d5e92b9-6fc9-48a9-bc43-0ec41f622dc2"},"sidebar":"tutorialSidebar","previous":{"title":"Makemore 2 - MLP","permalink":"/notes/docs/p/698a3541-6dd9-4a29-a38c-18bbe80f8d33"},"next":{"title":"5. Makemore 4 - Backpropagation Ninja","permalink":"/notes/docs/p/b52964bd-d3bf-4f0c-bce8-750a995dfb2f"}}');var r=t(74848),a=t(28453);const s={created_at:"2025-11-02",page_link:"/p/7d5e92b9-6fc9-48a9-bc43-0ec41f622dc2",slug:"/p/7d5e92b9-6fc9-48a9-bc43-0ec41f622dc2"},o="Course Notes",d={},h=[{value:"6.1 Running bn_mean and bn_std.",id:"61-running-bn_mean-and-bn_std",level:2},{value:"6.2: Removal of b1",id:"62-removal-of-b1",level:2},{value:"1. <strong>Overconfidence and Generalization</strong>",id:"1-overconfidence-and-generalization",level:3},{value:"2. <strong>Softmax Saturation</strong>",id:"2-softmax-saturation",level:3},{value:"11.1 Activation Distribution",id:"111-activation-distribution",level:2},{value:"11.2 Gradient Distribution",id:"112-gradient-distribution",level:2},{value:"11.3 Weights Gradient Distribution",id:"113-weights-gradient-distribution",level:2},{value:"11.4 Change of Update Rate",id:"114-change-of-update-rate",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Links:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Youtube: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=P6sfmUTpUmc",children:"https://www.youtube.com/watch?v=P6sfmUTpUmc"})]}),"\n",(0,r.jsxs)(n.li,{children:["makemore on github: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa2JnOEZmY2VxN0s2b1NiWWhEZTMyNFY1M0Y0Z3xBQ3Jtc0tuRGNQOFEtODcyVGY3dGNOdjd1TGtzUUpvcUFrV0NtR0Yzd00ySUd3aXg2SXVxLUQ3SVVvUWdUSk5iVnV5RzZwQURDLVkyTF92Wk1tWjBJMmFaNkcxeGlNT3ZsTGIxM1VVdF92RVp4UlNiT3l4Y3BCZw&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fmakemore&v=P6sfmUTpUmc",children:"https://github.com/karpathy/makemore"})]}),"\n",(0,r.jsxs)(n.li,{children:["jupyter notebook I built in this video: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa0N3SlZVY1I2c0xKYUJVZTczYzJZN1dOZUs2Z3xBQ3Jtc0tuWVU5UUwwN0JmQVQwZ3VUVkdQdWRFa0FGY0l4Wjg2YWdNQmc3cWFLZERkM1V3dlBIcjJueTR1bWhIU0hZN1VzQTNOT0p6Nm1Jc2RHd1hJaXNsMEpuT1Bkd0tmSDRPRlhoWWVuVW1jVGx0ZTJ4VXdJRQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fnn-zero-to-hero%2Fblob%2Fmaster%2Flectures%2Fmakemore%2Fmakemore_part3_bn.ipynb&v=P6sfmUTpUmc",children:"https://github.com/karpathy/nn-zero-t..."})]}),"\n",(0,r.jsxs)(n.li,{children:["collab notebook: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqazJRSGF2MGdTbjJzenRWYnlfU2xUQVAxZ2UxUXxBQ3Jtc0tsVFctZUE1VFlHZUlVLVMxYTF0ZE9OYXNIazE3MFVJOUotd3NUQkk0OEZacHpCTV83dmt1eE1ZWEhTUXptZlFPOE5hYW5PbElSRFYtVmNpb0NxU0ZUcmp0MkJzY0NGNzZHX1gtMEFLYXBMWVhvaUstcw&q=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1H5CSy-OnisagUgDUXhHwo1ng2pjKHYSN%3Fusp%3Dsharing&v=P6sfmUTpUmc",children:"https://colab.research.google.com/dri..."})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Useful links:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:['"Kaiming init" paper:',(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjVBbU9tX2NXeDRYclo4R1hMT3A0eU9iUFRmZ3xBQ3Jtc0ttSmZ3N2IyTks5RDhyM1hZcjhYTHpXQWtBdXlHYzZQY050OEtBcGZmLVljbFVGS3VwLUxpbWhYb0d6VXJ2NkJTdEJoUC1rVDY0LS1neDdZZVVURmdIZDRVUkExTzh1czc0a2VKM05WSlo0VmhtQ2l6TQ&q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.01852&v=P6sfmUTpUmc",children:"https://arxiv.org/abs/1502.01852"})]}),"\n",(0,r.jsxs)(n.li,{children:["BatchNorm paper: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWtfREtEMndUb0E2VHpHako4TFpDWmlMaXg2d3xBQ3Jtc0tuOVRjMXBseUY3djgwbU83eFpIQldTTGxVY0xyRWtBa2tidjk4al93RExPOVRuMjF4M3FYU1JDZUNkR00zVVVHcmJNTUFXYlFVS2Qwd19LX1RJRmVwd29GTXVnWkZuNWNSMC1KZ21kRkJWd19vTVhTdw&q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.03167&v=P6sfmUTpUmc",children:"https://arxiv.org/abs/1502.03167"})]}),"\n",(0,r.jsxs)(n.li,{children:["Bengio et al. 2003 MLP language model paper (pdf): ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqazVuU2NUbWg4N0RhbmZ6a1M4bmk1MjRsMFpGd3xBQ3Jtc0tuUEFDVUN4VUpOUHVsS1QzTm9mVkN2UVlKRDJNTzEyZEVFNkZLWThBYmkwUkRtTGtCQzQyS3FkLXMyMWYzUWoxQndaUUpHQ3VJb1Z5bzJrNmJsUWtqQ05Obl91NGs5cHBGblVxc2VOdGpPX0hQSWdsdw&q=https%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume3%2Fbengio03a%2Fbengio03a.pdf&v=P6sfmUTpUmc",children:"https://www.jmlr.org/papers/volume3/b..."})]}),"\n",(0,r.jsxs)(n.li,{children:["Good paper illustrating some of the problems with batchnorm in practice: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa0VGNldiSkcxOVpIVlVBeF9VWGlJWTJOQVJFd3xBQ3Jtc0tsbEdCNDl6RU5FQXdoOTFQVmIzX3NWUVpybWtZQnhmbk5naXNVcThfVWFxX2dQaXlJbWJGaHo4T3I2Rk5LSUFiYnVwdFJFaVdGTUNjY2lhNHFqUGZWTGQ5Y1JKZm5hS0E0bVBpeUNOTTVMYkRMdlFfNA&q=https%3A%2F%2Farxiv.org%2Fabs%2F2105.07576&v=P6sfmUTpUmc",children:"https://arxiv.org/abs/2105.07576"})]}),"\n"]}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"course-notes",children:"Course Notes"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h1,{id:"0-this-is-the-code-we-end-up-with-from-last-section-we-will-fix-some-issues-with-the-code-today",children:"0. This is the code we end up with from last section. We will fix some issues with the code today."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nwords = open('makemore-master/names.txt', 'r').read().splitlines()\nwords[:8]\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n# build the dataset\nblock_size = 3  # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        w = '...' + w + \".\"\n        for c1, c2, c3, c4 in zip(w, w[1:], w[2:], w[3:]):\n            X.append([stoi[ele] for ele in [c1,c2,c3]])\n            Y.append(stoi[c4])\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\n\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])  # 80%\nXdev, Ydev = build_dataset(words[n1:n2])  # 10%\nXte, Yte = build_dataset(words[n2:])  # 10%s\n\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\nb1 = torch.randn(n_hidden,                        generator=g)\nW2 = torch.randn((n_hidden, vocab_size),          generator=g)\nb2 = torch.randn(vocab_size,                      generator=g) \n\nparameters = [C, W1, b1, W2, b2]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n\n    emb = C[Xb].view([Xb.size(0), -1])  # [7, 3, 2] => [7, 6]\n    tanh = torch.tanh(emb @ W1 + b1)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, Yb)\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i < 100000 else 0.01  # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0:  # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\nplt.plot(lossi)\n\n"})}),"\n",(0,r.jsx)(n.p,{children:"The code plots the below"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042308448.png",alt:""})}),"\n",(0,r.jsx)(n.h1,{id:"1-avoid-hockey-shape-loss-function",children:"1. Avoid Hockey Shape Loss Function"}),"\n",(0,r.jsx)(n.p,{children:"In the above code, the parameters are not initialized well, original logits are a bit extreme (e.g., 4, 5,6, instead of 0.2, -0.2, etc)."}),"\n",(0,r.jsx)(n.p,{children:"This will cause initial loss to be very high (e..g, optimal loss = 2, but initial loss = 27)."}),"\n",(0,r.jsx)(n.p,{children:"This will cause for the first few rounds of batches, we are only making the weights less to the extreme."}),"\n",(0,r.jsx)(n.p,{children:"To avoid that, when we initialize parameters, we can do"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"g = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) \nb1 = torch.randn(n_hidden,                        generator=g) \nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n"})}),"\n",(0,r.jsx)(n.p,{children:"this way the loss function will look like below"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042308156.png",alt:""})}),"\n",(0,r.jsx)(n.h1,{id:"2--avoid-dead-neurons-from-saturated-tanh",children:"2.  Avoid Dead Neurons from Saturated Tanh"}),"\n",(0,r.jsx)(n.p,{children:"remember the training function, in the middle there\u2019s a tanh function"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"for i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n\n    emb = C[Xb].view([Xb.size(0), -1])  # [7, 3, 2] => [7, 6]\n    tanh = torch.tanh(emb @ W1 + b1)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, Yb)\n"})}),"\n",(0,r.jsx)(n.p,{children:"However, as we can see in the below image, almost all activation function has a place where gradient is almost zero, when the data is very big or very small"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042309938.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"We can examine our tanh layer to see what it looks like"}),"\n",(0,r.jsxs)(n.p,{children:["Using the tanh layer from the ",(0,r.jsx)(n.strong,{children:"first"})," step/epoch to plot a histogram:"]}),"\n",(0,r.jsxs)(n.p,{children:["Why we can use the ",(0,r.jsx)(n.strong,{children:"first epoch,"})," instead of doing it at the end of the training?"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["that\u2019s because we don\u2019t want the data to be ",(0,r.jsx)(n.strong,{children:"initialized"})," wrong."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"plt.hist(tanh.view(-1).tolist(), 50)\nplt.show()\n"})}),"\n",(0,r.jsx)(n.p,{children:"It looks like this"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042309419.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"This means the tanh layer is \u201cvery activated / saturated\u201d. This is a bad thing, because the gradient of tanh at -1 and 1 are almost zero (see the above plots for the activation functions)."}),"\n",(0,r.jsx)(n.p,{children:"We can also see how many data has abs > 0.99"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"plt.figure(figsize=(20, 5))\nplt.imshow(abs(tanh)>0.99, cmap='grey')\nplt.colorbar()\nplt.show()\n"})}),"\n",(0,r.jsx)(n.p,{children:"there\u2019s lots of white pixels, meaning those that have abs > 0.99"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042311663.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"why did this happen? let\u2019s check the input to tanh, which is emb@W1 + b1"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"plt.hist((emb @ W1 + b1).view(-1).tolist(), 50)\nplt.show()\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042312964.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"We can see that many data are quite extreme (with abs over 3). If we check back the tanh plot, tanh(3) is already 0.9951"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\nx = torch.arange(-10, 10, 0.1)\ntanh = torch.tanh(x)\nplt.plot(x.tolist(), tanh.tolist())\nplt.show()\n\nprint(torch.tanh(torch.tensor(1)))\nprint(torch.tanh(torch.tensor(2)))\nprint(torch.tanh(torch.tensor(3)))\n"""\noutput:\ntensor(0.7616)\ntensor(0.9640)\ntensor(0.9951)\n"""\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042312540.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"To fix this issue, we need to make sure W1 and b1 are not too extreme either."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"g = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.1\nb1 = torch.randn(n_hidden,                        generator=g) * 0.01\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * \n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Confirming the above is effective:"})}),"\n",(0,r.jsxs)(n.p,{children:["Here\u2019s the new ",(0,r.jsx)(n.code,{children:"emb@W1 + b1"})," data (at the first epoch)"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042312631.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"Here\u2019s the new histogram of the tanh data (at the first epoch)"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042313033.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"Here\u2019s the imshow graph, every data is 0"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042313346.png",alt:""})}),"\n",(0,r.jsx)(n.h1,{id:"3--final-way-to-verify-if-the-above-optimization-make-sense",children:"3.  Final way to verify if the above optimization make sense"}),"\n",(0,r.jsx)(n.p,{children:"The Final way to verify if the above optimization make sense is to check the loss."}),"\n",(0,r.jsx)(n.p,{children:"With the same amount of epoch, with these updates, the final loss should be less."}),"\n",(0,r.jsx)(n.p,{children:"These updates are very important for more complicated models, as the model will be much less tolerable to biases introduced from initialization."}),"\n",(0,r.jsx)(n.h1,{id:"4-calculate-the-init-scale-kaiming-init",children:"4. Calculate the Init Scale: Kaiming Init"}),"\n",(0,r.jsx)(n.p,{children:"How do we know what multiplier is best for W1?"}),"\n",(0,r.jsxs)(n.p,{children:['Based on the Kaiming init paper ("Kaiming init" paper:',(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjVBbU9tX2NXeDRYclo4R1hMT3A0eU9iUFRmZ3xBQ3Jtc0ttSmZ3N2IyTks5RDhyM1hZcjhYTHpXQWtBdXlHYzZQY050OEtBcGZmLVljbFVGS3VwLUxpbWhYb0d6VXJ2NkJTdEJoUC1rVDY0LS1neDdZZVVURmdIZDRVUkExTzh1czc0a2VKM05WSlo0VmhtQ2l6TQ&q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.01852&v=P6sfmUTpUmc",children:"https://arxiv.org/abs/1502.01852"}),"), we need to do"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"W1 = (W1 * 5/3) / 30 ** 0.5\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["why divide by ",(0,r.jsx)(n.code,{children:"30 ** 0.5"})," ?"]})}),"\n",(0,r.jsx)(n.p,{children:"remember W1 is initialized to be"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n"})}),"\n",(0,r.jsx)(n.p,{children:"n-embd = 10, block_size = 3"}),"\n",(0,r.jsx)(n.p,{children:"Intuitively, if a data originally has mean = 0, std = 1"}),"\n",(0,r.jsxs)(n.p,{children:["then the 30 items of W1 increased the std to be ",(0,r.jsx)(n.code,{children:"sqrt(30)"})]}),"\n",(0,r.jsx)(n.p,{children:"you can imagine that as multiply a set of data with mean = 0, std = 1 by 30"}),"\n",(0,r.jsx)(n.p,{children:"the resulting std is roughly sqrt(30)"}),"\n",(0,r.jsxs)(n.p,{children:["So to make the std back to 1, we devide it by ",(0,r.jsx)(n.code,{children:"sqrt(30)"})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why multiply by 5/3 ?"})}),"\n",(0,r.jsx)(n.p,{children:"Intuitively, this is because tanh is a squashing function, we need to counter the squashing effect by giving it some additional gain."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsxs)(n.strong,{children:["In Torch, the above can be done directly by ",(0,r.jsx)(n.code,{children:"torch.nn.kaiming_normal_"})]})," ",(0,r.jsx)(n.a,{href:"https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_",children:"https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_"})]}),"\n",(0,r.jsxs)(n.p,{children:["What we described above is the default ",(0,r.jsx)(n.code,{children:"fan_in"})," mode."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Will the scaling be compatible with backpropagation?"})}),"\n",(0,r.jsxs)(n.p,{children:['Yes, according to this "Kaiming init" paper:',(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbjVBbU9tX2NXeDRYclo4R1hMT3A0eU9iUFRmZ3xBQ3Jtc0ttSmZ3N2IyTks5RDhyM1hZcjhYTHpXQWtBdXlHYzZQY050OEtBcGZmLVljbFVGS3VwLUxpbWhYb0d6VXJ2NkJTdEJoUC1rVDY0LS1neDdZZVVURmdIZDRVUkExTzh1czc0a2VKM05WSlo0VmhtQ2l6TQ&q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.01852&v=P6sfmUTpUmc",children:"https://arxiv.org/abs/1502.01852"}),", the scaling is not only appropriate for forwarding with std = 1, it is also compatible with backpropagation."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Updated initialization script:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"g = torch.Generator().manual_seed(2147483647) \nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / (n_embd * block_size) ** 0.5\nb1 = torch.randn(n_hidden,                        generator=g) * 0.01\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n"})}),"\n",(0,r.jsx)(n.h1,{id:"5-batch-normalization",children:"5. Batch Normalization"}),"\n",(0,r.jsx)(n.p,{children:"The above methods more finicky and fragile, compared to some modern methods to avoid the above issues."}),"\n",(0,r.jsxs)(n.p,{children:["Intuition: If our goal is to have the hidden layer has 0 mean and 1 std, why don\u2019t we just ",(0,r.jsx)(n.strong,{children:"batch normalize"})," them to have 0 mean and 1 std?"]}),"\n",(0,r.jsxs)(n.p,{children:["from this paper: BatchNorm paper: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWtfREtEMndUb0E2VHpHako4TFpDWmlMaXg2d3xBQ3Jtc0tuOVRjMXBseUY3djgwbU83eFpIQldTTGxVY0xyRWtBa2tidjk4al93RExPOVRuMjF4M3FYU1JDZUNkR00zVVVHcmJNTUFXYlFVS2Qwd19LX1RJRmVwd29GTXVnWkZuNWNSMC1KZ21kRkJWd19vTVhTdw&q=https%3A%2F%2Farxiv.org%2Fabs%2F1502.03167&v=P6sfmUTpUmc",children:"https://arxiv.org/abs/1502.03167"})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042313135.png",alt:""})}),"\n",(0,r.jsx)(n.p,{children:"Implementation:"}),"\n",(0,r.jsx)(n.p,{children:"initialize another 2 hyperparameters:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"bngain = torch.ones(n_hidden)\nbnbias = torch.zeros(n_hidden)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Then, during training"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"for i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n\n    emb = C[Xb].view([Xb.size(0), -1])  # [7, 3, 2] => [7, 6]\n    hpreact = emb @ W1 + b1\n    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n    tanh = torch.tanh(hpreact)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, Yb)\n\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Think:"})," we already have it normalized to mean=0 std=1 during the normalize step, ",(0,r.jsx)(n.strong,{children:"why do we need a scale and shift step?"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Idea:"})," We don\u2019t want the data to keep being a standard normal distribution during training, we want to introduce some variabilities so that the weights can scale and shift and allow us to back propagate and have the model fit to the training data."]}),"\n",(0,r.jsxs)(n.p,{children:["Batch normalization layer is usually added after a ",(0,r.jsx)(n.strong,{children:"linear"})," or ",(0,r.jsx)(n.strong,{children:"convolutional"})," layer."]}),"\n",(0,r.jsx)(n.h1,{id:"6-two-additional-feature--bugfix-for-batch-normalization",children:"6. Two Additional Feature / Bugfix for Batch Normalization"}),"\n",(0,r.jsx)(n.h2,{id:"61-running-bn_mean-and-bn_std",children:"6.1 Running bn_mean and bn_std."}),"\n",(0,r.jsxs)(n.p,{children:["As we added this line ",(0,r.jsx)(n.code,{children:"hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias"})," during training, we need to add it in eval too."]}),"\n",(0,r.jsx)(n.p,{children:"However, issue is, when we are evaluating one sample at a time, we don\u2019t know the mean and std."}),"\n",(0,r.jsx)(n.p,{children:"To resolve that, there are two ways:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"At the end of training, run one pass to get the overall bn_mean and bn_std, use that as the eval bn_mean and bn_std."}),"\n",(0,r.jsx)(n.li,{children:"(More commonly used): calculate running bn_mean and bn_std as a parallel step during training."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"To do that, on top of the existing training code,"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"for i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n\n    emb = C[Xb].view([Xb.size(0), -1])  # [7, 3, 2] => [7, 6]\n    hpreact = emb @ W1 + b1\n    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n    tanh = torch.tanh(hpreact)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, Yb)\n"})}),"\n",(0,r.jsx)(n.p,{children:"We add a few lines so that it looks like"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"running_bn_mean, running_bn_std = 0, 1\nfor i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n\n    emb = C[Xb].view([Xb.size(0), -1])  # [7, 3, 2] => [7, 6]\n    hpreact = emb @ W1 + b1\n    bn_mean = hpreact.mean(0, keepdim=True)\t\t    \n\t\tbn_std = hpreact.std(0, keepdim=True)\n\n    with torch.no_grad():\n\n\t\t\t\trunning_bn_mean = 0.999 * running_bn_mean + 0.001 * bn_mean\n\t\t\t\trunning_bn_std = 0.999 * running_bn_std + 0.001 * bn_std\n\t\t    \n    hpreact = bngain * (hpreact - bn_mean) / bn_std + bnbias\n    tanh = torch.tanh(hpreact)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, Yb)\n"})}),"\n",(0,r.jsx)(n.p,{children:"This is how torch does it in the batch normalization layer too."}),"\n",(0,r.jsx)(n.h2,{id:"62-removal-of-b1",children:"6.2: Removal of b1"}),"\n",(0,r.jsx)(n.p,{children:"We no longer need b1, as it will be subtracted in the next step during normalization."}),"\n",(0,r.jsx)(n.p,{children:"so we can instead just do"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"hpreact = emb @ W1\n"})}),"\n",(0,r.jsx)(n.h1,{id:"7-side-effect-of-batch-normalization",children:"7. Side Effect of Batch Normalization"}),"\n",(0,r.jsx)(n.p,{children:"generating logits are no longer"}),"\n",(0,r.jsx)(n.p,{children:"Before, samples in the batches are processed independently."}),"\n",(0,r.jsx)(n.p,{children:"Now, samples inside the batches can be affected by other samples."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"e.g., one sample having high value will cause other samples to reduce the value"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"But, this is a good thing."}),"\n",(0,r.jsx)(n.p,{children:"It pads out other data, kind of a data augmentation. By introducing these noises, it makes it harder for the model to overfit for single data samples."}),"\n",(0,r.jsx)(n.p,{children:"However, it couples the effect of the model on each individual batches. This would cause issues. So people have been trying to remove it and move on to other normalizations, such as layer normalizations. But it has been hard, as it is quite effective to use batch normalization."}),"\n",(0,r.jsxs)(n.h1,{id:"8-implement-batch-normalization-using-nnbatchnorm1d-tips--caveats",children:["8. Implement Batch Normalization using ",(0,r.jsx)(n.code,{children:"nn.BatchNorm1d"})," (tips & caveats)"]}),"\n",(0,r.jsx)(n.p,{children:"To recap, the code is what we discussed from above about implementing batch normalization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / (n_embd * block_size) ** 0.5\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\nbngain = torch.ones(n_hidden)\nbnbias = torch.zeros(n_hidden)\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n  p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 2\nbatch_size = 32\nlossi = []\ntanh_data = []\n\nbnmean_running, bnstd_running = 0, 1\n\nfor i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y\n\n    emb = C[Xb].view([Xb.size(0), -1])  # [7, 3, 2] => [7, 6]\n    hpreact = emb @ W1 + b1\n    bnmean_i = hpreact.mean(0, keepdim=True)\n    bnstd_i = hpreact.std(0, keepdim=True)\n    \n    hpreact = bngain * (hpreact - bnmean_i) / bnstd_i + bnbias\n    \n    with torch.no_grad():\n        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean_i\n        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstd_i\n    \n    tanh = torch.tanh(hpreact)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, Yb)\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i < 100000 else 0.01  # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0:  # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n"})}),"\n",(0,r.jsx)(n.p,{children:"in the above, this is how BatchNorm is running"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"......during init\nbngain = torch.ones(n_hidden)\nbnbias = torch.zeros(n_hidden)\n\n......during training\n    bnmean_i = hpreact.mean(0, keepdim=True)\n    bnstd_i = hpreact.std(0, keepdim=True)\n    \n    hpreact = bngain * (hpreact - bnmean_i) / bnstd_i + bnbias\n    \n    with torch.no_grad():\n        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean_i\n        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstd_i\n"})}),"\n",(0,r.jsx)(n.p,{children:"Now let\u2019s do the same using nn.Linear and nn.BatchNorm1d"}),"\n",(0,r.jsxs)(n.p,{children:["check torch\u2019s documentation ",(0,r.jsx)(n.a,{href:"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d",children:"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d"})]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["torch.nn.BatchNorm1d(",(0,r.jsx)(n.em,{children:"num_features"}),",\xa0",(0,r.jsx)(n.em,{children:"eps=1e-05"}),",\xa0",(0,r.jsx)(n.em,{children:"momentum=0.1"}),",\xa0",(0,r.jsx)(n.em,{children:"affine=True"}),",\xa0",(0,r.jsx)(n.em,{children:"track_running_stats=True"}),",\xa0",(0,r.jsx)(n.em,{children:"device=None"}),",\xa0",(0,r.jsx)(n.em,{children:"dtype=None"}),")"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"to do that in torch, we can say:"}),"\n",(0,r.jsx)(n.p,{children:"batchnorm = nn.BatchNorm1d(n_hidden, momentum=0.001)."}),"\n",(0,r.jsxs)(n.p,{children:["the 0.001 from ",(0,r.jsx)(n.code,{children:"bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean_i"}),"  is the momentum."]}),"\n",(0,r.jsx)(n.p,{children:"If batch size is large, we can use 0.1/0.001 they are all fine,"}),"\n",(0,r.jsx)(n.p,{children:"but if the batch size is small (e.g., like us, we only have 32), it will cause the running mean to fluctuate and have difficulties to converge."}),"\n",(0,r.jsx)(n.p,{children:"Note, before the nn.BatchNorm1d layer, we can set up the hpreact layer to have (bias=False)."}),"\n",(0,r.jsxs)(n.p,{children:["e.g., ",(0,r.jsx)(n.code,{children:"nn.Linear(bias=False)"}),". This is same as we remove b1 in the above implementation, as that parameter is useless - they get subtracted during the batch norm layer anyway."]}),"\n",(0,r.jsx)(n.h1,{id:"8-implement-all-above-in-pytorch",children:"8. Implement all above in PyTorch"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport random\nfrom torch import nn\n\nwords = open('makemore-master/names.txt', 'r').read().splitlines()\nwords = words\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s: i + 1 for i, s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i: s for s, i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n# build the dataset\nblock_size = 3  # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):\n    X, Y = [], []\n\n    for w in words:\n        w = '...' + w + \".\"\n        for c1, c2, c3, c4 in zip(w, w[1:], w[2:], w[3:]):\n            X.append([stoi[ele] for ele in [c1,c2,c3]])\n            Y.append(stoi[c4])\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8 * len(words))\nn2 = int(0.9 * len(words))\n\nXtr, Ytr = build_dataset(words[:n1])  # 80%\nXdev, Ydev = build_dataset(words[n1:n2])  # 10%\nXte, Yte = build_dataset(words[n2:])  # 10%s\n\n# MLP revisited\nembd_dim = 10  # the dimensionality of the character embedding vectors\nn_hidden = 200  # the number of neurons in the hidden layer of the MLP\n\nclass EmbeddingConcat(nn.Module):\n    def forward(self, X):\n        return X.view(X.size(0), -1)\n\ndef build_network():\n    layers = [\n        nn.Embedding(num_embeddings=vocab_size, embedding_dim=embd_dim),\n        EmbeddingConcat(),\n\n        nn.Linear(in_features=block_size * embd_dim, out_features=n_hidden, bias=False),\n        nn.BatchNorm1d(num_features=n_hidden, momentum=0.001),\n        nn.Tanh(),\n\n        nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=False),\n        nn.BatchNorm1d(num_features=n_hidden, momentum=0.001),\n        nn.Tanh(),\n\n        nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=False),\n        nn.BatchNorm1d(num_features=n_hidden, momentum=0.001),\n        nn.Tanh(),\n\n        nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=False),\n        nn.BatchNorm1d(num_features=n_hidden, momentum=0.001),\n        nn.Tanh(),\n\n        nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=False),\n        nn.BatchNorm1d(num_features=n_hidden, momentum=0.001),\n        nn.Tanh(),\n\n        nn.Linear(in_features=n_hidden, out_features=vocab_size, bias=False),\n        nn.BatchNorm1d(num_features=vocab_size, momentum=0.001),\n\n    ]\n    parameters = []\n    for layer in layers:\n        parameters.extend(list(layer.parameters()))\n    layers[-1].weight.data *= 0.1 # make the last layer less confident\n\n    print(f\"{sum([p.nelement() for p in parameters])}\")\n    for p in parameters:\n        p.requires_grad = True\n    return layers, parameters\n\nbuild_network()\n\nloss_fn = nn.CrossEntropyLoss()\nNUM_EPOCHS = 1\nstep_size = 0.1\n\ndef train(X, Y):\n    layers, parameters = build_network()\n    lossi = []\n    dataset = torch.utils.data.TensorDataset(X, Y)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, drop_last=True) #use drop_last = true to avoid one sample batch\n    for i in range(NUM_EPOCHS):\n        for train_x, train_y in dataloader:\n            for p in parameters:\n                p.grad = None\n            y_pred = train_x\n            for layer in layers:\n                y_pred = layer.forward(y_pred)\n            loss = loss_fn(y_pred, train_y)\n            print(f\"{loss=}\")\n            loss.backward()\n            lossi.append(loss.log10().item())\n            for p in parameters:\n                p.data -= step_size * p.grad\n    plt.plot(lossi)\n    plt.show()\n    return layers\n\ng = torch.Generator().manual_seed(42)\ndef generate():\n    layers = train(Xtr, Ytr)\n    word_lst = []\n    for layer in layers:\n        layer.eval()\n    with torch.no_grad():\n        for _ in range(100):\n            xi = torch.zeros(1, 3, dtype=torch.int32)\n            cur_word = []\n            y = '[INIT]'\n            while y != \".\":\n                y_prob = xi\n                for layer in layers:\n                    y_prob = layer.forward(y_prob)\n                y_prob = y_prob.softmax(dim=-1)\n                iy = torch.multinomial(y_prob, num_samples=1, generator=g).item()\n                y = itos[iy]\n                xi = torch.tensor([xi[0, 1:].tolist() + [iy]], dtype=torch.int32)\n                cur_word.append(y)\n            word_lst.append(\"\".join(cur_word))\n\n    print(word_lst)\n\ngenerate()\n"})}),"\n",(0,r.jsx)(n.h1,{id:"10-important-to-remember",children:"10. Important to Remember:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Softmax() should not be put into the model for forward call, as it will mess up with the cross entropy loss function and the backward gradient descent process."}),"\n",(0,r.jsx)(n.li,{children:"Note that the last layer (here the batch norm layer) is made less confident by multiplying its weights by 0.1 at initialization. Below are the reason that\u2019s explained by ChatGPT:"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In neural networks, especially in classification tasks, we sometimes want to make the last layer less confident to prevent the model from overfitting and becoming too certain about its predictions. This is particularly relevant in the following contexts:"}),"\n",(0,r.jsxs)(n.h3,{id:"1-overconfidence-and-generalization",children:["1. ",(0,r.jsx)(n.strong,{children:"Overconfidence and Generalization"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Overconfidence"}),": If the network becomes too confident in its predictions (i.e., assigning very high probabilities to certain classes), it can lead to ",(0,r.jsx)(n.strong,{children:"overfitting"})," on the training data, which harms generalization to unseen data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generalization"}),": By making the last layer less confident, we introduce uncertainty, which encourages the model to rely on the patterns in the data rather than memorizing specific details of the training set."]}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"2-softmax-saturation",children:["2. ",(0,r.jsx)(n.strong,{children:"Softmax Saturation"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"In classification tasks, the softmax function is often used in the final layer to convert logits (raw scores) into probabilities. When the model becomes overconfident, the softmax outputs approach extreme values (close to 0 or 1), which can cause the gradient to vanish during training, slowing down the learning process."}),"\n",(0,r.jsx)(n.li,{children:"Reducing the confidence makes the softmax distribution more spread out, keeping the gradients larger and allowing better gradient flow during backpropagation."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"My understanding is that, as the gradient descent goes, the last layer is getting first affected, so its initial values affects the first few steps of gradient descent the most. So making them less confident is helpful for stabilizing the gradient descent process."}),"\n",(0,r.jsx)(n.h1,{id:"11-ways-to-check-the-gradient-and-issues-with-the-model",children:"11. Ways to check the gradient and issues with the model:"}),"\n",(0,r.jsx)(n.p,{children:"(Note: I didn\u2019t work on this in much detail, as it\u2019s hard to verify how this would impact the final results. I will use these methods during my text classification project)."}),"\n",(0,r.jsx)(n.h2,{id:"111-activation-distribution",children:"11.1 Activation Distribution"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042314268.png",alt:""})}),"\n",(0,r.jsxs)(n.p,{children:["In the first few rounds of training, we want the results from tanh layer to be ",(0,r.jsx)(n.strong,{children:"evenly distributed between -1 and 1."})]}),"\n",(0,r.jsx)(n.h2,{id:"112-gradient-distribution",children:"11.2 Gradient Distribution"}),"\n",(0,r.jsx)(n.p,{children:"We want most gradient to be around 0. and have a fat curve between -0.005, 0.005, etc."}),"\n",(0,r.jsx)(n.p,{children:"We most importantly don\u2019t want the tail to extend to e.g., beyond 1 or even beyond 5."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042314067.png",alt:""})}),"\n",(0,r.jsx)(n.h2,{id:"113-weights-gradient-distribution",children:"11.3 Weights Gradient Distribution"}),"\n",(0,r.jsx)(n.p,{children:"Follow the same rule as the gradient distribution"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042314598.png",alt:""})}),"\n",(0,r.jsx)(n.h2,{id:"114-change-of-update-rate",children:"11.4 Change of Update Rate"}),"\n",(0,r.jsx)(n.p,{children:"We want update rate to converge at 1e-3."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507042314129.png",alt:""})})]})}function l(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);