"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[33669],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var s=t(96540);const o={},a=s.createContext(o);function r(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(a.Provider,{value:n},e.children)}},66652:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"DL Theory 100/Self-Attention step by step implementation","title":"Self-Attention step by step implementation","description":"Step 1: \u5355\u5934attention","source":"@site/docs/02. DL Theory 100/002. Self-Attention step by step implementation.md","sourceDirName":"02. DL Theory 100","slug":"/p/43a96f3c-a9c2-4990-9538-c7601da9d719","permalink":"/notes/docs/p/43a96f3c-a9c2-4990-9538-c7601da9d719","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/02. DL Theory 100/002. Self-Attention step by step implementation.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"created_at":"2025-12-19","page_link":"/p/43a96f3c-a9c2-4990-9538-c7601da9d719","slug":"/p/43a96f3c-a9c2-4990-9538-c7601da9d719"},"sidebar":"tutorialSidebar","previous":{"title":"1. Explain Transformer architecture (components and overall mechanism)","permalink":"/notes/docs/p/73e70ad8-d53b-4524-a52f-a0f338135a3b"},"next":{"title":"RLHF pipeline for aligning LLMs steps SFT reward model PPO vs DPO vs RLAIF","permalink":"/notes/docs/p/1d96dc28-f10a-4f2b-893f-e4223e5be261"}}');var o=t(74848),a=t(28453);const r={created_at:"2025-12-19",page_link:"/p/43a96f3c-a9c2-4990-9538-c7601da9d719",slug:"/p/43a96f3c-a9c2-4990-9538-c7601da9d719"},i=void 0,l={},c=[{value:"Step 1: \u5355\u5934attention",id:"step-1-\u5355\u5934attention",level:2},{value:"step 2: \u4fee\u6539\u6210multi-head attention",id:"step-2-\u4fee\u6539\u6210multi-head-attention",level:2},{value:"Step 3: \u5173\u952e\uff01\uff01\u52a0\u4e0aW_out",id:"step-3-\u5173\u952e\u52a0\u4e0aw_out",level:2},{value:"step 3: update to regular nn module",id:"step-3-update-to-regular-nn-module",level:2}];function d(e){const n={code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"step-1-\u5355\u5934attention",children:"Step 1: \u5355\u5934attention"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\ndef self_attention(X: torch.tensor):\n    B, T, C = X.shape\n    Wq, Wk, Wv = torch.randn(C, C),torch.randn(C, C),torch.randn(C, C)\n    Q = X @ Wq # B, T, C\n    K = X @ Wk # B, T, C\n    V = X @ Wv # B, T, C\n    A = Q @ K.transpose(-1, -2) # B, T, T\n    A = A / C ** 0.5\n    # when C is big, more bigger points (caused by q * v) will appear in A, causing softmax to become sensitive. (called extreme value dominance)\n    mask = torch.tril(torch.ones(T, T)) #lower trangle is 1\n    A = A.masked_fill(mask == 0, -torch.inf) # mask to -torch.inf, softmax to 0\n    # row 1 is for all q1 (same query, all keys) adds up to 1, meaning we do softmax on the columns\n    A = A.softmax(dim=-1)\n    print(A)\n    out = A @ V\n    return out\n"})}),"\n",(0,o.jsx)(n.p,{children:"most important:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"extreme value dominance"}),"\n",(0,o.jsx)(n.li,{children:"for causal: use tril but use where it = 0 to change to -inf"}),"\n",(0,o.jsxs)(n.li,{children:["softmax is reducing at -1 dim, because it should allow attention for the same ",(0,o.jsx)(n.strong,{children:"query"})," sum up to 1"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"step-2-\u4fee\u6539\u6210multi-head-attention",children:"step 2: \u4fee\u6539\u6210multi-head attention"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"B T C => B T H D. C = H * D"}),"\n",(0,o.jsx)(n.li,{children:"transpose 1,2 \u7ef4\uff1a\u56e0\u4e3a\u540e\u9762\u7684\u77e9\u9635\u4e58\u6cd5\u90fd\u662f\u57282\uff0c3\u7ef4\u8fdb\u884c\u7684\uff0c\u653e\u57281\uff0c2\u7ef4\u53ef\u4ee5\u4fdd\u62a4\u5b83\u662fseparate\u7684\u3002"}),"\n",(0,o.jsxs)(n.li,{children:["when putting them back\uff0c\u5148transpose\uff0c\u7136\u540e\u7528 ",(0,o.jsx)(n.code,{children:"reshape(B, T, C)"}),"\uff0c \u4e0d\u8981\u7528view\uff0c\u56e0\u4e3aview\u8981\u8fde\u7eed\u5185\u5b58\u624d\u884c\uff0c\u8fd9\u91cc\u5185\u5b58\u4e0d\u662f\u8fde\u7eed\u7684"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\ndef multi_head_attention(X: torch.tensor, H: int):\n    B, T, C = X.shape\n    Wq, Wk, Wv = torch.randn(C, C,generator=g), torch.randn(C, C,generator=g), torch.randn(C, C,generator=g)\n    Q, K, V = X @ Wq, X @ Wk, X @ Wv\n    D = C // H\n    Q = Q.view(B, T, H, D).transpose(1, 2) # B, H, T, D\n    K = K.view(B, T, H, D).transpose(1, 2)  # B, H, T, D\n    V = V.view(B, T, H, D).transpose(1, 2)  # B, H, T, D\n    A = Q @ K.transpose(-1, -2) # B, H, T, T\n    A = A * D ** -0.5\n    mask = torch.tril(torch.ones(T, T))\n    A = A.masked_fill(mask == 0, -torch.inf)\n    A = A.softmax(dim = -1)\n    out = A @ V\n    out = out.transpose(1,2).reshape(B, T, C)\n    return out\n# X: [banana chocolate cake] B = 1, T = 3, C = 2\n# 0.1,0.9  0.2 0.7 0.3 0.7\n\ninput = torch.tensor([[[0.1, 0.9], [0.2, 0.7], [0.3, 0.7]]])\noutput = multi_head_attention(input,2)\nprint(output)\n"})}),"\n",(0,o.jsx)(n.p,{children:"\u6ce8\u610f \u4ee5\u4e0a\u4e24\u4e2astep\uff0c\u5982\u679c\u7528\u540c\u4e00\u4e2arandon number generator\ng = torch.Generator()\ng.manual_seed(1)\nWq, Wk, Wv \u52a0\u4e0a = randn(C, C, generator = g)\n\u7b2c\u4e00\u6b21forward\u51fa\u6765\u7684\u7ed3\u679c\u662f\u5b8c\u5168\u4e00\u6837\u7684\u3002"}),"\n",(0,o.jsx)(n.h2,{id:"step-3-\u5173\u952e\u52a0\u4e0aw_out",children:"Step 3: \u5173\u952e\uff01\uff01\u52a0\u4e0aW_out"}),"\n",(0,o.jsx)(n.p,{children:"W_out \u662f weighted sum of all heads"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"Wo = torch.randn(C, C)\nout = out @ Wo\n"})}),"\n",(0,o.jsx)(n.h2,{id:"step-3-update-to-regular-nn-module",children:"step 3: update to regular nn module"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, C:int, H:int):\n        super().__init__()\n        assert C % H == 0\n        # because Ws are (C, C) and num_heads is H, we only need these two params\n        self.Wq = nn.Linear(C, C, bias=False)\n        self.Wk = nn.Linear(C, C, bias=False)\n        self.Wv = nn.Linear(C, C, bias=False)\n        self.Wo = nn.Linear(C, C, bias=False)\n        self.H = H\n        self.D = C // self.H\n\n    def forward(self, X):\n        B, T, C = X.shape\n        Q,K,V = self.Wq(X), self.Wk(X), self.Wv(X)\n        Q = Q.view(B, T, self.H, self.D).transpose(1,2)\n        K = K.view(B, T, self.H, self.D).transpose(1, 2)\n        V = V.view(B, T, self.H, self.D).transpose(1, 2)\n    \n        A = Q @ K.transpose(-1, -2)\n        A = A * self.D ** -0.5\n        mask = torch.tril(torch.ones(T, T))\n        A = A.masked_fill(mask == 0, -torch.inf)\n        A = A.softmax(dim = -1)\n        out = A @ V\n        out = out.transpose(1,2).reshape(B, T, C)\n        out = self.Wo(out)\n        return out\n"})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);