"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[99366],{28453:(n,e,a)=>{a.d(e,{R:()=>i,x:()=>l});var t=a(96540);const r={},s=t.createContext(r);function i(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:i(n.components),t.createElement(s.Provider,{value:e},n.children)}},87936:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Zero To Hero/Makemore 2 - MLP","title":"Makemore 2 - MLP","description":"\u94fe\u63a5","source":"@site/docs/04. Zero To Hero/04. Makemore 2 - MLP.md","sourceDirName":"04. Zero To Hero","slug":"/p/698a3541-6dd9-4a29-a38c-18bbe80f8d33","permalink":"/notes/docs/p/698a3541-6dd9-4a29-a38c-18bbe80f8d33","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/04. Zero To Hero/04. Makemore 2 - MLP.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/698a3541-6dd9-4a29-a38c-18bbe80f8d33","slug":"/p/698a3541-6dd9-4a29-a38c-18bbe80f8d33"},"sidebar":"tutorialSidebar","previous":{"title":"Makemore 1.2 - trigram 1","permalink":"/notes/docs/p/34b50f11-765b-4bf1-9b91-7232cedd6a34"},"next":{"title":"Makemore 3 - Activations & Gradients, BatchNorm","permalink":"/notes/docs/p/7d5e92b9-6fc9-48a9-bc43-0ec41f622dc2"}}');var r=a(74848),s=a(28453);const i={created_at:"2025-11-02",page_link:"/p/698a3541-6dd9-4a29-a38c-18bbe80f8d33",slug:"/p/698a3541-6dd9-4a29-a38c-18bbe80f8d33"},l=void 0,o={},d=[{value:"\u94fe\u63a5",id:"\u94fe\u63a5",level:2},{value:"\u7ec3\u4e60 1: \u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e embedding \u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u901a\u8fc7 3 \u4e2a\u5b57\u7b26\u9884\u6d4b\u7b2c 4 \u4e2a\u5b57\u7b26",id:"\u7ec3\u4e60-1-\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e-embedding-\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u901a\u8fc7-3-\u4e2a\u5b57\u7b26\u9884\u6d4b\u7b2c-4-\u4e2a\u5b57\u7b26",level:2},{value:"Example Input &amp; Output",id:"example-input--output",level:2},{value:"\u7b2c 1 \u6b65\uff1a\u5b9e\u73b0\u4e00\u6b21\u524d\u5411\u8f93\u51fa one forward pass, for just one word (emma)",id:"\u7b2c-1-\u6b65\u5b9e\u73b0\u4e00\u6b21\u524d\u5411\u8f93\u51fa-one-forward-pass-for-just-one-word-emma",level:2},{value:"1.1 Build_X_y",id:"11-build_x_y",level:3},{value:"\u7f51\u7edc\u5404\u5c42\u8bf4\u660e",id:"\u7f51\u7edc\u5404\u5c42\u8bf4\u660e",level:2},{value:"\u8f93\u5165 x_train",id:"\u8f93\u5165-x_train",level:3},{value:"C embedding \u67e5\u627e\u8868",id:"c-embedding-\u67e5\u627e\u8868",level:3},{value:"embedding_layer \u8f6c\u6362",id:"embedding_layer-\u8f6c\u6362",level:3},{value:"tanh layer",id:"tanh-layer",level:3},{value:"softmax layer",id:"softmax-layer",level:3},{value:"Step 2: implement training, with epochs",id:"step-2-implement-training-with-epochs",level:2},{value:"Step 3: Optimizing Learning Rate and Batch Size (Important)",id:"step-3-optimizing-learning-rate-and-batch-size-important",level:2},{value:"Step 4: Apply the learning rate and calculate final loss on all data",id:"step-4-apply-the-learning-rate-and-calculate-final-loss-on-all-data",level:2},{value:"Step 5: Compare training and validation loss.",id:"step-5-compare-training-and-validation-loss",level:2},{value:"Step 6: Visualize the embedding",id:"step-6-visualize-the-embedding",level:2},{value:"Step 7: Generate Words",id:"step-7-generate-words",level:3}];function h(n){const e={a:"a",annotation:"annotation",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h2,{id:"\u94fe\u63a5",children:"\u94fe\u63a5"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Makemore \u9879\u76ee GitHub"}),": ",(0,r.jsx)(e.a,{href:"https://github.com/karpathy/makemore",children:"https://github.com/karpathy/makemore"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"\u8bfe\u7a0b notebook"}),": ",(0,r.jsx)(e.a,{href:"https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb",children:"makemore_part2_mlp.ipynb"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Bengio et al. 2003 \u8bba\u6587"}),": ",(0,r.jsx)(e.a,{href:"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf",children:"MLP Language Model PDF"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"PyTorch internals blog"}),": ",(0,r.jsx)(e.a,{href:"http://blog.ezyang.com/2019/05/pytorch-internals/",children:"http://blog.ezyang.com/2019/05/pytorch-internals/"})]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"\u7ec3\u4e60-1-\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e-embedding-\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u901a\u8fc7-3-\u4e2a\u5b57\u7b26\u9884\u6d4b\u7b2c-4-\u4e2a\u5b57\u7b26",children:"\u7ec3\u4e60 1: \u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e embedding \u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u901a\u8fc7 3 \u4e2a\u5b57\u7b26\u9884\u6d4b\u7b2c 4 \u4e2a\u5b57\u7b26"}),"\n",(0,r.jsx)(e.p,{children:"\u4f7f\u7528 Bengio 2003 \u8bba\u6587\u4e2d\u7684\u7f51\u7edc\u67b6\u6784\uff1a"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507032221327.png",alt:"MLP Architecture"})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"example-input--output",children:"Example Input & Output"}),"\n",(0,r.jsxs)(e.p,{children:["\u4ee5\u5355\u8bcd ",(0,r.jsx)(e.code,{children:"emma"})," \u4e3a\u4f8b\uff0c\u521d\u59cb x \u548c y \u5982\u4e0b\uff1a"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"... -> e\n..e -> m\n.em -> m\nemm -> a\nmma -> .\n"})}),"\n",(0,r.jsxs)(e.p,{children:["\u4e3a\u4ec0\u4e48\u8981\u7528",(0,r.jsx)(e.code,{children:"..."})," \u5f00\u5934\uff1f\u56e0\u4e3a\u8fd9\u6837\u6211\u4eec\u80fd\u591f\u9884\u6d4b\u9996\u5b57\u7b26\u51fa\u73b0\u7684\u6982\u7387\uff0c\u65e0\u9700\u5355\u72ec\u6784\u9020\u4e00\u4e2a\u5206\u5e03\u3002"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"\u7b2c-1-\u6b65\u5b9e\u73b0\u4e00\u6b21\u524d\u5411\u8f93\u51fa-one-forward-pass-for-just-one-word-emma",children:"\u7b2c 1 \u6b65\uff1a\u5b9e\u73b0\u4e00\u6b21\u524d\u5411\u8f93\u51fa one forward pass, for just one word (emma)"}),"\n",(0,r.jsx)(e.h3,{id:"11-build_x_y",children:"1.1 Build_X_y"}),"\n",(0,r.jsx)(e.p,{children:"\u5148\u5047\u8bbe\u53ea\u6709itos\u548cstoi\u53ea\u67094\u4e2a\uff0c.,a, e, m"}),"\n",(0,r.jsx)(e.h2,{id:"\u7f51\u7edc\u5404\u5c42\u8bf4\u660e",children:"\u7f51\u7edc\u5404\u5c42\u8bf4\u660e"}),"\n",(0,r.jsx)(e.h3,{id:"\u8f93\u5165-x_train",children:"\u8f93\u5165 x_train"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u5171 5 \u6761\u8bb0\u5f55\uff0c\u6bcf\u6761\u542b 3 \u4e2a\u5b57\u7b26 -> 5 x 3"}),"\n",(0,r.jsxs)(e.li,{children:["one hot \u4ee5\u540e\u53d8\u6210 ",(0,r.jsx)(e.code,{children:"5 * 3 * 4"})]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"c-embedding-\u67e5\u627e\u8868",children:"C embedding \u67e5\u627e\u8868"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u5047\u8bbe\u6bcf\u4e2a\u5b57\u7b26\u6709 2 \u7ef4\u5ea6 embedding"}),"\n",(0,r.jsx)(e.li,{children:"\u5219\u5168\u8868\u4e3a 4 x 2"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"embedding_layer-\u8f6c\u6362",children:"embedding_layer \u8f6c\u6362"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u6bcf 3 \u4e2a\u5b57\u7b26 -> 6 \u4e2a\u503c"}),"\n",(0,r.jsx)(e.li,{children:"\u7ed3\u679c: 5 x 6"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"tanh-layer",children:"tanh layer"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u5168\u8fde\u5c42\uff0c\u5047\u8bbe 100 \u4e2a\u5143 -> 6 x 100"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"softmax-layer",children:"softmax layer"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"\u8f93\u51fa 27 \u7c7b\u578b\u6982\u7387 -> 100 x 27"}),"\n",(0,r.jsxs)(e.li,{children:["\u603b\u7ed3\u6982\u7387\u540e\u8ba1\u7b97\u635f\u5931: ",(0,r.jsxs)(e.span,{className:"katex",children:[(0,r.jsx)(e.span,{className:"katex-mathml",children:(0,r.jsx)(e.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,r.jsxs)(e.semantics,{children:[(0,r.jsxs)(e.mrow,{children:[(0,r.jsx)(e.mo,{children:"\u2212"}),(0,r.jsx)(e.mo,{children:"\u2211"}),(0,r.jsx)(e.mi,{children:"log"}),(0,r.jsx)(e.mo,{children:"\u2061"}),(0,r.jsx)(e.mo,{stretchy:"false",children:"("}),(0,r.jsx)(e.mi,{children:"p"}),(0,r.jsx)(e.mo,{stretchy:"false",children:")"})]}),(0,r.jsx)(e.annotation,{encoding:"application/x-tex",children:"-\\sum \\log(p)"})]})})}),(0,r.jsx)(e.span,{className:"katex-html","aria-hidden":"true",children:(0,r.jsxs)(e.span,{className:"base",children:[(0,r.jsx)(e.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,r.jsx)(e.span,{className:"mord",children:"\u2212"}),(0,r.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,r.jsx)(e.span,{className:"mop op-symbol small-op",style:{position:"relative",top:"0em"},children:"\u2211"}),(0,r.jsx)(e.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,r.jsxs)(e.span,{className:"mop",children:["lo",(0,r.jsx)(e.span,{style:{marginRight:"0.01389em"},children:"g"})]}),(0,r.jsx)(e.span,{className:"mopen",children:"("}),(0,r.jsx)(e.span,{className:"mord mathnormal",children:"p"}),(0,r.jsx)(e.span,{className:"mclose",children:")"})]})})]})]}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"names = ['emma']\nitos = '.aem'  \nstoi = {c: i for i, c in enumerate(itos)}  \n  \n  \ndef build_X_y(names):  \n    X_raw, y_raw = [], []  \n    for name in names:  \n        name = \"...\" + name + \".\"  \n        for c1, c2, c3, c4 in zip(name, name[1:], name[2:], name[3:]):  \n            X_raw.append([stoi[c] for c in [c1, c2, c3]])  \n            y_raw.append(stoi[c4])  \n    X = F.one_hot(torch.tensor(X_raw), num_classes = len(stoi)).float()  \n    y = F.one_hot(torch.tensor(y_raw), num_classes = len(stoi))  \n    return X, y  \n  \nX, y = build_X_y(names)\nprint(X)\n"})}),"\n",(0,r.jsx)(e.p,{children:"output:\ndimension of X: 5 * 3 * 4"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"tensor([[[1., 0., 0., 0.],\n         [1., 0., 0., 0.],\n         [1., 0., 0., 0.]],\n\n        [[1., 0., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.]],\n\n        [[1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [0., 0., 0., 1.]],\n\n        [[0., 0., 1., 0.],\n         [0., 0., 0., 1.],\n         [0., 0., 0., 1.]],\n\n        [[0., 0., 0., 1.],\n         [0., 0., 0., 1.],\n         [0., 1., 0., 0.]]])\n"})}),"\n",(0,r.jsx)(e.p,{children:"can be understood as"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"1000, 1000, 1000,\n1000, 1000, 0010,\n1000, 0010, 0001,\n0010, 0001, 0001,\n0001, 0001, 0100\n"})}),"\n",(0,r.jsx)(e.p,{children:"Let\u2019s start with implementing one forward pass, for just one word (emma), using the full 27 chars to int."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"chars = '.abcdefghijklmnopqrstuvwxyz'\nstoi = {c: i for i, c in enumerate(chars)}\nnames = ['emma']\nx_train_raw = []\ny_train_raw = []\nfor name in names:\n    name = '...' + name + '...'\n    for c1, c2, c3, c4 in zip(name, name[1:], name[2:], name[3:]):\n        x_train_raw.append([stoi[c1], stoi[c2], stoi[c3]])\n        y_train_raw.append(stoi[c4])\n        \nX = torch.tensor(x_train_raw)  # [7, 3]\ny = torch.tensor(y_train_raw)  # [7]\nC = torch.randn([27, 2])\nemb = C[X].view([X.size(0), -1])  # [7, 3, 2] => [7, 6]\nW1 = torch.randn([6, 100])\nb1 = torch.randn(100)\ntanh = torch.tanh(emb @ W1 + b1) # [7, 100]\nW2 = torch.randn([100, 27])\nb2 = torch.randn(27)\nlogits = tanh @ W2 + b2  # [7, 27]\nloss = F.cross_entropy(logits, y)\nprint(loss)\n"})}),"\n",(0,r.jsx)(e.h2,{id:"step-2-implement-training-with-epochs",children:"Step 2: implement training, with epochs"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn.functional as F\n\n"""\n================================ 1. read data ======================================\n"""\nnames = open("makemore-master/names.txt", \'r\').read().splitlines()\nchars = \'.abcdefghijklmnopqrstuvwxyz\'\nstoi = {c: i for i, c in enumerate(chars)}\n# names = [\'emma\']\nx_train_raw = []\ny_train_raw = []\nfor name in names:\n    name = \'...\' + name + \'...\'\n    for c1, c2, c3, c4 in zip(name, name[1:], name[2:], name[3:]):\n        x_train_raw.append([stoi[c1], stoi[c2], stoi[c3]])\n        y_train_raw.append(stoi[c4])\n\n"""\n================================ 2. initialize parameters, set their gradient to 0 ======================================\n"""\nC = torch.randn([27, 2], requires_grad=True)\nW1 = torch.randn([6, 100], requires_grad=True)\nb1 = torch.randn(100, requires_grad=True)\nW2 = torch.randn([100, 27], requires_grad=True)\nb2 = torch.randn(27, requires_grad=True)\nparams = [C, W1, b1, W2, b2]\n\n"""\n============================== 3. training ==================================================\n"""\nnum_epoch = 100\nlr = 0.1\nX_full = torch.tensor(x_train_raw)  # [7, 3]\ny_full = torch.tensor(y_train_raw)  # [7]\nfor epoch in range(num_epoch):\n    ix = torch.randint(0, X_full.size(0), (32, ))\n    X = X_full[ix]\n    y = y_full[ix]\n    emb = C[X].view([X.size(0), -1])  # [7, 3, 2] => [7, 6]\n    tanh = torch.tanh(emb @ W1 + b1)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, y)\n    for p in params:\n        p.grad = None\n    loss.backward()\n    print(loss)\n    for p in params:\n        p.data -= lr * p.grad\n\n'})}),"\n",(0,r.jsx)(e.h2,{id:"step-3-optimizing-learning-rate-and-batch-size-important",children:"Step 3: Optimizing Learning Rate and Batch Size (Important)"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'num_epoch = 1000\nlri = torch.linspace(-3, 0, 1000)\nlre = 100 ** lri\nX_full = torch.tensor(x_train_raw)  # [7, 3]\ny_full = torch.tensor(y_train_raw)  # [7]\nlossi = []\nfor epoch in range(num_epoch):\n    ix = torch.randint(0, X_full.size(0), (32, ))\n    X = X_full[ix]\n    y = y_full[ix]\n    emb = C[X].view([X.size(0), -1])  # [7, 3, 2] => [7, 6]\n    tanh = torch.tanh(emb @ W1 + b1)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, y)\n    for p in params:\n        p.grad = None\n    loss.backward()\n    lr = lre[epoch].item()\n    for p in params:\n        p.data -= lr * p.grad\n    print(loss)\n    lossi.append(loss.item())\n\nimport matplotlib.pyplot as plt\nplt.plot(lri, lossi)\nplt.show()\nlowest_index = torch.tensor(lossi).argmin(0)\nprint(lri[lowest_index])\nprint(lre[lowest_index])\n\n"""\noutput:\ntensor(-1.0060)\ntensor(0.0986)\nmeaning Lowest learning rate is at around 10**(-1), i.e., 0.1\n"""\n'})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507032223412.png",alt:""})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"TRICK 1: Why can we use ONE PASS to decide the best learning rate?"})}),"\n",(0,r.jsx)(e.p,{children:"Explanation (That I put to discord):"}),"\n",(0,r.jsx)(e.p,{children:"I think I have came to an understanding of why Andrej used this method. So, let's assume 0.01 is the optimal learning rate. then"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"if smaller than 0.01, we can always get the best params, with lowest loss, it just take more time."}),"\n",(0,r.jsx)(e.li,{children:"if bigger than 0.01, that's when we will take less time, but results will be unstable."}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"So, what we need to find is, after which point, does the model becomes unstable. So, the plot uses params smaller than 0.01 to get to the optimal point. and then uses params bigger than 0.01 to understand when it's getting stable. This is a genius method, this is indeed using one shot to find the best learning rate."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"TRICK2: We can also adjust the batch size based on this graph."})}),"\n",(0,r.jsxs)(e.p,{children:["The lower the batch size, the bigger ",(0,r.jsx)(e.strong,{children:"volatility"})," the graph is."]}),"\n",(0,r.jsx)(e.p,{children:"Right now, at around lri = -1, the loss can change in about 2 units (i.e., seems to be from 1.5 to 3.5)."}),"\n",(0,r.jsx)(e.p,{children:"Here\u2019s the graph if we change the batch size to 100, instead of 32:"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507032223817.png",alt:""})}),"\n",(0,r.jsx)(e.p,{children:"Here\u2019s the graph if we change the batch size to 1000"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507032224554.png",alt:""})}),"\n",(0,r.jsx)(e.p,{children:"Based on graphs, we can use batch_size = 100 to lower the volatility. Note that the learning rate needs to be adjusted using the new batch size as well."}),"\n",(0,r.jsx)(e.p,{children:"For the below example, let\u2019s keep using batch size = 32."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsxs)(e.strong,{children:["TRICK 3: We could use ",(0,r.jsx)(e.code,{children:"loss.log10().item()"})," to view the loss as well."]})}),"\n",(0,r.jsx)(e.h2,{id:"step-4-apply-the-learning-rate-and-calculate-final-loss-on-all-data",children:"Step 4: Apply the learning rate and calculate final loss on all data"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'num_epoch = 1000\nlr = 0.1\nX_full = torch.tensor(x_train_raw)  # [7, 3]\ny_full = torch.tensor(y_train_raw)  # [7]\nfor epoch in range(num_epoch):\n    ix = torch.randint(0, X_full.size(0), (32, ))\n    X = X_full[ix]\n    y = y_full[ix]\n    emb = C[X].view([X.size(0), -1])  # [7, 3, 2] => [7, 6]\n    tanh = torch.tanh(emb @ W1 + b1)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, y)\n    for p in params:\n        p.grad = None\n    loss.backward()\n    for p in params:\n        p.data -= lr * p.grad\n    print(loss)\n\nX = X_full\ny = y_full\nemb = C[X].view([X.size(0), -1])  # [7, 3, 2] => [7, 6]\ntanh = torch.tanh(emb @ W1 + b1)  # [7, 100]\nlogits = tanh @ W2 + b2  # [7, 27]\nloss = F.cross_entropy(logits, y)\nprint("loss on full data: ", loss)\n'})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"loss on full data: tensor(2.0710, grad_fn=<NllLossBackward0>)\n"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Note:"})," one good thing about using the learning rate from Step 3, is that we ",(0,r.jsx)(e.strong,{children:"don\u2019t need to update the num_epochs here"})," :) ,"]}),"\n",(0,r.jsx)(e.p,{children:"Under the same num_epoch, If we could achieve that low loss using even smaller learning rate, we could achieve that low loss using this 0.1 learning rate for sure."}),"\n",(0,r.jsx)(e.h2,{id:"step-5-compare-training-and-validation-loss",children:"Step 5: Compare training and validation loss."}),"\n",(0,r.jsx)(e.p,{children:"train, val, test split: usually 90, 10, 10 split."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import random\n\nimport torch\nimport torch.nn.functional as F\n\n"""\n================================ 1. read data ======================================\n"""\nnames = open("makemore-master/names.txt", \'r\').read().splitlines()\nchars = \'.abcdefghijklmnopqrstuvwxyz\'\nstoi = {c: i for i, c in enumerate(chars)}\ndef build_dataset(names):\n    x_raw = []\n    y_raw = []\n    for name in names:\n        name = \'...\' + name + \'...\'\n        for c1, c2, c3, c4 in zip(name, name[1:], name[2:], name[3:]):\n            x_raw.append([stoi[c1], stoi[c2], stoi[c3]])\n            y_raw.append(stoi[c4])\n    return torch.tensor(x_raw), torch.tensor(y_raw)\n\nrandom.shuffle(names)\nn1 = int(0.8 * len(names))\nn2 = int(0.9 * len(names))\nX_tr, y_tr = build_dataset(names[:n1])\nX_va, y_va = build_dataset(names[n1:n2])\nX_te, y_te = build_dataset(names[n2:])\n\n"""\n================================ 2. initialize parameters, set their gradient to 0 ======================================\n"""\ng = torch.Generator().manual_seed(100)\nC = torch.randn([27, 2], requires_grad=True)\nW1 = torch.randn([6, 100], requires_grad=True)\nb1 = torch.randn(100, requires_grad=True)\nW2 = torch.randn([100, 27], requires_grad=True)\nb2 = torch.randn(27, requires_grad=True)\nparams = [C, W1, b1, W2, b2]\n\n"""\n======================== 3. training ==================================================\n"""\nnum_epoch = 1000\nlr = 0.1\nfor epoch in range(num_epoch):\n    ix = torch.randint(0, X_tr.size(0), (32, ))\n    X = X_tr[ix]\n    y = y_tr[ix]\n    emb = C[X].view([X.size(0), -1])  # [7, 3, 2] => [7, 6]\n    tanh = torch.tanh(emb @ W1 + b1)  # [7, 100]\n    logits = tanh @ W2 + b2  # [7, 27]\n    loss = F.cross_entropy(logits, y)\n    for p in params:\n        p.grad = None\n    loss.backward()\n    for p in params:\n        p.data -= lr * p.grad\n    print(loss)\n\n"""\n======================== 4. use validation set to evaluate the loss ==================================================\n"""\n\n\nX = X_va\ny = y_va\nemb = C[X].view([X.size(0), -1])  # [7, 3, 2] => [7, 6]\ntanh = torch.tanh(emb @ W1 + b1)  # [7, 100]\nlogits = tanh @ W2 + b2  # [7, 27]\nloss = F.cross_entropy(logits, y)\nprint("loss on validation: ", loss)\n\n\n"""\noutput:\nloss on train:  tensor(2.0988, grad_fn=<NllLossBackward0>)\nloss on validation:  tensor(2.1087, grad_fn=<NllLossBackward0>)\n"""\n'})}),"\n",(0,r.jsxs)(e.p,{children:["Note: the training loss and test loss is roughly ",(0,r.jsx)(e.strong,{children:"equal."})]}),"\n",(0,r.jsxs)(e.p,{children:["this means we are ",(0,r.jsx)(e.strong,{children:"underfitting."})]}),"\n",(0,r.jsx)(e.p,{children:"This is caused by we have a small neural network."}),"\n",(0,r.jsx)(e.p,{children:"Ways to make the model more complex:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Make the tanh layer has more than 100 output neurons. Make it, e.g., 300"}),"\n",(0,r.jsx)(e.li,{children:"Scale up embedding size to > 2."}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Note that learning rate, num_epochs, and batch size needs to be adjusted with these changes too."})}),"\n",(0,r.jsx)(e.h2,{id:"step-6-visualize-the-embedding",children:"Step 6: Visualize the embedding"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"import matplotlib.pyplot as plt\n\nplt.figure(8)\nplt.scatter(C[:, 0].data, C[:, 1].data, s=200)\nfor i in range(C.size(0)):\n    plt.text(C[i, 0].item(), C[i, 1].item(), chars[i], ha='center', va='center', color='white')\nplt.grid('minor')\nplt.show()\n"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202507032225161.png",alt:""})}),"\n",(0,r.jsx)(e.h3,{id:"step-7-generate-words",children:"Step 7: Generate Words"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Note: remember to use torch.multinomial to sample from the distribution, instead of directly picking the largest prob index."})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# ================= generating 10 samples ====================\n\nfor _ in range(10):\n    X = torch.zeros(1, 3, dtype=torch.int32)\n    ix = -1\n    word = []\n    while ix != 0:\n        emb = C[X].view([X.size(0), -1])  # [1, 3, 2] => [1, 6]\n        tanh = torch.tanh(emb @ W1 + b1)  # [1, 100]\n        logits = tanh @ W2 + b2  # [1, 27]\n        prob = F.softmax(logits, dim=1)\n        ix = torch.multinomial(prob, num_samples=1, generator=g).item()\n        X = torch.concat([X[:, 1:], torch.tensor([[ix]])], dim=1) # dim is the dimension that DOES NOT CHANGE after concat\n        word.append(chars[ix])\n\n    print("".join(word))\n    \n    \n"""\noutput:\nari.\nraryho.\naoz.\nrarymh.\naina.\nhur.\nkaysi.\nrmisyysis.\nren.\nrorjoryhkame.\n"""\n'})})]})}function c(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(h,{...n})}):h(n)}}}]);