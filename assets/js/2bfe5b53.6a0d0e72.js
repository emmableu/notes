"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[20153],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var s=i(96540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}},68458:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ML General/Model Compare \u6bd4\u8f83\u6a21\u578b\u597d\u574f","title":"Model Compare \u6bd4\u8f83\u6a21\u578b\u597d\u574f","description":"random forest v.s. gradient boost","source":"@site/docs/05. ML General/1112.Model Compare \u6bd4\u8f83\u6a21\u578b\u597d\u574f.md","sourceDirName":"05. ML General","slug":"/p/56af319b-39c9-41e1-959a-0e470b51ffc0","permalink":"/notes/docs/p/56af319b-39c9-41e1-959a-0e470b51ffc0","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/05. ML General/1112.Model Compare \u6bd4\u8f83\u6a21\u578b\u597d\u574f.md","tags":[],"version":"current","sidebarPosition":1112,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/56af319b-39c9-41e1-959a-0e470b51ffc0","slug":"/p/56af319b-39c9-41e1-959a-0e470b51ffc0"},"sidebar":"tutorialSidebar","previous":{"title":"\u8bc4\u4ef7\u6a21\u578b Model Review","permalink":"/notes/docs/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3"},"next":{"title":"DBSCAN","permalink":"/notes/docs/p/97e0f4a6-4e81-49c1-8216-28e411710e65"}}');var t=i(74848),r=i(28453);const a={created_at:"2025-11-02",page_link:"/p/56af319b-39c9-41e1-959a-0e470b51ffc0",slug:"/p/56af319b-39c9-41e1-959a-0e470b51ffc0"},l=void 0,o={},d=[{value:"random forest v.s. gradient boost",id:"random-forest-vs-gradient-boost",level:2},{value:"Generative v.s. Discrimitive Model",id:"generative-vs-discrimitive-model",level:2},{value:"generative model pros and cons",id:"generative-model-pros-and-cons",level:3},{value:"pros",id:"pros",level:4},{value:"cons",id:"cons",level:4},{value:"discrimitive model pros and cons",id:"discrimitive-model-pros-and-cons",level:3},{value:"\u548cDiscrimitive\u6a21\u578b\u6bd4\u8d77\u6765\uff0cGenerative \u66f4\u5bb9\u6613overfitting\u8fd8\u662funderfitting",id:"\u548cdiscrimitive\u6a21\u578b\u6bd4\u8d77\u6765generative-\u66f4\u5bb9\u6613overfitting\u8fd8\u662funderfitting",level:3},{value:"GMM v.s. K-Means",id:"gmm-vs-k-means",level:2},{value:"gradient boost \u548c adaboost \u533a\u522b",id:"gradient-boost-\u548c-adaboost-\u533a\u522b",level:2}];function c(e){const n={a:"a",br:"br",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.h2,{id:"random-forest-vs-gradient-boost",children:[(0,t.jsx)(n.a,{href:"/pages/63f233/#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95-random-forest",children:"random forest"})," v.s. gradient boost"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/pages/63f233/#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95-random-forest",children:"random forest"}),": the process to train an ensemble where, each constituent model (a decision tree) trains on a random subset of training examples, sampled with replacement."]}),"\n",(0,t.jsxs)(n.li,{children:["gradient boosting:\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models."}),"\n",(0,t.jsx)(n.li,{children:"at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"\u540c\uff1a ensembling methods, combine the outputs from individual trees."}),"\n",(0,t.jsxs)(n.li,{children:["\u5f02\uff1a\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u4ececlassification \u7ed3\u679c\u6765\u8bf4\uff1a\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"comparing to RF, gradient boost have a lot more modeling capacity. They can model very complex relationships and decision boundaries."}),"\n",(0,t.jsx)(n.li,{children:"gradient boost: low bias, high variance, can lead to overfitting"}),"\n",(0,t.jsx)(n.li,{children:"random forest: high bias, low variance, can cause underfitting."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\u4ececlassification \u8fc7\u7a0b\u6765\u8bf4\uff1a\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/pages/63f233/#bagging-%E5%92%8C-boosting-%E7%9A%844-%E7%82%B9%E5%B7%AE%E5%88%AB",children:"/pages/63f233/#bagging-\u548c-boosting-\u76844-\u70b9\u5dee\u522b"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"generative-vs-discrimitive-model",children:"Generative v.s. Discrimitive Model"}),"\n",(0,t.jsxs)(n.p,{children:["from ",(0,t.jsx)(n.a,{href:"https://blog.csdn.net/Oh_MyBug/article/details/104343641",children:"csdn"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"generative mode"}),"l:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"a model that does either of the following:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Creates (generates) new sample from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)"}),"\n",(0,t.jsx)(n.li,{children:"Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"A generative model can understand the distribution of examples or particular features in a dataset."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"discriminative model"}),": A model that predicts labels from a set of one or more features. The goal of a discriminative model is to understand the conditional probability of an output given the features and weights; that is: ",(0,t.jsx)(n.code,{children:"p(output | features, weights)"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"For example, a model that predicts whether an email is spam from features and weights"}),"\n",(0,t.jsx)(n.li,{children:"Contrast with generative model."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/generative-model-0.png",width:"100%"}),"\nsimple generative model includes:\n- naive bayes \n- LDA (Linear Discrimitative Analysis)\n",(0,t.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/generative-model-1.png",width:"100%"}),"\n",(0,t.jsx)(n.h3,{id:"generative-model-pros-and-cons",children:"generative model pros and cons"}),"\n",(0,t.jsx)(n.h4,{id:"pros",children:"pros"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u5b9e\u9645\u4e0a\u5e26\u7684\u4fe1\u606f\u6bd4\u5224\u522b\u6a21\u578b\u4e30\u5bcc"}),"\n",(0,t.jsx)(n.li,{children:"\u7814\u7a76\u5355\u7c7b\u95ee\u9898\u6bd4\u5224\u522b\u6a21\u578b\u7075\u6d3b\u6027\u5f3a"}),"\n",(0,t.jsx)(n.li,{children:"\u80fd\u7528\u4e8e\u6570\u636e\u4e0d\u5b8c\u6574\u60c5\u51b5, \u57fa\u4e8e\u6982\u7387\u5206\u5e03\u7684\u5047\u8bbe\uff0c\u6240\u9700\u7684training data\u8f83\u5c11"}),"\n",(0,t.jsx)(n.li,{children:"\u5f88\u5bb9\u6613\u5c06\u5148\u9a8c\u77e5\u8bc6\u8003\u8651\u8fdb\u53bb"}),"\n",(0,t.jsx)(n.li,{children:"\u7a33\u5065\u578b\u597d\uff0c\u5f53\u6570\u636e\u5448\u73b0\u4e0d\u540c\u7279\u70b9\u65f6\uff0c\u5206\u7c7b\u6027\u80fd\u4e0d\u4f1a\u51fa\u73b0\u592a\u5927\u7684\u5dee\u5f02\u5bf9noise\u6bd4\u8f83robust"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"cons",children:"cons"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u5206\u7c7b:Naive Bayes\u91cc\u9762\u5047\u8bbe\u6bcf\u4e2a\u4e8b\u4ef6\u90fd\u662findependent\u7684\uff0c\u6bd4\u598200|01|10 & 11\u7684\u5206\u7c7b\uff0c\u6837\u672c\u4e0d\u5747\u7684\u65f6\u5019\u53ef\u80fd\u4f1a\u5206\u9519\uff0c\u56e0\u4e3amodel\u53ef\u80fd\u4f1a\u8111\u8865\u4e0d\u5b58\u5728\u7684\u60c5\u51b5"}),"\n",(0,t.jsx)(n.li,{children:"\u5b66\u4e60\u548c\u8ba1\u7b97\u8fc7\u7a0b\u6bd4\u8f83\u590d\u6742"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"discrimitive-model-pros-and-cons",children:"discrimitive model pros and cons"}),"\n",(0,t.jsx)(n.p,{children:"pros"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u5206\u7c7b\u8fb9\u754c\u66f4\u7075\u6d3b\uff0c\u6bd4\u4f7f\u7528\u7eaf\u6982\u7387\u65b9\u6cd5\u6216\u751f\u6210\u6a21\u578b\u5f97\u5230\u7684\u66f4\u9ad8\u7ea7"}),"\n",(0,t.jsx)(n.li,{children:"\u80fd\u6e05\u6670\u7684\u5206\u8fa8\u51fa\u591a\u7c7b\u6216\u67d0\u4e00\u7c7b\u4e0e\u5176\u5b83\u7c7b\u4e4b\u95f4\u7684\u5dee\u5f02\u7279\u5f81"}),"\n",(0,t.jsx)(n.li,{children:"\u5bf9\u4e8e\u591afeature\u7684\u60c5\u51b5\uff0cfeature\u4e4b\u95f4\u591a\u6709correlation\uff0c\u6bd4\u8d77naive bayes\uff0cmodels such as logistic regression is much more robust with correlated features."}),"\n",(0,t.jsx)(n.li,{children:"\u5224\u522b\u6a21\u578b\u7684\u6027\u80fd\u6bd4\u751f\u6210\u6a21\u578b\u8981\u7b80\u5355\uff0c\u6bd4\u8f83\u5bb9\u6613\u5b66\u4e60\ncons"}),"\n",(0,t.jsx)(n.li,{children:"\u4e0d\u80fd\u53cd\u5e94\u8bad\u7ec3\u6570\u636e\u672c\u8eab\u7684\u7279\u6027\uff0c\u53ea\u80fd\u544a\u8bc9\u4f60\u7684\u662f1\u8fd8\u662f2\uff0c\u4e0d\u80fd\u628a\u6574\u4e2a\u573a\u666f\u63cf\u8ff0\u51fa\u6765"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"\u548cdiscrimitive\u6a21\u578b\u6bd4\u8d77\u6765generative-\u66f4\u5bb9\u6613overfitting\u8fd8\u662funderfitting",children:"\u548cDiscrimitive\u6a21\u578b\u6bd4\u8d77\u6765\uff0cGenerative \u66f4\u5bb9\u6613overfitting\u8fd8\u662funderfitting"}),"\n",(0,t.jsxs)(n.p,{children:["\u66f4\u5bb9\u6613underfitting\u3002\nthis ",(0,t.jsx)(n.a,{href:"https://stats.stackexchange.com/questions/91484/do-discriminative-models-overfit-more-than-generative-models",children:"stack exchange"})," has some very math explanations.",(0,t.jsx)(n.br,{}),"\n\u6bd4\u8f83\u7b80\u5355\u7684\u89e3\u91ca\uff1a"]}),"\n",(0,t.jsx)(n.p,{children:"A generative model is typically underfitting because it allows the user to put in more side information in the form of class conditionals."}),"\n",(0,t.jsx)(n.p,{children:"Consider a generative model \ud835\udc5d(\ud835\udc50|\ud835\udc65)=\ud835\udc5d(\ud835\udc50)\ud835\udc5d(\ud835\udc65|\ud835\udc50). If the class conditionals are mulitvariate normals with shared covariance, this will have a linear decision boundary. Thus, the model by itself is just as powerful as a linear SVM or logistic regression."}),"\n",(0,t.jsx)(n.p,{children:"However, a discriminative classifier is much more free in the choice of decision function: it just has to find an appropriate hyperplane. The generative classifier however will need much less samples to find good parameters if the assumptions are valid."}),"\n",(0,t.jsx)(n.p,{children:"Sorry, this is rather handwavy and there is no hard math behind it. But it is an intuition."}),"\n",(0,t.jsx)(n.h2,{id:"gmm-vs-k-means",children:"GMM v.s. K-Means"}),"\n",(0,t.jsx)(n.p,{children:"k-means:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A clustering algorithm that clusters samples by:\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Assigns each sample to the closest center points."}),"\n",(0,t.jsx)(n.li,{children:"Iteratively determines the best k center points."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"GMM:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["A clustering algorithm that clusters samples by iteratively:\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"assign the probablilities each sample belong to each cluster, and"}),"\n",(0,t.jsx)(n.li,{children:"iteratively determine the best miu, and covariance big sigma for each of the clusters"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"keep iterating until it reaches convergence."}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["\u76f8\u540c\u70b9",(0,t.jsx)(n.br,{}),"\n\u90fd\u662f\u8fed\u4ee3\u6267\u884c\u7684\u7b97\u6cd5\uff0c\u4e14\u8fed\u4ee3\u7684\u7b56\u7565\u4e5f\u76f8\u540c\uff1a\u7b97\u6cd5\u5f00\u59cb\u6267\u884c\u65f6\u5148\u5bf9\u9700\u8981\u8ba1\u7b97\u7684\u53c2\u6570\u8d4b\u521d\u503c\uff0c\u7136\u540e\u4ea4\u66ff\u6267\u884c\u4e24\u4e2a\u6b65\u9aa4\uff0c\u4e00\u4e2a\u6b65\u9aa4\u662f\u5bf9\u6570\u636e\u7684\u4f30\u8ba1\uff08k-means\u662f\u4f30\u8ba1\u6bcf\u4e2a\u70b9\u6240\u5c5e\u7c07\uff1bGMM\u662f\u8ba1\u7b97\u9690\u542b\u53d8\u91cf\u7684\u671f\u671b\uff1b\uff09;\u7b2c\u4e8c\u6b65\u662f\u7528\u4e0a\u4e00\u6b65\u7b97\u51fa\u7684\u4f30\u8ba1\u503c\u91cd\u65b0\u8ba1\u7b97\u53c2\u6570\u503c\uff0c\u66f4\u65b0\u76ee\u6807\u53c2\u6570\uff08k-means\u662f\u8ba1\u7b97\u7c07\u5fc3\u4f4d\u7f6e\uff1bGMM\u662f\u8ba1\u7b97\u5404\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\u4e2d\u5fc3\u4f4d\u7f6e\u548c\u534f\u65b9\u5dee\u77e9\u9635\uff09"]}),"\n",(0,t.jsxs)(n.p,{children:["\u4e0d\u540c\u70b9",(0,t.jsx)(n.br,{}),"\n1\uff09\u9700\u8981\u8ba1\u7b97\u7684\u53c2\u6570\u4e0d\u540c\uff1ak-means\u662f\u7c07\u5fc3\u4f4d\u7f6e\uff1bGMM\u662f\u5404\u4e2a\u9ad8\u65af\u5206\u5e03\u7684\u53c2\u6570",(0,t.jsx)(n.br,{}),"\n2\uff09\u8ba1\u7b97\u76ee\u6807\u53c2\u6570\u7684\u65b9\u6cd5\u4e0d\u540c\uff1ak-means\u662f\u8ba1\u7b97\u5f53\u524d\u7c07\u4e2d\u6240\u6709\u5143\u7d20\u7684\u4f4d\u7f6e\u7684\u5747\u503c\uff1bGMM\u662f\u57fa\u4e8e\u6982\u7387\u7684\u7b97\u6cd5\uff0c\u662f\u901a\u8fc7\u8ba1\u7b97\u4f3c\u7136\u51fd\u6570\u7684\u6700\u5927\u503c\u5b9e\u73b0\u5206\u5e03\u53c2\u6570\u7684\u6c42\u89e3\u7684\u3002"]}),"\n",(0,t.jsx)(n.h2,{id:"gradient-boost-\u548c-adaboost-\u533a\u522b",children:"gradient boost \u548c adaboost \u533a\u522b"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\u7ec4\u6210\u90e8\u5206\uff1a\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["adaboost starts by building a very short tree, called a ",(0,t.jsx)(n.strong,{children:"stump"}),", from the training data."]}),"\n",(0,t.jsx)(n.li,{children:"gradient boost starts by making a small leaf, instead of a tree or stump, this leaf represents an initial guess for the weights of all of the samples. \u7136\u540egradient boost \u4f1a\u5efa\u6811\uff0c\u4f46\u662f\u4f1a\u6bd4\u4e00\u4e2astump\u5927"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["prediction target:\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"adaboost predicts the final value"}),"\n",(0,t.jsx)(n.li,{children:"gradient boost predicts the residuals"}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);