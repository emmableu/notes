"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[57325],{28453:(e,i,s)=>{s.d(i,{R:()=>o,x:()=>a});var n=s(96540);const t={},r=n.createContext(t);function o(e){const i=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),n.createElement(r.Provider,{value:i},e.children)}},50969:(e,i,s)=>{s.r(i),s.d(i,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"ML General/Logistic Regression","title":"Logistic Regression","description":"mostly from stanford notes","source":"@site/docs/05. ML General/23.Logistic Regression.md","sourceDirName":"05. ML General","slug":"/p/b026b72a-d550-4301-8b1d-c443c79d9e0c","permalink":"/notes/docs/p/b026b72a-d550-4301-8b1d-c443c79d9e0c","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/05. ML General/23.Logistic Regression.md","tags":[],"version":"current","sidebarPosition":23,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/b026b72a-d550-4301-8b1d-c443c79d9e0c","slug":"/p/b026b72a-d550-4301-8b1d-c443c79d9e0c"},"sidebar":"tutorialSidebar","previous":{"title":"Linear Regression","permalink":"/notes/docs/p/85996f78-63d2-4da1-b968-385104020c3c"},"next":{"title":"Decision Tree","permalink":"/notes/docs/p/78d7a94f-b5a8-4b20-a94f-643f772a6ee2"}}');var t=s(74848),r=s(28453);const o={created_at:"2025-11-02",page_link:"/p/b026b72a-d550-4301-8b1d-c443c79d9e0c",slug:"/p/b026b72a-d550-4301-8b1d-c443c79d9e0c"},a=void 0,l={},c=[{value:"\u4ec0\u4e48\u662f\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b generalized linear model GLM",id:"\u4ec0\u4e48\u662f\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b-generalized-linear-model-glm",level:2},{value:"Components of a probabilistic machine learning classifier",id:"components-of-a-probabilistic-machine-learning-classifier",level:2},{value:"Sigmoid function in logistic regression",id:"sigmoid-function-in-logistic-regression",level:2},{value:"The cross-entropy loss function",id:"the-cross-entropy-loss-function",level:2},{value:"Gradient Descent",id:"gradient-descent",level:2},{value:"The Gradient for Logistic Regression",id:"the-gradient-for-logistic-regression",level:3},{value:"The Stochastic Gradient Descent Algorithm",id:"the-stochastic-gradient-descent-algorithm",level:3},{value:"Working through an Example",id:"working-through-an-example",level:3},{value:"Mini-batch training",id:"mini-batch-training",level:3},{value:"Regularization",id:"regularization",level:2},{value:"Multinomial Logistic Regression",id:"multinomial-logistic-regression",level:2},{value:"Features in Multinomial Logistic Regression",id:"features-in-multinomial-logistic-regression",level:3},{value:"Learning in Multinomial Logistic Regression",id:"learning-in-multinomial-logistic-regression",level:3},{value:"in logistic regression, why we don&#39;t use MSE as the loss function?",id:"in-logistic-regression-why-we-dont-use-mse-as-the-loss-function",level:2}];function g(e){const i={a:"a",br:"br",h2:"h2",h3:"h3",img:"img",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.p,{children:(0,t.jsx)(i.a,{href:"https://web.stanford.edu/~jurafsky/slp3/5.pdf",children:"mostly from stanford notes"})}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209202349541.png",alt:""})}),"\n",(0,t.jsx)(i.h2,{id:"\u4ec0\u4e48\u662f\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b-generalized-linear-model-glm",children:"\u4ec0\u4e48\u662f\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b generalized linear model GLM"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209140008911.png",alt:""})}),"\n",(0,t.jsx)(i.h2,{id:"components-of-a-probabilistic-machine-learning-classifier",children:"Components of a probabilistic machine learning classifier"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-0.png",alt:""})}),"\n",(0,t.jsx)(i.h2,{id:"sigmoid-function-in-logistic-regression",children:"Sigmoid function in logistic regression"}),"\n",(0,t.jsxs)(i.p,{children:["this type of function is also called classification function or link function.\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-1.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-2.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-3.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-4.png",alt:""})]}),"\n",(0,t.jsx)(i.h2,{id:"the-cross-entropy-loss-function",children:"The cross-entropy loss function"}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.strong,{children:"bernouli distribution"}),":",(0,t.jsx)(i.br,{}),"\nThe Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution).\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-6.png",alt:""})]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-5.png",alt:""})}),"\n",(0,t.jsx)(i.h2,{id:"gradient-descent",children:"Gradient Descent"}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-7.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-8.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-9.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-10.png",alt:""})]}),"\n",(0,t.jsx)(i.h3,{id:"the-gradient-for-logistic-regression",children:"The Gradient for Logistic Regression"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-11.png",alt:""})}),"\n",(0,t.jsx)(i.h3,{id:"the-stochastic-gradient-descent-algorithm",children:"The Stochastic Gradient Descent Algorithm"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-12.png",alt:""})}),"\n",(0,t.jsx)(i.h3,{id:"working-through-an-example",children:"Working through an Example"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-13.png",alt:""})}),"\n",(0,t.jsx)(i.h3,{id:"mini-batch-training",children:"Mini-batch training"}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-14.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-15.png",alt:""})]}),"\n",(0,t.jsx)(i.h2,{id:"regularization",children:"Regularization"}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-16.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-17.png",alt:""})]}),"\n",(0,t.jsx)(i.h2,{id:"multinomial-logistic-regression",children:"Multinomial Logistic Regression"}),"\n",(0,t.jsxs)(i.p,{children:[(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-18.png",alt:""}),"\n",(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-19.png",alt:""})]}),"\n",(0,t.jsx)(i.h3,{id:"features-in-multinomial-logistic-regression",children:"Features in Multinomial Logistic Regression"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-20.png",alt:""})}),"\n",(0,t.jsx)(i.h3,{id:"learning-in-multinomial-logistic-regression",children:"Learning in Multinomial Logistic Regression"}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/logistic-regression-21.png",alt:""})}),"\n",(0,t.jsx)(i.h2,{id:"in-logistic-regression-why-we-dont-use-mse-as-the-loss-function",children:"in logistic regression, why we don't use MSE as the loss function?"}),"\n",(0,t.jsx)(i.p,{children:"\u7528mse\u5bb9\u6613\u51fa\u73b0\u591a\u4e2a\u5c40\u90e8\u6700\u4f18\u89e3\uff08\u5373\u975e\u51f8\u51fd\u6570\uff09"})]})}function m(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(g,{...e})}):g(e)}}}]);