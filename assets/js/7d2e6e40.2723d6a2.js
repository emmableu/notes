"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[22918],{28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>c});var o=n(96540);const i={},a=o.createContext(i);function s(e){const t=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:t},e.children)}},62426:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>r,contentTitle:()=>c,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"DL Theory 100/FlashAttention what it is and how it makes attention computation more efficient","title":"FlashAttention what it is and how it makes attention computation more efficient","description":"\u26a1 \u4e8c\u3001FlashAttention \u7684\u6838\u5fc3\u601d\u60f3\uff1a\u51cf\u5c11\u663e\u5b58\u8bfb\u5199\uff08IO-aware\uff09","source":"@site/docs/02. DL Theory 100/032. FlashAttention what it is and how it makes attention computation more efficient.md","sourceDirName":"02. DL Theory 100","slug":"/p/2cbce3ef-6fc2-4620-a66d-63e1b48be171","permalink":"/notes/docs/p/2cbce3ef-6fc2-4620-a66d-63e1b48be171","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/02. DL Theory 100/032. FlashAttention what it is and how it makes attention computation more efficient.md","tags":[],"version":"current","sidebarPosition":32,"frontMatter":{"created_at":"2025-11-01","page_link":"/p/2cbce3ef-6fc2-4620-a66d-63e1b48be171","slug":"/p/2cbce3ef-6fc2-4620-a66d-63e1b48be171"},"sidebar":"tutorialSidebar","previous":{"title":"The role of the KV Cache in Transformer decoding how caching past keyvalue speeds up inference","permalink":"/notes/docs/p/3acfdb30-2f3e-4fa9-8e60-45d3858fd276"},"next":{"title":"ZeRO optimizer Zero Redundancy Optimizer how it enables training giant models by sharding weightsstates","permalink":"/notes/docs/p/910581e8-8fa6-4484-bb7c-07acc133a759"}}');var i=n(74848),a=n(28453);const s={created_at:"2025-11-01",page_link:"/p/2cbce3ef-6fc2-4620-a66d-63e1b48be171",slug:"/p/2cbce3ef-6fc2-4620-a66d-63e1b48be171"},c=void 0,r={},d=[];function l(e){const t={p:"p",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.p,{children:"\u26a1 \u4e8c\u3001FlashAttention \u7684\u6838\u5fc3\u601d\u60f3\uff1a\u51cf\u5c11\u663e\u5b58\u8bfb\u5199\uff08IO-aware\uff09"}),"\n",(0,i.jsx)(t.p,{children:"FlashAttention \u7684\u76ee\u6807\uff1a"}),"\n",(0,i.jsx)(t.p,{children:"\u2705 \u4ecd\u7136\u8ba1\u7b97\u5b8c\u5168\u7cbe\u786e\u7684 attention\uff0c\u4f46\u4e0d\u663e\u5f0f\u5b58 QK^T\u3002"}),"\n",(0,i.jsx)(t.p,{children:"\ud83d\ude80 \u6838\u5fc3\u601d\u60f3\u4e00\u53e5\u8bdd\uff1a"}),"\n",(0,i.jsx)(t.p,{children:"\u5728 GPU \u4e0a\u5206\u5757\uff08tiling\uff09\u8ba1\u7b97 attention\uff0c\u6bcf\u6b21\u53ea\u5728\u5bc4\u5b58\u5668\u6216 shared memory \u4e2d\u8ba1\u7b97\u548c softmax\uff0c\u4e0d\u628a\u4e2d\u95f4\u7ed3\u679c\u5199\u56de\u663e\u5b58\u3002"})]})}function p(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);