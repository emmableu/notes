"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[28],{28453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var s=i(96540);const l={},r=s.createContext(l);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},91958:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"ML General/\u8bc4\u4ef7\u6a21\u578b Model Review","title":"\u8bc4\u4ef7\u6a21\u578b Model Review","description":"- data \u7684assumption \u591a\u4e0d\u591a (\u6bd4\u5982feature\u76f8\u4e92\u72ec\u7acb\uff0c\u6b8b\u5dee\u6b63\u6001\u5206\u5e03\u4e4b\u7c7b\u7684)","source":"@site/docs/05. ML General/1111.\u8bc4\u4ef7\u6a21\u578b Model Review.md","sourceDirName":"05. ML General","slug":"/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3","permalink":"/notes/docs/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/05. ML General/1111.\u8bc4\u4ef7\u6a21\u578b Model Review.md","tags":[],"version":"current","sidebarPosition":1111,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3","slug":"/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3"},"sidebar":"tutorialSidebar","previous":{"title":"Definition Glossary","permalink":"/notes/docs/p/e087fb01-09b6-4fee-8ae0-8a70e379d71b"},"next":{"title":"Model Compare \u6bd4\u8f83\u6a21\u578b\u597d\u574f","permalink":"/notes/docs/p/56af319b-39c9-41e1-959a-0e470b51ffc0"}}');var l=i(74848),r=i(28453);const t={created_at:"2025-11-02",page_link:"/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3",slug:"/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3"},a=void 0,o={},d=[{value:"Linear Regression \u8bc4\u4ef7",id:"linear-regression-\u8bc4\u4ef7",level:2},{value:"Logistic Regression \u8bc4\u4ef7",id:"logistic-regression-\u8bc4\u4ef7",level:2},{value:"Lasso / Ridge (L1 / L2 Regularization) Regression \u8bc4\u4ef7",id:"lasso--ridge-l1--l2-regularization-regression-\u8bc4\u4ef7",level:2},{value:"Naive Baiyes \u8bc4\u4ef7",id:"naive-baiyes-\u8bc4\u4ef7",level:2},{value:"Decision Tree \u8bc4\u4ef7",id:"decision-tree-\u8bc4\u4ef7",level:2},{value:"SVM \u8bc4\u4ef7",id:"svm-\u8bc4\u4ef7",level:2},{value:"KNN \u8bc4\u4ef7",id:"knn-\u8bc4\u4ef7",level:2}];function c(e){const n={h2:"h2",img:"img",li:"li",p:"p",ul:"ul",...(0,r.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"data \u7684assumption \u591a\u4e0d\u591a (\u6bd4\u5982feature\u76f8\u4e92\u72ec\u7acb\uff0c\u6b8b\u5dee\u6b63\u6001\u5206\u5e03\u4e4b\u7c7b\u7684)"}),"\n",(0,l.jsx)(n.li,{children:"data \u662f\u5426\u9700\u8981\u5148\u8fdb\u884cscale"}),"\n",(0,l.jsxs)(n.li,{children:["higher dimension \u9ad8\u7ef4\u7279\u5f81 \u8868\u73b0\u80fd\u529b \uff08\u6bd4\u5982feature\u6bd4sample\u8fd8\u591a\uff09\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"feature > sample \u4e0d\u80fd\u6c42\u89e3\u7684\u65b9\u6cd5\uff1a linear regression, logistic regression \uff08\u4f46\u662f\u52a0\u4e86 L1/L2 regularization \u5c31\u53ef\u4ee5\u6c42\u89e3\uff09"}),"\n",(0,l.jsxs)(n.li,{children:["\u662f\u5426\u503e\u5411\u4e8e\u5f97\u5230\u7a00\u758f\u89e3 \uff08\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6c42\u89e3\u8fd9\u4e2aregression\u7684\u7ed3\u679c\u53bb\u6389\u4e00\u4e9b\u6ca1\u7528\u7684feature\uff09\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u503e\u5411\u4e8e\u5f97\u5230\u7a00\u758f\u89e3\u7684\u65b9\u6cd5: L1 regularization, ReLU activation function\uff0c svm with hinge loss"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u662f\u5426sensitive to outlier:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"good model: SVM"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u66f4\u503e\u5411\u4e8eoverfit (low bias, high variance),  \u8fd8\u662funderfit\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"overfit: Decision Tree, Neural Network"}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.li,{children:"\u8bad\u7ec3\u901f\u5ea6"}),"\n",(0,l.jsx)(n.li,{children:"hyper parameter tuning\u7684\u96be\u5ea6/\u9ebb\u70e6\u7a0b\u5ea6"}),"\n",(0,l.jsx)(n.li,{children:"\u5bf9\u4e8eimbalanced dataset\u7684\u5904\u7406\u80fd\u529b"}),"\n",(0,l.jsx)(n.li,{children:"\u6a21\u578b\u672c\u8eab\u662f\u4e0d\u662f\u5f88\u5bb9\u6613\u7406\u89e3\uff08\u6bd4\u5982decision tree\uff0clogistic regression \u5c31\u5f88\u5bb9\u6613\u7406\u89e3\uff09"}),"\n",(0,l.jsx)(n.li,{children:"\u80fd\u4e0d\u80fd\u5f97\u5230interpretable\u7684feature importance"}),"\n",(0,l.jsx)(n.li,{children:"minimum \u662f\u5426 = global minimum  (\u662f\u5426\u662fconvex function)"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"linear-regression-\u8bc4\u4ef7",children:"Linear Regression \u8bc4\u4ef7"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["data assumption \u591a\uff1a\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["residuals\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"iid"}),"\n",(0,l.jsx)(n.li,{children:"normal distribution with mean=0 (\u5982\u679c\u4e0d\u8fdb\u884cbeta\u7f6e\u4fe1\u533a\u95f4\u7684\u4f30\u8ba1\uff0c\u5c31\u662fmean 0, constant variance)"}),"\n",(0,l.jsx)(n.li,{children:"independent of X"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["X / features:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"independent with each other"}),"\n",(0,l.jsxs)(n.li,{children:["linearity: assuming relationship y = wx + b\n\u5173\u4e8eresidual\u7684\u89c1\u4e0b\u56fe\n",(0,l.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202210032015214.png",alt:""})]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.li,{children:"data does not need scaling:\u5bf9\u4e8e\u7ebf\u6027\u6a21\u578b y = wx + b \u800c\u8a00\uff0cx\u7684\u4efb\u610f\u7ebf\u6027\u53d8\u6362(\u5e73\u79fb\uff0c\u653e\u7f29)\u90fd\u4f1a\u53cd\u5e94\u5728w\uff0cb\u4e0a\uff0c\u6240\u4ee5\u4e0d\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u62df\u5408\u80fd\u529b\u3002"}),"\n",(0,l.jsxs)(n.li,{children:["does not work well with high dimension (many features)\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"feature > sample \u4e0d\u80fd\u6c42\u89e3\uff0c"}),"\n",(0,l.jsx)(n.li,{children:"\u5927\u90e8\u5206beta\u90fd\u4e0d\u4f1a\u5b8c\u5168\u662f0\uff0c \u6240\u4ee5\u4e0d\u53ef\u4ee5\u901a\u8fc7\u6c42\u89e3\u8fd9\u4e2aregression\u7684\u7ed3\u679c\u53bb\u6389\u4e00\u4e9b\u6ca1\u7528\u7684feature"}),"\n",(0,l.jsx)(n.li,{children:"prone to overfit if there are many features / dimensions, causing high variance."}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.li,{children:"sensitive to outliers comparing to models like SVM (\u56e0\u4e3a during loss function, treat each sample equally\uff0cincluding the outliers)"}),"\n",(0,l.jsxs)(n.li,{children:["\u5982\u679cassumption\u5b8c\u5168\u6ee1\u8db3\u7684\u8bdd\uff0c\u6839\u636egaussian-markov theorem, it does pretty good in terms of balancing under- and over- fit.\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u4f46\u662f\uff0c\u5982\u679cassumption \u4e0d\u6ee1\u8db3\u7684\u8bdd\uff0c \u66f4\u503e\u5411\u4e8eunderfit, \u56e0\u4e3amodel \u76f8\u5bf9decision tree\uff0c neural network \u66f4\u52a0\u7b80\u5355"}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.li,{children:"\u8bad\u7ec3\u901f\u5ea6\u5feb\uff0c\u6709 analytical solution, \u4e0d\u9700\u8981 iteratively do gradient descent"}),"\n",(0,l.jsx)(n.li,{children:"hyper parameter tuning \u5bb9\u6613\uff0c\u6ca1\u6709\u4ec0\u4e48hyperparameter"}),"\n",(0,l.jsx)(n.li,{children:"\uff08\u56e0\u4e3a\u662fcontinuous target variable\uff0c\u4e0d\u9700\u8981\u8ba8\u8bbaimbalance\u95ee\u9898\uff09"}),"\n",(0,l.jsx)(n.li,{children:"model is very interpretable"}),"\n",(0,l.jsx)(n.li,{children:"feature importance is very interpretable - can just look at the p value of the beta."}),"\n",(0,l.jsx)(n.li,{children:"MSE is a convex function, so gradient descent can always reach global minima"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"logistic-regression-\u8bc4\u4ef7",children:"Logistic Regression \u8bc4\u4ef7"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["data \u7684assumption \u591a\u4e0d\u591a (\u6bd4\u5982feature\u76f8\u4e92\u72ec\u7acb\uff0c\u6b8b\u5dee\u6b63\u6001\u5206\u5e03\u4e4b\u7c7b\u7684)\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u6bd4linear regression\u5c11\u5f88\u591a\uff0c\u6240\u6709\u5173\u4e8eresidual\u7684\u90fd\u4e0d\u9700\u8981"}),"\n",(0,l.jsxs)(n.li,{children:["\u53ea\u9700\u8981\uff1a\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["X / features:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"independent with each other"}),"\n",(0,l.jsx)(n.li,{children:"linearity: assumes linearity of independent variables and log odds."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["data \u662f\u5426\u9700\u8981\u5148\u8fdb\u884cscale\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u8981\uff0c\u56e0\u4e3a\u8981\u7528gradient descent \u6c42\u89e3"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["higher dimension \u9ad8\u7ef4\u7279\u5f81 \u8868\u73b0\u80fd\u529b \uff08\u6bd4\u5982feature\u6bd4sample\u8fd8\u591a\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u4e0d\u597d"}),"\n",(0,l.jsx)(n.li,{children:"\u5927\u90e8\u5206beta\u90fd\u4e0d\u4f1a\u5b8c\u5168\u662f0\uff0c \u6240\u4ee5\u4e0d\u53ef\u4ee5\u901a\u8fc7\u6c42\u89e3\u8fd9\u4e2aregression\u7684\u7ed3\u679c\u53bb\u6389\u4e00\u4e9b\u6ca1\u7528\u7684feature"}),"\n",(0,l.jsx)(n.li,{children:"prone to overfit if there are many features / dimensions, causing high variance."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u662f\u5426sensitive to outlier:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u4e0d\u5982SVM"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u66f4\u503e\u5411\u4e8eoverfit (low bias, high variance),  \u8fd8\u662funderfit\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"more underfit comparing to neural network"}),"\n",(0,l.jsx)(n.li,{children:"more overfit comparing to naive bayes."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u8bad\u7ec3\u901f\u5ea6\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"relatively low comparing to linear regression or naive bayes"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["hyper parameter tuning\u7684\u96be\u5ea6/\u9ebb\u70e6\u7a0b\u5ea6\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u597d\u50cf\u6ca1\u6709 hyper parameter\uff0c \u53ef\u4ee5\u52a0regularization"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u6a21\u578b\u672c\u8eab\u662f\u4e0d\u662f\u5f88\u5bb9\u6613\u7406\u89e3\uff08\u6bd4\u5982decision tree\uff0clogistic regression \u5c31\u5f88\u5bb9\u6613\u7406\u89e3\uff09\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u662f, \u53ef\u4ee5\u7528odds ratio The odds ratio represents the odds that an outcome will occur given the presence of a specific predictor,"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u80fd\u4e0d\u80fd\u5f97\u5230interpretable\u7684feature importance\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u80fd"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["minimum \u662f\u5426 = global minimum  (\u662f\u5426\u662fconvex function)\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u662f\uff0closs function is convex"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\uff08\u8df3\u8fc7\uff09\u5bf9\u4e8eimbalanced dataset\u7684\u5904\u7406\u80fd\u529b\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"there may not be sufficient patterns belonging to the minority class to adequately represent its distribution."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"lasso--ridge-l1--l2-regularization-regression-\u8bc4\u4ef7",children:"Lasso / Ridge (L1 / L2 Regularization) Regression \u8bc4\u4ef7"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"data assumption \u6bd4 linear regression \u5c11: \u4e0d\u9700\u8981\u6b8b\u5dee\u6b63\u6001\u5206\u5e03\uff0c\u56e0\u4e3a\u5f97\u5230\u7684beta\u4e0d\u662funbiased\u7684\uff0c\u4e0d\u7b26\u5408t\u5206\u5e03"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\u9700\u8981\u7684assumption\uff1a\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["residual:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"iid"}),"\n",(0,l.jsx)(n.li,{children:"0 mean, constant variance"}),"\n",(0,l.jsx)(n.li,{children:"independent of X"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["X / feature:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"don't need to be completely independent with each other"}),"\n",(0,l.jsx)(n.li,{children:"linearity: assuming relationship y = wx + b"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"data need scaling"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/feature-scaling-2.png",alt:""})}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"works better with high dimension:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"L1 allows sparcity -> some beta will be fitted to be 0, so, Lasso Regularization can exclude useless variables from the model and, in general, tends to perform well when we need to remove a lot of useless variables from a model."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"sensitive to outliers comparing to models like SVM"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"comparing to LR tends to underfit if too big lambda"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"hyper parameter tuning \u5bb9\u6613, \u53ea\u6709lambda"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"model is very interpretable"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"minimum \u662f\u5426 = global minimum  (\u662f\u5426\u662fconvex function):"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"L2 Ridge regression: loss function is convex,  gradient descent can always reach global minima"}),"\n",(0,l.jsx)(n.li,{children:"L1 Lasso: sometimes not convex, so gradient descent don't always reach global minima"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"naive-baiyes-\u8bc4\u4ef7",children:"Naive Baiyes \u8bc4\u4ef7"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["data \u7684assumption \u591a\u4e0d\u591a\uff1a\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u6709\u4e00\u4e2a\uff1a features need to be independent with each other"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["data \u662f\u5426\u9700\u8981\u5148\u8fdb\u884cscale\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"no"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["higher dimension \u9ad8\u7ef4\u7279\u5f81 \u8868\u73b0\u80fd\u529b \uff08\u6bd4\u5982feature\u6bd4sample\u8fd8\u591a\uff09\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"If your data has \ud835\udc58 dimensions, then a fully general ML algorithm which attempts to learn all possible correlations between these features has to deal with 2\ud835\udc58 possible feature interactions, and therefore needs on the order of 2\ud835\udc58 many data points to be performant."}),"\n",(0,l.jsx)(n.li,{children:"However because Naive Bayes assumes independence between features, it only needs on the order of \ud835\udc58 many data points, exponentially fewer."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u662f\u5426sensitive to outlier:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"sensitive to outliers comparing to SVM"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u66f4\u503e\u5411\u4e8eoverfit (low bias, high variance),  \u8fd8\u662funderfit\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"underfit, since the interactions are not modeled, some of the information in the data is ignored. This makes it an inherently high bias model; it has a high approximation error but as a result it also does not overfit."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u8bad\u7ec3\u901f\u5ea6\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"fast"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["hyper parameter tuning\u7684\u96be\u5ea6/\u9ebb\u70e6\u7a0b\u5ea6\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"easy, only one hyperparameter - pseudocount"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u5bf9\u4e8eimbalanced dataset\u7684\u5904\u7406\u80fd\u529b\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"not good"}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202210050154011.png",alt:""})}),"\n",(0,l.jsx)(n.li,{children:"Even though the likelihood probabilities are similar to some extent, but the posterior probability is badly affected by prior probabilities. Here in the above example the class +ve prior probability will be 9 times higher than the class -ve, this difference in naive bayes creates a bias for class +ve."}),"\n",(0,l.jsx)(n.li,{children:"One simple solution is to ignore the prior probabilities. (Or) You can do undersampling or oversampling."}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.li,{children:"(\u8df3\u8fc7)\u6a21\u578b\u672c\u8eab\u662f\u4e0d\u662f\u5f88\u5bb9\u6613\u7406\u89e3\uff08\u6bd4\u5982decision tree\uff0clogistic regression \u5c31\u5f88\u5bb9\u6613\u7406\u89e3\uff09\u8df3\u8fc7"}),"\n",(0,l.jsx)(n.li,{children:"(\u8df3\u8fc7)\u80fd\u4e0d\u80fd\u5f97\u5230interpretable\u7684feature importance \u8df3\u8fc7"}),"\n",(0,l.jsx)(n.li,{children:"(\u8df3\u8fc7)minimum \u662f\u5426 = global minimum  (\u662f\u5426\u662fconvex function) \u8df3\u8fc7"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"decision-tree-\u8bc4\u4ef7",children:"Decision Tree \u8bc4\u4ef7"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["data \u7684assumption \u591a\u4e0d\u591a (\u6bd4\u5982feature\u76f8\u4e92\u72ec\u7acb\uff0c\u6b8b\u5dee\u6b63\u6001\u5206\u5e03\u4e4b\u7c7b\u7684)\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u597d\u50cf\u6ca1\u6709"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["data \u662f\u5426\u9700\u8981\u5148\u8fdb\u884cscale\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"no need"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["higher dimension \u9ad8\u7ef4\u7279\u5f81 \u8868\u73b0\u80fd\u529b\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"tends to become very overfit,  the number of branches grows exponentially with the number of features"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u662f\u5426sensitive to outlier:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"not sensitive to outliers since the partitioning happens based on the proportion of samples within the split ranges and not on absolute values."}),"\n",(0,l.jsx)(n.li,{children:"especially when we use early stopping"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u66f4\u503e\u5411\u4e8eoverfit (low bias, high variance),  \u8fd8\u662funderfit\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"overfit,very specific rules"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u8bad\u7ec3\u901f\u5ea6\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"not as fast comparing to logistic regression / naive bayes, but still fast enough comparing to neural network"}),"\n",(0,l.jsx)(n.li,{children:"test time is fast as it's just linearly traversing the test rules."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["hyper parameter tuning\u7684\u96be\u5ea6/\u9ebb\u70e6\u7a0b\u5ea6\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u597d\u50cf\u6ca1\u6709hyper parameter\uff0c\u9664\u975e\u5b9a\u4e49early stopping"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u5bf9\u4e8eimbalanced dataset\u7684\u5904\u7406\u80fd\u529b\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u4e0d\u592a\u597d"}),"\n",(0,l.jsx)(n.li,{children:"Decision trees implementations normally use Gini impurity for finding splits. For each rule/condition, when calculating the gini impurity it is a weighted average on. it weights higher on the more prevaling condition (e.g., loves troll when predicting loves popcorn). But more prevaling sample has a higher say on the more prevaling condition."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u6a21\u578b\u672c\u8eab\u662f\u4e0d\u662f\u5f88\u5bb9\u6613\u7406\u89e3\uff08\u6bd4\u5982decision tree\uff0clogistic regression \u5c31\u5f88\u5bb9\u6613\u7406\u89e3\uff09\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"yes. has interpretable rules"}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.li,{children:"(\u8df3\u8fc7) \u80fd\u4e0d\u80fd\u5f97\u5230interpretable\u7684feature importance"}),"\n",(0,l.jsx)(n.li,{children:"(\u8df3\u8fc7) minimum \u662f\u5426 = global minimum  (\u662f\u5426\u662fconvex function) \u8df3\u8fc7"}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"svm-\u8bc4\u4ef7",children:"SVM \u8bc4\u4ef7"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["data \u7684assumption \u591a\u4e0d\u591a (\u6bd4\u5982feature\u76f8\u4e92\u72ec\u7acb\uff0c\u6b8b\u5dee\u6b63\u6001\u5206\u5e03\u4e4b\u7c7b\u7684)\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u6bd4\u8f83\u5bbd\u677e\uff0c\u53ea\u9700\u8981Data is linearly separable. Even if the linear boundary is in an augmented feature space. for example, after the kernel trick"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["data \u662f\u5426\u9700\u8981\u5148\u8fdb\u884cscale\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u8981\u3000"}),"\n",(0,l.jsx)(n.li,{children:"SVM tries to maximize the distance between the separating plane and the support vectors."}),"\n",(0,l.jsx)(n.li,{children:"If one feature (i.e. one dimension in this space) has very large values, it will dominate the other features when calculating the distance."}),"\n",(0,l.jsx)(n.li,{children:"If you rescale all features (e.g. to [0, 1]), they all have the same influence on the distance metric."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["higher dimension \u9ad8\u7ef4\u7279\u5f81 \u8868\u73b0\u80fd\u529b \uff08\u6bd4\u5982feature\u6bd4sample\u8fd8\u591a\uff09\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u8fd8\u884c\uff0c\u56e0\u4e3a\u5b83\u4f1a\u901a\u8fc7fit\u8fd9\u4e2amodel\u6765\u653e\u7f29 c => regularization parameter, the degree to which our model will accept misclassifications in our dataset\uff0c so that it generalise well over training data."}),"\n",(0,l.jsxs)(n.li,{children:["\u662f\u5426\u503e\u5411\u4e8e\u5f97\u5230\u7a00\u758f\u89e3 \uff08\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6c42\u89e3\u8fd9\u4e2amodel\u7684\u7ed3\u679c\u53bb\u6389\u4e00\u4e9b\u6ca1\u7528\u7684feature\uff09\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u7528hinge loss \u4f5c\u4e3aloss function\uff0c \u56e0\u4e3ahinge loss\u662f = max(0, 1-z), \u6240\u4ee5>1\u65f6\u662f0\uff0c\u6240\u4ee5\u4f1a\u6709 0 \u89e3"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u662f\u5426sensitive to outlier:\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\u4e0dsensitive to outlier\uff0c\u5f88robust\uff0c\u8fd8\u662f\u56e0\u4e3a\u5b83\u4f1a\u901a\u8fc7fit\u8fd9\u4e2amodel\u6765\u653e\u7f29 c\uff0c\u89c1\u4e0a\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u800c\u4e14\u53ea\u5173\u5fc3soft margin \u5468\u56f4\u7684\u70b9"}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.li,{children:"good model: SVM"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u66f4\u503e\u5411\u4e8eoverfit (low bias, high variance),  \u8fd8\u662funderfit\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u6bd4\u8d77lr \u548c nn \u548c decision tree \u8fd9\u4e9b\u66f4\u5bb9\u6613 underfit, \u56e0\u4e3a\u6709soft margin \u548c regularization parameter\u3000"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u8bad\u7ec3\u901f\u5ea6\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u4e0d\u662f\u5f88\u5feb\uff0c\u56e0\u4e3a\u8fd8\u662f\u8981\u7528gradient descent"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["hyper parameter tuning\u7684\u96be\u5ea6/\u9ebb\u70e6\u7a0b\u5ea6\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u9ebb\u70e6\uff0c\u6bd4\u5982\u786e\u5b9a\u54ea\u4e2akernel"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\u6a21\u578b\u672c\u8eab\u662f\u4e0d\u662f\u5f88\u5bb9\u6613\u7406\u89e3\uff08\u6bd4\u5982decision tree\uff0clogistic regression \u5c31\u5f88\u5bb9\u6613\u7406\u89e3\uff09\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u6bd4\u8f83\u96be\uff0c \u6709feature weights\uff0c but they don't correspond 1-1 to feature importance such as in logistic regression"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["minimum \u662f\u5426 = global minimum  (\u662f\u5426\u662fconvex function)\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u662fconvex"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"knn-\u8bc4\u4ef7",children:"KNN \u8bc4\u4ef7"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"data \u7684assumption \u591a\u4e0d\u591a"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"assuming similar points situated closely with each other."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"data \u662f\u5426\u9700\u8981\u5148\u8fdb\u884cscale"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u8981\uff0c\u56e0\u4e3a\u8981\u8ba1\u7b97\u8ddd\u79bb"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"higher dimension \u9ad8\u7ef4\u7279\u5f81 \u8868\u73b0\u80fd\u529b \uff08\u6bd4\u5982feature\u6bd4sample\u8fd8\u591a\uff09"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\u4e0d\u592a\u884c\uff0c\u56e0\u4e3a\u9ad8\u7ef4\u7684\u65f6\u5019\uff1a\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Our assumption of similar points being situated closely breaks\uff1a high dimension creates exponentially higher space, points tend to never be close together."}),"\n",(0,l.jsx)(n.li,{children:"\xa0 It becomes computationally more expensive to compute distance and find the nearest neighbors in high-dimensional space"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"\u662f\u5426sensitive to outlier:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"If \u2018K\u2019 value is low, the model is susceptible to outliers. => Let take K=1, suppose there is 1 outlier near to our test point and then the model predicts the label corresponding to the outlier."}),"\n",(0,l.jsx)(n.li,{children:"If \u2018K\u2019 value is high, the model is robust to outliers.\n\xa0\xa0"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"\u66f4\u503e\u5411\u4e8eoverfit (low bias, high variance),  \u8fd8\u662funderfit"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"k \u5c0f\uff0c \u9ad8\u7ef4\u6570\u636e\uff1a overfit"}),"\n",(0,l.jsx)(n.li,{children:"k \u5927\uff0c\u4f4e\u7ef4\u6570\u636e\uff1a underfit"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"\u8bad\u7ec3\u901f\u5ea6"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"\u6162\xa0\xa0for each testing sample, it requires calculating distance with each training sample"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"hyper parameter tuning\u7684\u96be\u5ea6/\u9ebb\u70e6\u7a0b\u5ea6"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"k \u5f88\u96be\u51b3\u5b9a\uff0c\u8981\u7528hyperparameter tuning"}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"\u5bf9\u4e8eimbalanced dataset\u7684\u5904\u7406\u80fd\u529b"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["theoretically, unbalanced classes are not a problem at all for the k-nearest neighbor algorithm.\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Because the algorithm is not influenced in any way by the size of the class, it will not favor any on the basis of size."}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.li,{children:"but:  there may not be sufficient patterns belonging to the minority class to adequately represent its distribution."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"\u6a21\u578b\u672c\u8eab\u662f\u4e0d\u662f\u5f88\u5bb9\u6613\u7406\u89e3\uff08\u6bd4\u5982decision tree\uff0clogistic regression \u5c31\u5f88\u5bb9\u6613\u7406\u89e3\uff09"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"not interpretable"}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}}}]);