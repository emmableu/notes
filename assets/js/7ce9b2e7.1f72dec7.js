"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[47183],{28453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>c});var a=n(96540);const o={},s=a.createContext(o);function i(e){const t=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(s.Provider,{value:t},e.children)}},36684:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>r,contentTitle:()=>c,default:()=>l,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"DL Theory 100/window attention, page attention, kv cache","title":"window attention, page attention, kv cache","description":"Q: Why don\u2019t we use KV-Cache during training?","source":"@site/docs/02. DL Theory 100/101. window attention, page attention, kv cache.md","sourceDirName":"02. DL Theory 100","slug":"/p/d54979a7-f0e9-4530-a0df-e48aa3bd1ddc","permalink":"/notes/docs/p/d54979a7-f0e9-4530-a0df-e48aa3bd1ddc","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/02. DL Theory 100/101. window attention, page attention, kv cache.md","tags":[],"version":"current","sidebarPosition":101,"frontMatter":{"created_at":"2025-12-19","page_link":"/p/d54979a7-f0e9-4530-a0df-e48aa3bd1ddc","slug":"/p/d54979a7-f0e9-4530-a0df-e48aa3bd1ddc"},"sidebar":"tutorialSidebar","previous":{"title":"kv cache","permalink":"/notes/docs/p/06965b63-dc1c-4576-8e08-c34967bc8c44"},"next":{"title":"Self-Attention math QKV and why use multi-head attention","permalink":"/notes/docs/p/4cb47741-f933-4bc7-9b25-56ec90c84807"}}');var o=n(74848),s=n(28453);const i={created_at:"2025-12-19",page_link:"/p/d54979a7-f0e9-4530-a0df-e48aa3bd1ddc",slug:"/p/d54979a7-f0e9-4530-a0df-e48aa3bd1ddc"},c=void 0,r={},d=[];function u(e){const t={img:"img",p:"p",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202511041600917.png",alt:""}),"Q: Why don\u2019t we use KV-Cache during training?"]}),"\n",(0,o.jsx)(t.p,{children:"A:\nBecause during training, we process the entire sequence in parallel \u2014 we already have all tokens\u2019 Q, K, V available in the same forward pass, so there\u2019s no need to cache them across steps.\nKV-Cache is only useful in autoregressive inference, where tokens are generated one by one and past keys/values must be reused."})]})}function l(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}}}]);