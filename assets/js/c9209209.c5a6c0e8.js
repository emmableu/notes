"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[29637],{17043:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"ML General/Gradient Boost \u7ec6\u8282","title":"Gradient Boost \u7ec6\u8282","description":"Definition","source":"@site/docs/05. ML General/39.Gradient Boost \u7ec6\u8282.md","sourceDirName":"05. ML General","slug":"/p/1f533186-531c-44b3-96e0-eaa311ac7ff8","permalink":"/notes/docs/p/1f533186-531c-44b3-96e0-eaa311ac7ff8","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/05. ML General/39.Gradient Boost \u7ec6\u8282.md","tags":[],"version":"current","sidebarPosition":39,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/1f533186-531c-44b3-96e0-eaa311ac7ff8","slug":"/p/1f533186-531c-44b3-96e0-eaa311ac7ff8"},"sidebar":"tutorialSidebar","previous":{"title":"GMM - Gaussian Mixed Models","permalink":"/notes/docs/p/75d0bf4d-fcc2-4e72-b273-4e862d8898f8"},"next":{"title":"T-SNE or tSNE or tsne","permalink":"/notes/docs/p/a5f44b92-03c1-475e-a773-76c53542b1ad"}}');var a=s(74848),n=s(28453);const r={created_at:"2025-11-02",page_link:"/p/1f533186-531c-44b3-96e0-eaa311ac7ff8",slug:"/p/1f533186-531c-44b3-96e0-eaa311ac7ff8"},l=void 0,o={},h=[{value:"Definition",id:"definition",level:2},{value:"gradient boost \u8fc7\u7a0b",id:"gradient-boost-\u8fc7\u7a0b",level:2},{value:"Step 1: Calculate the average of the target label",id:"step-1-calculate-the-average-of-the-target-label",level:3},{value:"Step 2: Calculate the residuals",id:"step-2-calculate-the-residuals",level:3},{value:"Step 3: Construct a decision tree",id:"step-3-construct-a-decision-tree",level:3},{value:"Step 4: Predict the target label using all of the trees within the ensemble",id:"step-4-predict-the-target-label-using-all-of-the-trees-within-the-ensemble",level:3},{value:"Step 5: Compute the new residuals",id:"step-5-compute-the-new-residuals",level:3},{value:"Step 6: Repeat steps 3 - 5 until the number of iterations matches the number specified by the hyperparameter (i.e., # iterations)",id:"step-6-repeat-steps-3---5-until-the-number-of-iterations-matches-the-number-specified-by-the-hyperparameter-ie--iterations",level:3},{value:"Step 7: Once trained, use all of the trees in the ensemble to make a final prediction as to the value of the target variable",id:"step-7-once-trained-use-all-of-the-trees-in-the-ensemble-to-make-a-final-prediction-as-to-the-value-of-the-target-variable",level:3},{value:"XGBoost step by step",id:"xgboost-step-by-step",level:2},{value:"Step 1: Make an Initial Prediction and Calculate Residuals",id:"step-1-make-an-initial-prediction-and-calculate-residuals",level:3},{value:"Step 2: Build an XGBoost Tree",id:"step-2-build-an-xgboost-tree",level:3},{value:"<strong><em>Step 3: Prune the Tree</em></strong>",id:"step-3-prune-the-tree",level:3},{value:"<strong><em>Step 4: Calculate the Output Values of Leaves</em></strong>",id:"step-4-calculate-the-output-values-of-leaves",level:3},{value:"<strong><em>Step 5: Make New Predictions</em></strong>",id:"step-5-make-new-predictions",level:3},{value:"<strong><em>Step 6: Calculate Residuals Using the New Predictions</em></strong>",id:"step-6-calculate-residuals-using-the-new-predictions",level:3},{value:"<strong><em>Step 7: Repeat Steps 2\u20136</em></strong>",id:"step-7-repeat-steps-26",level:3}];function c(e){const t={a:"a",blockquote:"blockquote",em:"em",h2:"h2",h3:"h3",img:"img",p:"p",strong:"strong",...(0,n.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h2,{id:"definition",children:"Definition"}),"\n",(0,a.jsx)(t.p,{children:"gradient boosting:\n- A training algorithm where weak models are trained to iteratively improve the quality (reduce the loss) of a strong model. For example, a weak model could be a linear or small decision tree model. The strong model becomes the sum of all the previously trained weak models.\n- at each iteration, a weak model is trained to predict the loss gradient of the strong model. Then, the strong model's output is updated by subtracting the predicted gradient, similar to gradient descent"}),"\n",(0,a.jsx)(t.h2,{id:"gradient-boost-\u8fc7\u7a0b",children:"gradient boost \u8fc7\u7a0b"}),"\n",(0,a.jsx)(t.p,{children:"Suppose, we were trying to predict the price of a house given their age, square footage and location."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302210359.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-1-calculate-the-average-of-the-target-label",children:"Step 1: Calculate the average of the target label"}),"\n",(0,a.jsx)(t.p,{children:"When tackling regression problems, we start with a leaf that is the average value of the variable we want to predict. This leaf will be used as a baseline to approach the correct solution in the proceeding steps."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302210774.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-2-calculate-the-residuals",children:"Step 2: Calculate the residuals"}),"\n",(0,a.jsx)(t.p,{children:"For every sample, we calculate the residual with the proceeding formula."}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsx)(t.p,{children:"residual = actual value \u2013 predicted value"}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"In our example, the predicted value is the equal to the mean calculated in the previous step and the actual value can be found in the price column of each sample. After computing the residuals, we get the following table."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302211021.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-3-construct-a-decision-tree",children:"Step 3: Construct a decision tree"}),"\n",(0,a.jsx)(t.p,{children:"Next, we build a tree with the goal of predicting the residuals. In other words, every leaf will contain a prediction as to the value of the residual (not the desired label)."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302213623.png",alt:""})}),"\n",(0,a.jsx)(t.p,{children:"In the event there are more residuals than leaves, some residuals will end up inside the same leaf. When this happens, we compute their average and place that inside the leaf."}),"\n",(0,a.jsx)(t.p,{children:"Thus, the tree becomes:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302213246.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-4-predict-the-target-label-using-all-of-the-trees-within-the-ensemble",children:"Step 4: Predict the target label using all of the trees within the ensemble"}),"\n",(0,a.jsx)(t.p,{children:"Each sample passes through the decision nodes of the newly formed tree until it reaches a given lead. The residual in said leaf is used to predict the house price."}),"\n",(0,a.jsx)(t.p,{children:"\u4f8b\u5982\uff0c\u5bf9\u5de6\u4e0b\u89d2\u7684\u8fd9\u4e2anode\uff0c\u5c31\u662f\u8fd9\u6837\u7b97\uff1a"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302220441.png",alt:""})}),"\n",(0,a.jsx)(t.p,{children:"\u6216\u8005\u4e2d\u95f4\u7684\u8fd9\u4e2anode\uff1a"}),"\n",(0,a.jsx)(t.p,{children:"predicted_price = 688 + 0.1 * (-208) = 667.2"}),"\n",(0,a.jsx)(t.h3,{id:"step-5-compute-the-new-residuals",children:"Step 5: Compute the new residuals"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302301353.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-6-repeat-steps-3---5-until-the-number-of-iterations-matches-the-number-specified-by-the-hyperparameter-ie--iterations",children:"Step 6: Repeat steps 3 - 5 until the number of iterations matches the number specified by the hyperparameter (i.e., # iterations)"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302223105.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-7-once-trained-use-all-of-the-trees-in-the-ensemble-to-make-a-final-prediction-as-to-the-value-of-the-target-variable",children:"Step 7: Once trained, use all of the trees in the ensemble to make a final prediction as to the value of the target variable"}),"\n",(0,a.jsx)(t.p,{children:"The final prediction will be equal to the mean we computed in the first step, plus all of the residuals predicted by the trees that make up the forest multiplied by the learning rate."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209302223078.png",alt:""})}),"\n",(0,a.jsx)(t.h2,{id:"xgboost-step-by-step",children:"XGBoost step by step"}),"\n",(0,a.jsxs)(t.p,{children:["Let\u2019s start with our training dataset which consists of five people. We recorded their ages, whether or not they have a master\u2019s degree, and their salary (in thousands). Our goal is to predict ",(0,a.jsx)(t.em,{children:"Salary"})," using the XGBoost Algorithm."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*_KUzdQTdTUfjPposrAxSfQ.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-1-make-an-initial-prediction-and-calculate-residuals",children:"Step 1: Make an Initial Prediction and Calculate Residuals"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.a,{href:"https://towardsdatascience.com/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb",children:"source"})}),"\n",(0,a.jsx)(t.p,{children:"This prediction can be anything. But let\u2019s assume our initial prediction is the average value of the variables we want to predict."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*GjxcMQDQQqU-UfgfvBeb1A.png",alt:""})}),"\n",(0,a.jsx)(t.p,{children:"We can calculate residuals using the following formula:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*AEwarDLHuDQZmMlB-2VDpw.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Here, our Observed Values are the values in the ",(0,a.jsx)(t.em,{children:"Salary"})," column and all Predicted Values are equal to 70 because that is what we chose our initial prediction to be."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*D5eCwr_TYcwuDP8NBOAOsg.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-2-build-an-xgboost-tree",children:"Step 2: Build an XGBoost Tree"}),"\n",(0,a.jsx)(t.p,{children:"Each tree starts with a single leaf and all the residuals go into that leaf."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1036/1*yYq5mmVkYk1Lwl_aspUq6A.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Now we need to calculate something called a ",(0,a.jsx)(t.strong,{children:"Similarity Score"})," of this leaf."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*ddxctrOTVbqPhppkcxCzFw.png",alt:""})}),"\n",(0,a.jsx)(t.p,{children:"\u03bb (lambda) is a regularization parameter that reduces the prediction\u2019s sensitivity to individual observations and prevents the overfitting of data (this is when a model fits exactly against the training dataset). The default value of \u03bb is 1 so we will let \u03bb = 1 in this example."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*3bc39LtxpRTqQ0kivyrQlw.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Now we should see if we can do a better job clustering the residuals if we split them into two groups using thresholds based on our predictors \u2014 ",(0,a.jsx)(t.em,{children:"Age"})," and ",(0,a.jsx)(t.em,{children:"Master\u2019s Degree?."})," Splitting the ",(0,a.jsx)(t.em,{children:"Residuals"})," basically means that we are adding branches to our tree."]}),"\n",(0,a.jsxs)(t.p,{children:["First, let\u2019s try splitting the leaf using ",(0,a.jsx)(t.em,{children:"Master\u2019s Degree"}),(0,a.jsx)(t.strong,{children:"?"})]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*iofkdqDDgFDhOHRxjmjz5w.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["And then calculate the ",(0,a.jsx)(t.strong,{children:"Similarity Scores"})," for the left and right leaves of the above split:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*rNlX9qaQqc8xGSUXvkwkKg.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Now we need to quantify how much better the leaves cluster similar ",(0,a.jsx)(t.em,{children:"Residuals"})," than the root does. We can do this by calculating the ",(0,a.jsx)(t.strong,{children:"Gain"})," of splitting the ",(0,a.jsx)(t.em,{children:"Residuals"})," into two groups. If the ",(0,a.jsx)(t.strong,{children:"Gain"})," is positive, then it\u2019s a good idea to split, otherwise, it is not."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1222/0*rIeNeoesHLox_5p8.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Then we compare this ",(0,a.jsx)(t.strong,{children:"Gain"})," to those of the splits in ",(0,a.jsx)(t.em,{children:"Age"}),". Since ",(0,a.jsx)(t.em,{children:"Age"})," is a continuous variable, the process to find the different splits is a little more involved. First, we arrange the rows of our dataset according to the ascending order of ",(0,a.jsx)(t.em,{children:"Age"}),". Then we calculate the average values of the adjacent values in ",(0,a.jsx)(t.em,{children:"Age"}),"."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/550/0*JruyoPHXdM9LxPPW",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Now we split the ",(0,a.jsx)(t.em,{children:"Residuals"})," using the four averages as thresholds and calculate ",(0,a.jsx)(t.strong,{children:"Gain"})," for each of the splits. The first split uses ",(0,a.jsx)(t.em,{children:"Age < 23.5"}),":"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1222/1*4285F3yPyqgvkDf0a3JtJA.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["For this split, we find the ",(0,a.jsx)(t.strong,{children:"Similarity Score"})," and ",(0,a.jsx)(t.strong,{children:"Gain"})," the same way we did for ",(0,a.jsx)(t.em,{children:"Master\u2019s Degree?"})]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*QG-8M5rLc_WFo293kdn32Q.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Do the same thing for the rest of the ",(0,a.jsx)(t.em,{children:"Age"})," splits:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*HrG-WfVT9O5ve7Yso3dL9g.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Out of the one ",(0,a.jsx)(t.em,{children:"Mater\u2019s Degree?"})," split and four ",(0,a.jsx)(t.em,{children:"Age"})," splits, the ",(0,a.jsx)(t.em,{children:"Master\u2019s Degree"})," split has the greatest ",(0,a.jsx)(t.strong,{children:"Gain"})," value, so we\u2019ll use that as our initial split. Now we can add more branches to the tree by splitting our ",(0,a.jsx)(t.em,{children:"Master\u2019s Degree?"})," leaves again using the same process described above. But, only this time, we use the initial ",(0,a.jsx)(t.em,{children:"Master\u2019s Degree?"})," leaves as our root nodes and try splitting them by getting the greatest ",(0,a.jsx)(t.strong,{children:"Gain"})," value that is greater than 0."]}),"\n",(0,a.jsxs)(t.p,{children:["Let\u2019s start with the left node. For this node, we only consider the observations that have the value \u2018Yes\u2019 in ",(0,a.jsx)(t.em,{children:"Master\u2019s Degree?"})," because only those observations land in the left node."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*ZaDD16j0hBb3kIp4iOgPcA.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["So we calculate the ",(0,a.jsx)(t.strong,{children:"Gain"})," of the ",(0,a.jsx)(t.em,{children:"Age"})," splits using the same process as before, but this time using the ",(0,a.jsx)(t.em,{children:"Residuals"})," in the highlighted rows only."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*ArdKYPYzu9oRcJCsoOgOKQ.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Since only ",(0,a.jsx)(t.em,{children:"Age < 25"})," gives us a positive ",(0,a.jsx)(t.strong,{children:"Gain"}),", we split the left node using this threshold. Moving onto our right node, we only look at values with \u2018No\u2019 values in ",(0,a.jsx)(t.em,{children:"Master\u2019s Degree?"})]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*wT0cScVpEqPSa4OAJSuJLg.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["We only have two observations in our right node, so the only split possible is ",(0,a.jsx)(t.em,{children:"Age < 24.5"})," because that is the average of the two ",(0,a.jsx)(t.em,{children:"Age"})," values in the highlighted rows."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*I4xa7BIP1YcZOyl_K1eBEg.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["The ",(0,a.jsx)(t.strong,{children:"Gain"})," of this split is positive, so our final tree is:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1298/1*LOq80DzUJSo3ndw1P9PoAQ.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-3-prune-the-tree",children:(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.em,{children:"Step 3: Prune the Tree"})})}),"\n",(0,a.jsxs)(t.p,{children:["Pruning is another way we can avoid overfitting the data. To do this we start from the bottom of our tree and work our way up to see if a split is valid or not. To establish validity, we use \u03b3 (gamma). If ",(0,a.jsx)(t.strong,{children:"Gain \u2014"})," \u03b3 is positive then we keep the split, otherwise, we remove it. The default value of \u03b3 is 0, but for illustrative purposes, let\u2019s set our \u03b3 to 50. From previous calculations we know the ",(0,a.jsx)(t.strong,{children:"Gain"})," values:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*I1ukSNGxqbB1tAHZBqw7qA.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["Since ",(0,a.jsx)(t.strong,{children:"Gain \u2014"})," \u03b3 is positive for all splits except that of ",(0,a.jsx)(t.em,{children:"Age < 24.5"}),", we can remove that branch. So the resulting tree is:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1168/1*n0b6WmYcJXw4UCpy0DCGRA.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-4-calculate-the-output-values-of-leaves",children:(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.em,{children:"Step 4: Calculate the Output Values of Leaves"})})}),"\n",(0,a.jsx)(t.p,{children:"We are almost there! All we have to do now is calculate a single value in our leaf nodes because we can not have a leaf node giving us multiple outputs."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*XmYMrpfsKZF6XWOb",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["This is similar to the formula to calculate ",(0,a.jsx)(t.strong,{children:"Similarity Score"})," except we are not squaring the ",(0,a.jsx)(t.em,{children:"Residuals"}),". Using the formula and \u03bb = 1, ",(0,a.jsx)(t.em,{children:(0,a.jsx)(t.em,{children:"drum roll"})})," our final tree is:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*FSIFRB1N8bR52vixSWR19A.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-5-make-new-predictions",children:(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.em,{children:"Step 5: Make New Predictions"})})}),"\n",(0,a.jsx)(t.p,{children:"Now that all that hard model building is behind us, we come to the exciting part and see how much our predictions improve using our new model. We can make predictions using this formula:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*tIEzTeLj_yHOIQWVVqPQ9w.png",alt:""})}),"\n",(0,a.jsx)(t.p,{children:"The XGBoost Learning Rate is \u025b (eta) and the default value is 0.3. So the predicted value of our first observation will be:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*EYwsm84uQB4-WNQfezfwCA.png",alt:""})}),"\n",(0,a.jsx)(t.p,{children:"Similarly, we can calculate the rest of the predicted values:"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*7tDf4x_6I-WehIG2yI7BMw.png",alt:""})}),"\n",(0,a.jsx)(t.h3,{id:"step-6-calculate-residuals-using-the-new-predictions",children:(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.em,{children:"Step 6: Calculate Residuals Using the New Predictions"})})}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*tIqXoaKD_41QeCBfe2xxqQ.png",alt:""})}),"\n",(0,a.jsxs)(t.p,{children:["We see that the new ",(0,a.jsx)(t.em,{children:"Residuals"})," are smaller than the ones before, this indicates that we\u2019ve taken a small step in the right direction. As we repeat this process, our ",(0,a.jsx)(t.em,{children:"Residuals"})," will get smaller and smaller indicating that our predicted values are getting closer to the observed values."]}),"\n",(0,a.jsx)(t.h3,{id:"step-7-repeat-steps-26",children:(0,a.jsx)(t.strong,{children:(0,a.jsx)(t.em,{children:"Step 7: Repeat Steps 2\u20136"})})}),"\n",(0,a.jsxs)(t.p,{children:["Now we just repeat the same process over and over again, building a new tree, making predictions, and calculating ",(0,a.jsx)(t.em,{children:"Residuals"})," at each iteration. We do this until the ",(0,a.jsx)(t.em,{children:"Residuals"})," are super small or we reached the maximum number of iterations we set for our algorithm. If the tree we built at each iteration is indicated by T\u1d62, where ",(0,a.jsx)(t.em,{children:"i"})," is the current iteration, then the formula to calculate predictions is:"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{src:"https://miro.medium.com/max/1400/1*MiHVOymdS80S8o_uv96Ubw.png",alt:""})}),"\n",(0,a.jsx)(t.p,{children:"And that\u2019s it. Thanks for reading and good luck with the rest of your algorithmic journey!"})]})}function d(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},28453:(e,t,s)=>{s.d(t,{R:()=>r,x:()=>l});var i=s(96540);const a={},n=i.createContext(a);function r(e){const t=i.useContext(n);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(n.Provider,{value:t},e.children)}}}]);