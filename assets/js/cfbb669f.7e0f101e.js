"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[65913],{28453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>o});var i=t(96540);const s={},r=i.createContext(s);function l(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(r.Provider,{value:n},e.children)}},50592:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>l,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"DL Theory 100/Self-Attention math QKV and why use multi-head attention","title":"Self-Attention math QKV and why use multi-head attention","description":"\u5b9a\uff1aSelf-Attention \u7528 Q/K/V \u5c06\u5e8f\u5217\u5185 token \u4e24\u4e24\u76f8\u4f3c\u5ea6\u8f6c\u6210\u52a0\u6743\u548c\uff1aAttn(Q,K,V)=softmax(QK\u1d40/\u221ad_k)V\uff1b\u591a\u5934\u5728\u4e0d\u540c\u5b50\u7a7a\u95f4\u5e76\u884c\u6355\u6349\u5173\u7cfb\u3002","source":"@site/docs/02. DL Theory 100/102. Self-Attention math QKV and why use multi-head attention.md","sourceDirName":"02. DL Theory 100","slug":"/p/4cb47741-f933-4bc7-9b25-56ec90c84807","permalink":"/notes/docs/p/4cb47741-f933-4bc7-9b25-56ec90c84807","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/02. DL Theory 100/102. Self-Attention math QKV and why use multi-head attention.md","tags":[],"version":"current","sidebarPosition":102,"frontMatter":{"created_at":"2025-11-01","page_link":"/p/4cb47741-f933-4bc7-9b25-56ec90c84807","slug":"/p/4cb47741-f933-4bc7-9b25-56ec90c84807"},"sidebar":"tutorialSidebar","previous":{"title":"window attention, page attention, kv cache","permalink":"/notes/docs/p/d54979a7-f0e9-4530-a0df-e48aa3bd1ddc"},"next":{"title":"03. Leetcode","permalink":"/notes/docs/leetcode"}}');var s=t(74848),r=t(28453);const l={created_at:"2025-11-01",page_link:"/p/4cb47741-f933-4bc7-9b25-56ec90c84807",slug:"/p/4cb47741-f933-4bc7-9b25-56ec90c84807"},o=void 0,c={},a=[{value:"\u7ec6\u8282",id:"\u7ec6\u8282",level:2},{value:"2.1: self-attention trick: matrix multiplication",id:"21-self-attention-trick-matrix-multiplication",level:2}];function h(e){const n={blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",hr:"hr",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"\u5b9a\uff1aSelf-Attention \u7528 Q/K/V \u5c06\u5e8f\u5217\u5185 token \u4e24\u4e24\u76f8\u4f3c\u5ea6\u8f6c\u6210\u52a0\u6743\u548c\uff1aAttn(Q,K,V)=softmax(QK\u1d40/\u221ad_k)V\uff1b\u591a\u5934\u5728\u4e0d\u540c\u5b50\u7a7a\u95f4\u5e76\u884c\u6355\u6349\u5173\u7cfb\u3002"}),"\n",(0,s.jsx)(n.p,{children:"\u56e0\uff1a\u5355\u5934\u5bb9\u91cf\u6709\u9650\uff0c\u96be\u4ee5\u540c\u65f6\u5efa\u6a21\u591a\u7c7b\u5173\u7cfb\uff08\u8bed\u6cd5/\u8bed\u4e49/\u4f4d\u7f6e\uff09\uff1b\u591a\u5934\u63d0\u4f9b\u201c\u89c6\u89d2\u591a\u6837\u6027\u201d\u3002"}),"\n",(0,s.jsx)(n.p,{children:"\u6cd5\uff1a\n1.\t\u7ebf\u6027\u6295\u5f71\uff1aQ=XW_Q, K=XW_K, V=XW_V\uff1b\n2.\t\u8ba1\u7b97 logits\uff1aL=QK\u1d40/\u221ad_k\uff1b\n3.\tsoftmax \u6743\u91cd\uff1aA=softmax(L)\uff1b\n4.\t\u52a0\u6743\uff1aO = AV\uff1b\n5.\t\u591a\u5934\u5e76\u8054 concat \u540e\u8fc7 W_O\u3002"}),"\n",(0,s.jsx)(n.p,{children:"\u679c\uff1a\u8868\u793a\u66f4\u4e30\u5bcc\u3001\u68af\u5ea6\u66f4\u7a33\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}),"\n",(0,s.jsx)(n.p,{children:"\u4f18\uff1a\u5e76\u884c\u9ad8\u6548\u3001\u8868\u8fbe\u529b\u5f3a\u3001\u53ef\u4e0e\u63a9\u7801/\u4f4d\u7f6e\u504f\u7f6e\u7ec4\u5408\u3002"}),"\n",(0,s.jsx)(n.p,{children:"\u7ec6\u8282 & \u4f8b\u5b50\n\u53e5\u5b50 \u201cA smart robot\u201d. \u4e09\u4e2a token \u7684 QK\u1d40 \u5f62\u6210 3\xd73 \u76f8\u4f3c\u5ea6\u77e9\u9635\uff1bsoftmax \u540e\u5f97\u5230\u6bcf\u4e2a\u8bcd\u5bf9\u5176\u5b83\u8bcd\u7684\u6ce8\u610f\u529b\u5206\u5e03\uff1b\u591a\u5934\uff1a\u67d0\u5934\u504f\u8bed\u6cd5\uff08\u5f62\u5bb9\u8bcd\u2192\u540d\u8bcd\uff09\uff0c\u53e6\u4e00\u5934\u504f\u8bed\u4e49\uff08\u540c\u4e49/\u5171\u73b0\uff09\u3002"}),"\n",(0,s.jsx)(n.p,{children:"3 \u4e2a Follow-up\nA. \u201c\u4e3a\u4ec0\u4e48\u8981\u7f29\u653e 1/\u221ad_k\uff1f\u201d\uff08\u5b9a\u56e0\u6cd5\u679c\u4f18\u7565\uff09\u9632\u6b62\u5185\u79ef\u968f\u7ef4\u5ea6\u589e\u5927\u800c\u65b9\u5dee\u8fc7\u5927\u5bfc\u81f4 softmax \u9971\u548c\uff0c\u4fdd\u8bc1\u68af\u5ea6\u7a33\u5b9a\u3002\nB. \u201cAdditive Attention \u548c Dot-Product \u5dee\u5f02\uff1f\u201d Additive \u7528 MLP \u8bc4\u5206\uff0c\u7cbe\u7ec6\u4f46\u6162\uff1b\u70b9\u79ef\u9002\u914d\u77e9\u9635\u4e58\u786c\u4ef6\u66f4\u5feb\uff0cTransformer \u9009\u540e\u8005\u3002\nC. \u201c\u591a\u5934\u6570\u91cf\u600e\u4e48\u9009\uff1f\u201d \u7ecf\u9a8c\u4e0e\u663e\u5b58/\u541e\u5410\u6298\u4e2d\uff08\u5982 12/16/32 \u5934\uff09\uff1b\u7ef4\u5ea6\u56fa\u5b9a\u65f6\u5934\u592a\u591a\u4f1a\u8ba9\u6bcf\u5934\u7ef4\u5ea6\u592a\u5c0f\uff0c\u8868\u8fbe\u53d7\u9650\u3002"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202511022057851.png",alt:""})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202511022058852.png",alt:""})}),"\n",(0,s.jsx)(n.p,{children:"implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\nfrom torch import nn\n\n# scenario: ["I like green"] => ["I like green vegetables"] (b = 1, T = 3, C = 2)\n# vocab: [I:0, like:1, green:2, vegetables: 3]\nX = torch.tensor([[0, 1, 2]])\nemb = nn.Embedding(4, 2) ## 2 * 2 dimension\nX = emb(X)\n\n\n\n# ---- self attention ----\nB, T, C = X.shape\nquery_layer = nn.Linear(C, 2) # attention weight has 2 dimension\nkey_layer = nn.Linear(C, 2)\n\nq = query_layer(X) # 1, 3, 2\nk = key_layer(X) # 1, 3, 2\n\nW = q @ k.transpose(1,2) / 2 ** 0.5 # q dot product k transposed, finding similarity of the two vector\nlower_tri = torch.ones(T, T).tril()\nW = W.masked_fill(lower_tri == 0, -torch.inf) # use -torch.inf because softmax(-torch.inf)\nW = W.softmax(dim = -1) # the softmax sums up each row, which reduce through number of columns, which is the last dimension\n\nprint(W)\nprint(W.shape)\n\n\nvalue_layer = nn.Linear(C, 2)\n\nv = value_layer(X)\n\nout = W @ v # 3*3, 3*2\nprint(out.shape)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"\u7ec6\u8282",children:"\u7ec6\u8282"}),"\n",(0,s.jsx)(n.h1,{id:"step-2-build-self-attention",children:"Step 2: Build Self-Attention"}),"\n",(0,s.jsx)(n.h2,{id:"21-self-attention-trick-matrix-multiplication",children:"2.1: self-attention trick: matrix multiplication"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Question 1: let\u2019s say we have an X that\u2019s [1,2,3,4], how do we update it to [1,3,6,10]?"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Motivation: we want tokens to communicate to their past."}),"\n",(0,s.jsx)(n.p,{children:"B (batch), T (time), C (channel), we need to take the token and allow it to have the aggregated info of things in the past (back in the T(time) dimension)."}),"\n",(0,s.jsx)(n.p,{children:"How do we do it with least amount of loops/iterations?"}),"\n",(0,s.jsx)(n.p,{children:"turns out:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"[1,2,3,4] * [[1,1,1,1],    = [1,3,6,10]\n\t\t\t\t\t\t\t[0,1,1,1],      \n\t\t\t\t\t\t\t[0,0,1,1],      \n\t\t\t\t\t\t\t[0,0,0,1]]      \n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsxs)(n.p,{children:["Follow-up question 2, let\u2019s say X is ",(0,s.jsxs)(n.em,{children:["[[[1,2,3,4]] * 10], how do we simultaneously update all of them to ",(0,s.jsx)(n.code,{children:"[[[1,3,6,10]] * 10]"})," ?"]})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u76f4\u63a5\u7528 ",(0,s.jsx)(n.code,{children:"*[[[1,2,3,4] ]* 10]"})," ** \u521a\u521a\u90a3\u4e2a\u4e2d\u95f4\u90a3\u4e2amatrix"]}),"\n",(0,s.jsx)(n.p,{children:"\u7528torch\u8868\u793a"}),"\n",(0,s.jsx)(n.p,{children:"use torch.tril\uff0c\u7ed9\u4e00\u4e2ainput\u5168\u662f1\u7684matrix\uff0c\u51fa\u6765\u4e00\u4e2a\u548c\u4e0a\u9762\u53cd\u8fc7\u6765\u7684lower triangle"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"t = torch.tensor([[1,2,3,4] for _ in range(10)], dtype=torch.float32)\nmul = torch.tril(torch.ones((4,4),dtype=torch.float32)).T\n\nres = t @ mul\nprint(res)\nprint(res.shape)\n"})}),"\n",(0,s.jsx)(n.p,{children:"output:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"tensor([[ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.],\n        [ 1.,  3.,  6., 10.]])\ntorch.Size([10, 4])\n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Question 3: What if X is of dimension: 10 * 4 * 3, and we want to get the sum of the 1th dimension, i,e, the dimension of 1,2,3,4?"}),"\n",(0,s.jsxs)(n.p,{children:["10 * 4 * 3 ",(0,s.jsx)(n.code,{children:"@"})," 4* 4, this does not work, even if torch add a batch dimension so that 4 * 4 becomes 10 * 4 * 4"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s think about how to do it the other way around."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"[[1,0,0,0],   * [1,2,3,4]  = [[1],[3],[6],[10]]\n[1,1,0,0],      \n[1,1,1,0],      \n[1,1,1,0]]    \n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"torch.tril(torch.ones(4,4))"})," * ",(0,s.jsx)(n.code,{children:"10\u4e2a[1,2,3,4]"})," \u8f6c\u7f6e\uff0c\u5c31\u7b49\u4e8e10\u4e2a ",(0,s.jsx)(n.code,{children:"[1,3,6,10]"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"4 * 4 @ 10 * 4 *3"})}),"\n",(0,s.jsxs)(n.p,{children:["torch\u4f1a\u7ed9",(0,s.jsx)(n.code,{children:"4 * 4"}),"\u518d\u52a0\u4e00\u4e2abatch dimension\uff0c \u53d8\u6210 10 * 4 * 4"]}),"\n",(0,s.jsx)(n.p,{children:"\u5b9e\u9645\u4e0a\u7684\u8ba1\u7b97\u662f\u6bcf\u4e2abatch\u5185\u8fdb\u884c4_4 \u7684tril\u548c4_3\u7684matrix\u7684\u70b9\u4e58"}),"\n",(0,s.jsx)(n.p,{children:"\u6240\u4ee5\uff0c\u6700\u7ec8\u7684torch implementation\u662f\u8fd9\u6837"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"t = torch.randn(10,4,3)\nmul = torch.tril(torch.ones((4,4),dtype=torch.float32))\n\nres = mul @ t\nprint(res.shape) # torch.Size([10, 4, 3])\n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Question 4: \u6211\u4eec\u73b0\u5728\u9700\u8981\u7684\u662f\u4e00\u4e2aweight matrix\u7528\u6765\u6700\u7ec8\u548c \u5904\u7406\u8fc7\u7684x\u8fdb\u884c\u70b9\u4e58\uff0c \u8fd9\u4e2aweight matrix\u9700\u8981\u6ee1\u8db3\u8fd9\u4e9b\u6761\u4ef6\uff1a"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u6bd4\u5982 I like green vegetable, \u73b0\u5728\u7684\u8868\u793a\u662f ",(0,s.jsx)(n.code,{children:"[[I0.3,I0.5], [like0.5,like0.5],[green0.6,green0.1], [vegetable0.9,vegetable0.1]]"})," \u8981\u53d8\u6210"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"[[I-I1, 0,0,0],\n [like-I0.3, like-like0.7,0,0,0]],\n [green-I0.1, green-like0.1,green-green0.8,0,0]],\n [vegetable-I0.05, vegetable-like0.15,vegetable-green0.6,vegetable-vegetable0.2]],\n"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"\u6211\u4eec\u9700\u8981\u505a\u7684\u5c31\u662fquery \u548c key \u70b9\u4e58\uff0c"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"query\uff1a what I want"}),"\n",(0,s.jsx)(n.li,{children:"key: what I have."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"\u5bf9\u4e8e\u4ee5\u4e0a\u7684\u4f8b\u5b50\uff0c\u6211\u4eec\u9700\u8981"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\t[[I0.3, like0.5, green0.6, vegetable0.9],   @  [[I0.3,I0.5], \n [I0.5, like0.6, green0.1, vegetable0.1]]         [like0.5,like0.5],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[green0.6,green0.1], \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[vegetable0.9,vegetable0.1]]\n"})}),"\n",(0,s.jsx)(n.p,{children:"\u5bf9\u4e8e\u8fd9\u4e2a\u7ed3\u679c\uff0c\u628a\u672a\u6765\u672a\u77e5\u7684\u90e8\u5206mask\u6210inf\uff0c\u7136\u540e\u505asoftmax"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Why did we use softmax at the last step for ",(0,s.jsx)(n.code,{children:"q*k"}),"?\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u8bb0\u5f97\u6211\u4eec\u5728backprop-ninja\u90a3\u4e2asession\u63a8\u5bfc\u8fc7negative log likelihood loss\u5bf9\u4e8e\u6bcf\u4e2a\u5143\u7d20\u7684\u5bfc\u6570\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u5bf9\u4e8e\u6b63\u786e\u7684prediction\u662f1-p"}),"\n",(0,s.jsx)(n.li,{children:"\u5bf9\u4e8e\u9519\u8bef\u7684prediction\u662fp"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\u6240\u4ee5\u6709\u4e86softmax\uff0c\u5b83\u53ef\u4ee5\u628a",(0,s.jsx)(n.code,{children:"q*k"}),"\u5f80 \u8d8a\u6709\u5173\u7cfb\uff0c\u5c31\u8d8a\u9760\u8fd11\uff0c\u8d8a\u6ca1\u6709\u5173\u7cfb\uff0c\u5c31\u8d8a\u9760\u8fd10 \u8fd9\u4e2a\u65b9\u5411\u8fdb\u884cbackpropagation"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"\u4ee5\u4e0a\u5f97\u5230\u662f\u65b0\u7684weight\uff0c \u518d\u4e58\u4ee5\u5904\u7406\u8fc7\u7684x"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u6211\u4eec\u662f\u5426\u9700\u8981\u4e00\u76f4mask\u672a\u6765\u672a\u77e5\u7684\u90e8\u5206\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["mask\u6389\u672a\u6765\u672a\u77e5\u7684\u90e8\u5206\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4e0d\u518d\u5173\u5fc3 e.g., I-green\u7684\u5173\u7cfb\uff0c\u800c\u662f\u53ea\u5173\u5fc3green-I \u7684\u5173\u7cfb\uff0c\u5728transformer\u4e2d\u8fd9\u4e2a\u5904\u7406\u662f\u6709\u7528\u7684\uff0c\u4f46\u662f\u5982\u679c\u6bd4\u5982\u6211\u4eec\u662f\u505atext analysis\uff0ce.g.\uff0ctext classification\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0dmask\u6389\u672a\u6765\u7684\u672a\u77e5\u90e8\u5206\uff0c\u56e0\u4e3a\u6211\u4eec\u662f\u5173\u5fc3\u90a3\u4e9b\u5185\u5bb9\u7684\u3002\u8fd9\u4e2a\u5728torch\u7684\u5904\u7406\u4e0a\u5c31\u662f\u628a",(0,s.jsx)(n.code,{children:"decoder"})," \u53d8\u6210 ",(0,s.jsx)(n.code,{children:"encoder"}),", \u5c31\u4f1a\u53bb\u6389mask\u7684\u90a3\u4e00\u884c\u3002"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"torch implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\tx = torch.randn(10,4,32)\n\tquery_layer = nn.Linear(32, 2, bias=False)\n\tkey_layer = nn.Linear(32, 2, bias=False)\n\tq = query_layer(x)\n\tk = key_layer(x)\n\tweight = q @ k.transpose(1,2)\n\ttril = torch.tril(torch.ones(4,4))\n\tweight = weight.masked_fill(tril == 0, -torch.inf)\n\tweight = weight.softmax(dim=-1)\n\tvalue_layer = nn.Linear(32, 2, bias=False)\n\tv = value_layer(x)\n\tout = weight @ v\n\tprint(weight[0])\n\tprint(out.shape)\n"})}),"\n",(0,s.jsx)(n.p,{children:"output"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n        [0.5654, 0.4346, 0.0000, 0.0000],\n        [0.3271, 0.2887, 0.3841, 0.0000],\n        [0.2639, 0.2359, 0.2198, 0.2805]], grad_fn=<SelectBackward0>)\ntorch.Size([10, 4, 2])\n"})}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Question 5: during weight initialization, how do we make the weight less to the extreme? (remember, softmax makes bigger values even closer to 1)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"answer:"}),"\n",(0,s.jsx)(n.p,{children:"we modify"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"weight = q @ k.transpose(1,2)\n"})}),"\n",(0,s.jsx)(n.p,{children:"to"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"weight = q @ k.transpose(1,2) / head_size ** -0.5\n"})}),"\n",(0,s.jsx)(n.p,{children:"in our case, head_size = 2."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"Summary"}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"I am a sentence."}),"\n",(0,s.jsx)(n.li,{children:"Here\u2019s what I am interested in (q)"}),"\n",(0,s.jsx)(n.li,{children:"Here\u2019s what I have (k)"}),"\n",(0,s.jsxs)(n.li,{children:["if I find some info interesting, here\u2019s what I present to you ",(0,s.jsx)(n.code,{children:"(weight @ v)"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Self-Attention \u7684 q @ k.T"}),": \u8861\u91cf token \u4e4b\u95f4\u201c\u65b9\u5411\u76f8\u4f3c\u5ea6\u201d\uff0c\u51b3\u5b9a\u6ce8\u610f\u529b\u6743\u91cd"]}),"\n",(0,s.jsx)(n.p,{children:"![[Pasted image 20251031170410.png]]![[Pasted image 20251031170430.png]]"})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);