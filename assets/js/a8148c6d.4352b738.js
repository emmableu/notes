"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[88310],{5564:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>n,toc:()=>h});const n=JSON.parse('{"id":"ML General/T-SNE or tSNE or tsne","title":"T-SNE or tSNE or tsne","description":"Definition:","source":"@site/docs/05. ML General/40.T-SNE or tSNE or tsne.md","sourceDirName":"05. ML General","slug":"/p/a5f44b92-03c1-475e-a773-76c53542b1ad","permalink":"/notes/docs/p/a5f44b92-03c1-475e-a773-76c53542b1ad","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/05. ML General/40.T-SNE or tSNE or tsne.md","tags":[],"version":"current","sidebarPosition":40,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/a5f44b92-03c1-475e-a773-76c53542b1ad","slug":"/p/a5f44b92-03c1-475e-a773-76c53542b1ad"},"sidebar":"tutorialSidebar","previous":{"title":"Gradient Boost \u7ec6\u8282","permalink":"/notes/docs/p/1f533186-531c-44b3-96e0-eaa311ac7ff8"},"next":{"title":"Convexity, local, global minima","permalink":"/notes/docs/p/5e97f5b9-0ff7-4097-9053-526db6d75222"}}');var s=i(74848),a=i(28453);const o={created_at:"2025-11-02",page_link:"/p/a5f44b92-03c1-475e-a773-76c53542b1ad",slug:"/p/a5f44b92-03c1-475e-a773-76c53542b1ad"},r=void 0,l={},h=[{value:"Definition:",id:"definition",level:2},{value:"process",id:"process",level:2},{value:"detailed process",id:"detailed-process",level:2},{value:"How t-SNE works?",id:"how-t-sne-works",level:2},{value:"Probability Distribution",id:"probability-distribution",level:2},{value:"Scattered clusters and variance",id:"scattered-clusters-and-variance",level:2},{value:"Dealing with different distances",id:"dealing-with-different-distances",level:2},{value:"The lie :)",id:"the-lie-",level:2},{value:"Perplexity",id:"perplexity",level:2},{value:"Original formula interpretation",id:"original-formula-interpretation",level:2},{value:"Create low-dimensional space",id:"create-low-dimensional-space",level:2},{value:"Tricks (optimizations) done in t-SNE to perform better",id:"tricks-optimizations-done-in-t-sne-to-perform-better",level:2},{value:"Early Compression",id:"early-compression",level:2},{value:"Early Exaggeration",id:"early-exaggeration",level:2}];function d(e){const t={a:"a",blockquote:"blockquote",em:"em",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h2,{id:"definition",children:"Definition:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"What t-SNE does is find a way to project data into a low dimensional space, so that the clustering in the high dimensional space is preserved"}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"process",children:"process"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202210061845422.png",alt:""})}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"we start with putting the points to random points in low dimensional space (on the line)"}),"\n",(0,s.jsx)(t.li,{children:"at each step, a point in the new dimensional space (on the line) is attracted to points it is near in the original space (the scatter point), and repelled by points it is far from."}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"detailed-process",children:"detailed process"}),"\n",(0,s.jsx)(t.h2,{id:"how-t-sne-works",children:"How t-SNE works?"}),"\n",(0,s.jsx)(t.h2,{id:"probability-distribution",children:"Probability Distribution"}),"\n",(0,s.jsxs)(t.p,{children:["Let\u2019s start with ",(0,s.jsx)(t.strong,{children:"SNE"})," part of t-SNE. I\u2019m far better with explaining things visually so this is going to be our dataset:"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*NYve_va3wHU4zNnj.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["It has 3 different classes and you can easily distinguish them from each other. The first part of the algorithm is to create a ",(0,s.jsx)(t.strong,{children:"probability distribution"})," that represents similarities between neighbors. What is \u201csimilarity\u201d? ",(0,s.jsx)(t.a,{href:"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",children:"Original paper"})," states \u201c ",(0,s.jsx)(t.strong,{children:"similarity of datapoint"})," x\u2c7c ",(0,s.jsx)(t.strong,{children:"to datapoint"})," x\u1d62 ",(0,s.jsx)(t.strong,{children:"is the conditional probability"})," p_{j|i}",(0,s.jsx)(t.strong,{children:", that"})," x\u1d62 ",(0,s.jsx)(t.strong,{children:"would pick"})," x\u2c7c ",(0,s.jsx)(t.strong,{children:"as its neighbor"})," \u201c."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*ETuCH0wXgSyit1i3.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"We\u2019ve picked one of the points from the dataset. Now we have to pick another point and calculate Euclidean Distance between them |x\u1d62 \u2014 x\u2c7c|"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*g1bTTv6Wu632piCu.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["The next part of the original paper states that it has to be ",(0,s.jsx)(t.strong,{children:"proportional to probability density under a Gaussian centered at"})," x\u1d62. So we have to generate Gaussian distribution with mean at x\u1d62_,_ and place our distance on the X-axis."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*j6P77qstfwQ6mkT8.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["Right now you might wonder about ",(0,s.jsx)(t.em,{children:"\u03c3\xb2"})," (variance) and that\u2019s a good thing. But let\u2019s just ignore it for now and assume I\u2019ve already decided what it should be. After calculating the first point we have to do the same thing for every single point out there."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*Afr8xsKrl6dwZ10Q.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"You might think, we\u2019re already done with this part. But that\u2019s just the beginning."}),"\n",(0,s.jsx)(t.h2,{id:"scattered-clusters-and-variance",children:"Scattered clusters and variance"}),"\n",(0,s.jsx)(t.p,{children:"Up to this point, our clusters were tightly bounded within its group. What if we have a new cluster like that:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*nZrA0hPQqM1Ce7-j.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"We should be able to apply the same process as before, shouldn\u2019t we?"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*0MzS6wydzd5Dr02Y.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"We\u2019re still not done. You can distinguish between similar and non-similar points but absolute values of probability are much smaller than in the first example (compare Y-axis values)."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*kpctqZwCMDkqJK3u.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"We can fix that by dividing the current projection value by the sum of the projections."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/526/1*1gBOzGPwWEN4L_HhYLN-VQ.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"Which if you apply to the first example will look sth like:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1228/1*r3tuMMndpFZswRfIVNCEaw.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"And for the second example:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1262/1*9XfcJTE-gFjGxFgdqOQKsQ.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"This scales all values to have a sum equal to 1. It\u2019s a good place to mention that p_{i|i}\u200b is set to be equal to 0, not 1."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/228/1*2oCxcLq7hvxQcrjmFiClMA.png",alt:""})}),"\n",(0,s.jsx)(t.h2,{id:"dealing-with-different-distances",children:"Dealing with different distances"}),"\n",(0,s.jsx)(t.p,{children:"If we take two points and try to calculate conditional probability between them then values of p_{i|j}\u200b and p_{j|i}\u200b will be different:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*pTTqRArwYV_tGnF0.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*-JXLaDNYjjSSGVdf.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"The reason for that is because they are coming from two different distributions. Which one should we pick to the calculation then?"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/336/1*gFbUxuUZlcRhIzYXvYzUCA.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["Where ",(0,s.jsx)(t.em,{children:"N"})," is a number of dimensions."]}),"\n",(0,s.jsx)(t.h2,{id:"the-lie-",children:"The lie :)"}),"\n",(0,s.jsxs)(t.p,{children:["Now when we have everything scaled to 1 (yes, the sum of all equals 1), I can tell you that I wasn\u2019t completely honest about while the process with you :) Calculation all of that would be quite painful for the algorithm and that\u2019s not what exactly is in ",(0,s.jsx)(t.a,{href:"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",children:"t-SNE paper"}),"."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/736/1*J4lRX3F6qR9TF9VgH6Vd1Q.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"This is an original formula to calculate p_{j|i}. Why did I lie to you? First, because it\u2019s easier to get an intuition about how it works. Second, because I was going to show you the trough either way."}),"\n",(0,s.jsx)(t.h2,{id:"perplexity",children:"Perplexity"}),"\n",(0,s.jsx)(t.p,{children:"If you look at this formula. You can spot that our"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/226/1*2q0ECctHTmnbCop71LlJOQ.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"is"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/446/1*3_qQH7KjQR89ymcDk0Y5Yw.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["If I would show you this straight away, it would be hard to explain where ",(0,s.jsx)(t.em,{children:"\u03c3\xb2"})," is coming from and what is a dependency between it and our clusters. Now you know that variance depends on Gaussian and the number of points surrounding the center of it. This is the part where ",(0,s.jsx)(t.strong,{children:"perplexity"})," value comes. A perplexity is more or less a target number of neighbors for our central point. Basically, the higher the perplexity is the higher value variance has. Our \u201cred\u201d group is close to each other and if we set perplexity to 4, it searches the right value of to \u201cfit\u201d our 4 neighbors. If you want to be more specific then you can quote the original paper:"]}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.em,{children:"SNE performs a binary search for the value of sigma that produces probability distribution with a fixed perplexity that is specified by the user"})}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/496/1*Csv1yl-2zOxC42wV-WGVkQ.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"Where"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/292/1*jhLo78eY9Jky4vMP42RC8Q.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["\u200b is ",(0,s.jsx)(t.strong,{children:"Shannon entropy"}),". But unless you want to implement t-SNE yourself, the only thing you need to know is that perplexity you choose is positively correlated with the value of \\mu_i_\u03bci_\u200b and for the same perplexity you will have multiple different \\mu_i_\u03bci_\u200b, base on distances. Typical perplexity value ranges between 5 and 50."]}),"\n",(0,s.jsx)(t.h2,{id:"original-formula-interpretation",children:"Original formula interpretation"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/736/1*J4lRX3F6qR9TF9VgH6Vd1Q.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"When you look on this formula you might notice that our Gaussian is converted into"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/436/1*xi6CUrwPLG4-1CZwt-hX0Q.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"Let me show you how that looks like:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*sNHrck20Xt7uS7X9.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["If you play with ",(0,s.jsx)(t.em,{children:"\u03c3\xb2"})," for a while you can notice that the blue curve remains fixed at point ",(0,s.jsx)(t.em,{children:"x"}),"=0. It only stretches when ",(0,s.jsx)(t.em,{children:"\u03c3\xb2"})," increases."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*SQWPC3TlgUqnsuSu.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"That helps distinguish neighbor\u2019s probabilities and because you\u2019ve already understood the whole process you should be able to adjust it to new values."}),"\n",(0,s.jsx)(t.h2,{id:"create-low-dimensional-space",children:"Create low-dimensional space"}),"\n",(0,s.jsxs)(t.p,{children:["The next part of t-SNE is to create low-dimensional space with the same number of points as in the original space. Points should be spread randomly on a new space. The goal of this algorithm is to find similar probability distribution in low-dimensional space. The most obvious choice for new distribution would be to use Gaussian again. That\u2019s not the best idea, unfortunately. One of the properties of Gaussian is that it has a \u201cshort tail\u201d and because of that it creates a ",(0,s.jsx)(t.strong,{children:"crowding problem"}),". To solve that we\u2019re going to use ",(0,s.jsx)(t.strong,{children:"Student t-distribution"})," with a single degree of freedom. More of how this distribution was selected and why Gaussian is not the best idea you can find in the ",(0,s.jsx)(t.a,{href:"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",children:"paper"}),". I decided not to spend much time on it and allow you to read this article within a reasonable time. So now our new formula will look like:"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/632/1*ZDiRzmCfK1xuCzldJ8-tvQ.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"instead of:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/690/1*Hax1tT4LMqH9RqUN4d7ulA.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"If you\u2019re more \u201cvisual\u201d person this might help (values on X-axis are distributed randomly):"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*z_xyrUJsAlkdKNjX.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*SEhDsFAK-qH6fLJv.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["Using Student distribution has exactly what we need. It \u201cfalls\u201d quickly and has a \u201clong tail\u201d so points won\u2019t get squashed into a single point. This time we don\u2019t have to bother with ",(0,s.jsx)(t.em,{children:"\u03c3\xb2"})," because we don\u2019t have one in q_{ij} formula. I won\u2019t generate the whole process of calculating q_{ij} because it works exactly the same as p_{ij}. Instead, just leave you with those two formulas and skip to sth more important:"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/726/1*l4Gjd2F_cnZQDvSi6zlu1w.png",alt:""})}),"\n",(0,s.jsx)(t.h1,{id:"gradient-descent",children:"Gradient descent"}),"\n",(0,s.jsxs)(t.p,{children:["To optimize this distribution t-SNE is using ",(0,s.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence",children:(0,s.jsx)(t.strong,{children:"Kullback-Leibler divergence"})})," between the conditional probabilities p_{j|i} and q_{j|i}"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/846/1*xi-IjvMSJmNu-jfHlZ0qvA.png",alt:""})}),"\n",(0,s.jsxs)(t.p,{children:["I\u2019m not going through the math here because it\u2019s not important. What we need is a derivate for (it\u2019s derived in ",(0,s.jsx)(t.strong,{children:"Appendix A"})," inside the ",(0,s.jsx)(t.a,{href:"http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf",children:"original paper"}),")."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/938/1*WK-kP2JJsAbYgw49hQENhA.png",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"You can treat that gradient as repulsion and attraction between points. A gradient is calculated for each point and describes how \u201cstrong\u201d it should be pulled and the direction it should choose. If we start with our random 1D plane and perform gradient on the previous distribution it should look like this."}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"https://miro.medium.com/max/1400/0*gx5m_CS7gVUn8WLH.gif",alt:""})}),"\n",(0,s.jsx)(t.p,{children:"Ofc. this is an exaggeration. t-SNE doesn\u2019t run that quickly. I\u2019ve just skipped a lot of steps in there to make it faster. Besides that, the values here are not completely correct, but it\u2019s good enough to show you the process."}),"\n",(0,s.jsx)(t.h2,{id:"tricks-optimizations-done-in-t-sne-to-perform-better",children:"Tricks (optimizations) done in t-SNE to perform better"}),"\n",(0,s.jsx)(t.p,{children:"t-SNE performs well on itself but there are some improvements allow it to do even better."}),"\n",(0,s.jsx)(t.h2,{id:"early-compression",children:"Early Compression"}),"\n",(0,s.jsx)(t.p,{children:"To prevent early clustering t-SNE is adding L2 penalty to the cost function at the early stages. You can treat it as standard regularization because it allows the algorithm not to focus on local groups."}),"\n",(0,s.jsx)(t.h2,{id:"early-exaggeration",children:"Early Exaggeration"}),"\n",(0,s.jsx)(t.p,{children:"This trick allows moving clusters of (q_{ij}\u200b) more. This time we\u2019re multiplying p_{ij}\u200b in early stages. Because of that clusters don\u2019t get in each other\u2019s ways."}),"\n",(0,s.jsx)(t.h1,{id:"conclusions",children:"Conclusions"}),"\n",(0,s.jsx)(t.p,{children:"t-SNE is a great tool to understand high-dimensional datasets. It might be less useful when you want to perform dimensionality reduction for ML training (cannot be reapplied in the same way). It\u2019s not deterministic and iterative so each time it runs, it could produce a different result. But even with that disadvantages it still remains one of the most popular method in the field."})]})}function c(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,t,i)=>{i.d(t,{R:()=>o,x:()=>r});var n=i(96540);const s={},a=n.createContext(s);function o(e){const t=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),n.createElement(a.Provider,{value:t},e.children)}}}]);