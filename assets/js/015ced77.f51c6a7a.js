"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[28043],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(96540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},54552:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"Zero To Hero/Makemore 1.2 - trigram 1","title":"Makemore 1.2 - trigram 1","description":"Youtube link//www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2","source":"@site/docs/04. Zero To Hero/03.Makemore 1.2 - trigram 1.md","sourceDirName":"04. Zero To Hero","slug":"/p/34b50f11-765b-4bf1-9b91-7232cedd6a34","permalink":"/notes/docs/p/34b50f11-765b-4bf1-9b91-7232cedd6a34","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/04. Zero To Hero/03.Makemore 1.2 - trigram 1.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/34b50f11-765b-4bf1-9b91-7232cedd6a34","slug":"/p/34b50f11-765b-4bf1-9b91-7232cedd6a34"},"sidebar":"tutorialSidebar","previous":{"title":"Makemore 1.1 - bigram - Train and Evaluate a Bigram Model","permalink":"/notes/docs/p/d9d7f60b-8e6c-4fcd-a40d-6e44deaca3c8"},"next":{"title":"Makemore 2 - MLP","permalink":"/notes/docs/p/698a3541-6dd9-4a29-a38c-18bbe80f8d33"}}');var r=i(74848),t=i(28453);const o={created_at:"2025-11-02",page_link:"/p/34b50f11-765b-4bf1-9b91-7232cedd6a34",slug:"/p/34b50f11-765b-4bf1-9b91-7232cedd6a34"},a="Exercises: Train and Evaluate a Trigram Model",l={},d=[{value:"Exercise 1: Use trigram probabilities to generate new trigram samples.",id:"exercise-1-use-trigram-probabilities-to-generate-new-trigram-samples",level:2},{value:"1.1 Step 1: Create a Count Matrix N",id:"11-step-1-create-a-count-matrix-n",level:2},{value:"1.2 Step 2: Use a Multinomial Probability to Generate Trigrams",id:"12-step-2-use-a-multinomial-probability-to-generate-trigrams",level:2},{value:"1.2.1: To do that, first generate the probability matrix",id:"121-to-do-that-first-generate-the-probability-matrix",level:3},{value:"1.2.2: Add a fake count (Regularization)",id:"122-add-a-fake-count-regularization",level:3},{value:"1.2.3: Verify the created probability matrix is correct",id:"123-verify-the-created-probability-matrix-is-correct",level:3},{value:"1.2.4: Generate words",id:"124-generate-words",level:3},{value:"2.1 Step 1: Prepare X and y",id:"21-step-1-prepare-x-and-y",level:3},{value:"2.2. Step 2: Model training",id:"22-step-2-model-training",level:3},{value:"2.3 Step 3: Verify from the loss in step 1.",id:"23-step-3-verify-from-the-loss-in-step-1",level:3},{value:"2.4 Step 4: Generate names based on the probabilities",id:"24-step-4-generate-names-based-on-the-probabilities",level:3},{value:"2.5 Step 5: Smoothing/Regularization",id:"25-step-5-smoothingregularization",level:3},{value:"Appendix 1. Broadcasting rules",id:"appendix-1-broadcasting-rules",level:2},{value:"Trailing Dimension",id:"trailing-dimension",level:3},{value:"Broadcasting",id:"broadcasting",level:3},{value:"Rules for Broadcasting",id:"rules-for-broadcasting",level:3},{value:"The Statement Explained",id:"the-statement-explained",level:3},{value:"Examples",id:"examples",level:3},{value:"Example 1: Compatible Arrays",id:"example-1-compatible-arrays",level:3},{value:"Example 2: Incompatible Arrays",id:"example-2-incompatible-arrays",level:3},{value:"Visual Example",id:"visual-example",level:3},{value:"Appendix 2. Torch.Generator",id:"appendix-2-torchgenerator",level:2},{value:"What It Does",id:"what-it-does",level:3},{value:"Why You Need This",id:"why-you-need-this",level:3},{value:"Difference If Not Using This",id:"difference-if-not-using-this",level:3},{value:"Example Usage",id:"example-usage",level:3},{value:"Summary",id:"summary",level:3},{value:"Appendix 3: Indexing multiple items in torch",id:"appendix-3-indexing-multiple-items-in-torch",level:2},{value:"<code>matrix[[r1, r2, r3], [c1, c2, c3]]</code>",id:"matrixr1-r2-r3-c1-c2-c3",level:3},{value:"Appendix 4: <code>dim</code>",id:"appendix-4-dim",level:2},{value:"Key Idea:",id:"key-idea",level:3},{value:"Appendix 5: torch.cat",id:"appendix-5-torchcat",level:2},{value:"Example 1: Concatenating 2D Tensors",id:"example-1-concatenating-2d-tensors",level:3},{value:"<strong>Concatenate along\xa0<code>dim=0</code></strong>\xa0(rows): - means expanding on number of rows.",id:"concatenate-alongdim0rows---means-expanding-on-number-of-rows",level:3},{value:"<strong>Concatenate along\xa0<code>dim=1</code></strong>\xa0(columns):",id:"concatenate-alongdim1columns",level:3}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["Youtube link: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2",children:"https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2"})]}),"\n",(0,r.jsxs)(n.p,{children:["github link: ",(0,r.jsx)(n.a,{href:"https://github.com/karpathy/makemore",children:"https://github.com/karpathy/makemore"})]}),"\n",(0,r.jsxs)(n.p,{children:["jupyter notebook: ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbWtKMk5XaW81QzJMTjVFSUhvdG9kMlo4ZGgyQXxBQ3Jtc0ttMmNrUUNDcWJFcHYxWUdFV3J0ZkppVE1HUG1yZktyUE52QmMzTWxZWW90c280N3NVeG55ekFibF9vdlluTnBBRFhDc2FnZ29hdjdiaGVxeVhleUczcWdpQ0pqcnNQbHozUUNacWlTbE91OFllMmpUcw&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fnn-zero-to-hero%2Fblob%2Fmaster%2Flectures%2Fmakemore%2Fmakemore_part1_bigrams.ipynb&v=PaCmpygFfXo"}),(0,r.jsx)(n.a,{href:"https://github.com/karpathy/nn-zero-t",children:"https://github.com/karpathy/nn-zero-t"}),"..."]}),"\n",(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"exercises-train-and-evaluate-a-trigram-model",children:"Exercises: Train and Evaluate a Trigram Model"})}),"\n",(0,r.jsx)(n.h2,{id:"exercise-1-use-trigram-probabilities-to-generate-new-trigram-samples",children:"Exercise 1: Use trigram probabilities to generate new trigram samples."}),"\n",(0,r.jsx)(n.p,{children:"Use the probabilities to generate new sequence of names, that looks like"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:".aylandia.\n.hosannaleine.\n.pieliyya.\n.jeah.\n.drielana.\n.anishlia.\n.el.\n.comon.\n...\n"})}),"\n",(0,r.jsx)(n.h2,{id:"11-step-1-create-a-count-matrix-n",children:"1.1 Step 1: Create a Count Matrix N"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"N[1,2,3]"}),' means how many counts of trigrams are "abc".']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"N[1,:,:].sum()"}),' means the number of all trigrams that starts with "a".']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["here\u2019s the names.txt file: ",(0,r.jsx)(n.a,{href:"https://github.com/karpathy/makemore/blob/master/names.txt",children:"https://github.com/karpathy/makemore/blob/master/names.txt"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nnames = open(\"makemore-master/names.txt\", 'r').read().splitlines()\nN = torch.zeros((27, 27, 27), dtype=torch.int32)\n\nchars = '.abcedfghijklmnopqrstuvwxyz'\nstoi = {s: i for i, s in enumerate(chars)}\nfor w in names:\n    chs = ['.'] + list(w) + ['.']\n    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n        trigram = (stoi[ch1], stoi[ch2], stoi[ch3])\n        N[trigram] += 1\n"})}),"\n",(0,r.jsx)(n.h2,{id:"12-step-2-use-a-multinomial-probability-to-generate-trigrams",children:"1.2 Step 2: Use a Multinomial Probability to Generate Trigrams"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'The chances of "." appearing first = 1 (since "." is always the starting point).'}),"\n",(0,r.jsxs)(n.li,{children:['If "." is the first, the chances of "a" appearing next = ',(0,r.jsx)(n.code,{children:"N[0, 1, :].sum() / N[0, :, :].sum()"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Using the below ",(0,r.jsx)(n.code,{children:"second_prob_matrix"}),", this same chance can be obtained at ",(0,r.jsx)(n.code,{children:"second_prob_matrix[1]"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:['If "a" is the second, the chances of "b" appearing after ".a" = ',(0,r.jsx)(n.code,{children:"N[0, 1, 2] / N[0, 1, :].sum()"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Using the below ",(0,r.jsx)(n.code,{children:"third_prob_matrix"}),", this same chance can be obtained at ",(0,r.jsx)(n.code,{children:"third_prob_matrix[0, 1, 2]"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:['If the previous 2 items are "ab", the chances of "c" appearing after "ab" = ',(0,r.jsx)(n.code,{children:"N[1, 2, 3] / N[1, 2, :].sum()"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Using the below ",(0,r.jsx)(n.code,{children:"third_prob_matrix"}),", this same chance can be obtained at ",(0,r.jsx)(n.code,{children:"third_prob_matrix[1, 2, 3]"}),"."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:['If the previous 2 items are "bc", the chances of "d" appearing after "bc" = ',(0,r.jsx)(n.code,{children:"N[2, 3, 4] / N[2, 3, :].sum()"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:['If the previous 2 items are "cd", the chances of "e" appearing after "cd" = ',(0,r.jsx)(n.code,{children:"N[3, 4, 5] / N[3, 4, :].sum()"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"121-to-do-that-first-generate-the-probability-matrix",children:"1.2.1: To do that, first generate the probability matrix"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"second_prob_matrix = N[0, :, :].sum(dim=1) / N[0, :, :].sum()\nthird_prob_matrix = N[:, :, :] / N[:, :, :].sum(dim=2, keepdim=True)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," It is essential to use ",(0,r.jsx)(n.code,{children:"keepdim=True"})," in the ",(0,r.jsx)(n.code,{children:"third_prob_matrix"})," calculation. If not, the dimensions will not align correctly for subsequent operations. Please check the below example to see the issue:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'M = torch.ones((2, 3, 4))\nprint(M.sum(dim=2).shape)\nprint(M/M.sum(dim=2))\n\n"""\noutput:\ntorch.Size([2, 3])\nTraceback (most recent call last):\n  File "/Users/wengranwang/Documents/applebot_page_classification/page-classification/test-script/zero-to-hero/test.py", line 4, in <module>\n    print(M/M.sum(dim=2))\nRuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 2\n"""\n\nM = torch.ones((2, 3, 4))\nprint(M.sum(dim=2, keepdim=True).shape)\nprint(M/M.sum(dim=2, keepdim=True))\n"""\noutput:\ntorch.Size([2, 3, 1])\ntensor([[[0.2500, 0.2500, 0.2500, 0.2500],\n         [0.2500, 0.2500, 0.2500, 0.2500],\n         [0.2500, 0.2500, 0.2500, 0.2500]],\n\n        [[0.2500, 0.2500, 0.2500, 0.2500],\n         [0.2500, 0.2500, 0.2500, 0.2500],\n         [0.2500, 0.2500, 0.2500, 0.2500]]])\n"""\n'})}),"\n",(0,r.jsx)(n.p,{children:"In our example, as the N matrix is 27 * 27 * 27, without keepdim=True it will not throw errors, but the way the matrix broadcast (i.e., copy item) is on a different direction."}),"\n",(0,r.jsx)(n.h3,{id:"122-add-a-fake-count-regularization",children:"1.2.2: Add a fake count (Regularization)"}),"\n",(0,r.jsxs)(n.p,{children:["Note, if you print ",(0,r.jsx)(n.code,{children:"third_prob_matrix"}),", you can see quite a few that has nan, e.g., ",(0,r.jsx)(n.code,{children:"vp"}),", this is because there\u2019s no trigram starts with ",(0,r.jsx)(n.code,{children:"vp"})," , so the denominator becomes 0. This would cause issues:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["in later step (1.2.4), when we generate samples based on this probability, we can\u2019t use ",(0,r.jsx)(n.code,{children:"nan"})," as the probability."]}),"\n",(0,r.jsxs)(n.li,{children:["Also, it is common to use the log probability as part of the calculation for the negative log likelihood loss function. For this calculation, ",(0,r.jsx)(n.code,{children:"- log(0)"})," becomes ",(0,r.jsx)(n.code,{children:"inf"}),", and we can\u2019t calculate ",(0,r.jsx)(n.code,{children:"log(nan)"})," ."]}),"\n",(0,r.jsx)(n.li,{children:"To fix this, we add a fake count to each element in N."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"N = N + 1\nsecond_prob_matrix = N[0, :, :].sum(dim=1) / N[0, :, :].sum()\nthird_prob_matrix = N[:, :, :] / N[:, :, :].sum(dim=2, keepdim=True)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," This is actually a regularization step that when done, can prevent overfitting. We will revisit why this is so in Exercise 2."]}),"\n",(0,r.jsx)(n.h3,{id:"123-verify-the-created-probability-matrix-is-correct",children:"1.2.3: Verify the created probability matrix is correct"}),"\n",(0,r.jsx)(n.p,{children:"probabilities of [a,b,:] should sum up to 1, [a,c,:] sum up to 1, .."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"trigrams = []\nprob_sum = 0\nfor c1 in chars:\n    for c2 in chars:\n        cur_sum = 0\n        for c3 in chars:\n            prob = third_prob_matrix[stoi[c1], stoi[c2], stoi[c3]].item()\n            prob_sum += prob\n            cur_sum += prob\n        assert 1 - cur_sum < 0.001\n        # ^ this should sum up to 1\n\nassert prob_sum - 27 * 27 < 0.001\n# prob sum should sum up to 27*27 = 729\n# this is because [a,b,:] should sum up to 1, [a,c,:] sum up to 1, ..\n"})}),"\n",(0,r.jsx)(n.h3,{id:"124-generate-words",children:"1.2.4: Generate words"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'for _ in range(10):\n    first = 0\n    second = torch.multinomial(second_prob_matrix, replacement=True, generator=g, num_samples=1).item()\n    third = None\n    generated_sequence = [first, second]\n    while third != 0:\n        third = torch.multinomial(third_prob_matrix[first, second], replacement=True, generator=g, num_samples=1).item()\n        generated_sequence.append(third)\n        first = second\n        second = third\n\n    generated_word = "".join(chars[i] for i in generated_sequence)\n    print(generated_word)\n \n """\n output:\n .aylandia.\n.hvtonnaleine.\n.pieliyya.\n.jeah.\n.drielana.\n.anishlia.\n.el.\n.cy.\n.lanicolie.\n.den.\n """\n'})}),"\n",(0,r.jsx)(n.p,{children:"how to verify the generated word is indeed coming from the trigram probability:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["replace the multinomial sampling to:\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.code,{children:"third = torch.multinomial(torch.ones(27)/27.0, replacement=True, generator=g, num_samples=1).item()"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"and then check the results. You can see that the words changed from pronounceable to non-pronounceable and quite random."}),"\n"]}),"\n",(0,r.jsx)(n.h1,{id:"exercise-2-use-neural-network-to-complete-the-same-above",children:"Exercise 2: Use Neural Network to complete the same above."}),"\n",(0,r.jsxs)(n.p,{children:["In this exercise, instead of generating ",(0,r.jsx)(n.code,{children:"third_prob_matrix"})," using the probability of the counts, we generate the ",(0,r.jsx)(n.code,{children:"third_prob_matrix"})," by estimating all of the parameters using gradient descent."]}),"\n",(0,r.jsxs)(n.p,{children:["i.e., we estimate the probability of ",(0,r.jsx)(n.code,{children:'given \u201cab\u201d, the third character is "c"'})," using gradient descent, instead of the observed probability from the data."]}),"\n",(0,r.jsxs)(n.p,{children:["Because this is ",(0,r.jsx)(n.code,{children:"ab"}),", ",(0,r.jsx)(n.code,{children:"ac"})," , \u2026 ",(0,r.jsx)(n.code,{children:"zy"}),", ",(0,r.jsx)(n.code,{children:"zz"})," , .. ",(0,r.jsx)(n.code,{children:".a"})," , .. 27_27 types of occurrance of first 2 chars, we construct a 1 layer neural network with 27_27 = 729 neurons."]}),"\n",(0,r.jsxs)(n.p,{children:["Let\u2019s use 1 input word ",(0,r.jsx)(n.code,{children:"emma"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:".em"})," , ",(0,r.jsx)(n.code,{children:"emm"}),", ",(0,r.jsx)(n.code,{children:"mma"}),", ",(0,r.jsx)(n.code,{children:"ma."})," are the 4 trigrams,"]}),"\n",(0,r.jsxs)(n.li,{children:["seperating them to x and y, then x = [ ",(0,r.jsx)(n.code,{children:".e"})," , ",(0,r.jsx)(n.code,{children:"em"}),", ",(0,r.jsx)(n.code,{children:"mm"}),", ",(0,r.jsx)(n.code,{children:"ma"})," ], y = [ ",(0,r.jsx)(n.code,{children:"m"}),", ",(0,r.jsx)(n.code,{children:"m"}),", ",(0,r.jsx)(n.code,{children:"a"}),", ",(0,r.jsx)(n.code,{children:"."})," ] )"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Use ",(0,r.jsx)(n.code,{children:"emma"})," as an example, here\u2019s how the neural network look like:"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://prod-files-secure.s3.us-west-2.amazonaws.com/453ca8e2-cee0-4cc0-94b7-2515fce2cbff/fd57270b-8d45-4013-830b-4eb13526184b/Untitled.png",alt:"Untitled"})}),"\n",(0,r.jsx)(n.p,{children:"In the above function, how do we estimate how fit the parameters in W are:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"We use the Maximum Likelihood Estimation, i.e., we maximum the probability that our output matches [m, m, a, .]"}),"\n",(0,r.jsxs)(n.li,{children:["To do that, we want the probability of ",(0,r.jsx)(n.code,{children:"e^xi/sum(e^xi) for all xi in each row"})," for all 4 rows multiplied to be the highest,\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"e.g., p(first row = m) = 0.3, p(second row = m) = 0.5, p(third row = a) = 0.1, p(4th row = .) = 0.9, then all prob = 0.3 * 0.5 * 0.1 * 0.9. We want this prob to be the highest possible."}),"\n",(0,r.jsxs)(n.li,{children:["that means having the log likelihood to be highest possible\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["i.e., ",(0,r.jsx)(n.code,{children:"log(0.3) + log(0.5) + log(0.1) + log(0.9)"})," to be highest possible."]}),"\n",(0,r.jsx)(n.li,{children:"which means for the negative log likelihood to be lowest possible."}),"\n",(0,r.jsxs)(n.li,{children:["i.e., ",(0,r.jsx)(n.code,{children:"- (log(0.3) + log(0.5) + log(0.1) + log(0.9)) / 4"})," to be lowest possible. Note getting the mean is the convention of how this is done."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["this ",(0,r.jsx)(n.code,{children:"- (log(0.3) + log(0.5) + log(0.1) + log(0.9))/4"})," negative log likelihood is called the ",(0,r.jsx)(n.strong,{children:"loss function."})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Based on the above, let\u2019s write down how to use gradient descent to estimate the W."}),"\n",(0,r.jsx)(n.h3,{id:"21-step-1-prepare-x-and-y",children:"2.1 Step 1: Prepare X and y"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn.functional as F\n\nnames = open(\"makemore-master/names.txt\", 'r').read().splitlines()\nchars = '.abcdefghijklmnopqrstuvwxyz'\nstoi_27 = {c: i for i, c in enumerate(chars)}\nitos_729 = []\nstoi_729 = {}\ni = 0\nfor c1 in chars:\n    for c2 in chars:\n        i += 1\n        stoi_729[(c1, c2)] = i\n        itos_729.append((c1, c2,))\n\n# names = ['emma']  # starts with one word only\nX = []\ny = []\nfor name in names:\n    name = '.' + name + '.'\n    for c1, c2, c3 in zip(name, name[1:], name[2:]):\n        X.append(stoi_729[(c1, c2)])\n        y.append(stoi_27[c3])\n\nX = torch.tensor(X)\nX = F.one_hot(X, num_classes=730).float() \n# 27 * 27 = 729, but seems like torch needs the num_classes to be larger than the actual classes\ny = torch.tensor(y, dtype=torch.int)\n\nprint(X.shape, y.shape)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"22-step-2-model-training",children:"2.2. Step 2: Model training"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"W = torch.randn([730, 27], requires_grad=True)\n\nfor _ in range(1000):\n    logits = X @ W\n    counts = torch.exp(logits)\n    prob = counts / counts.sum(dim=1, keepdim=True)\n    prob = prob[torch.arange(X.size(0)), y]\n    loss = - torch.log(prob).mean()  # this is negative log likelihood loss\n    loss.backward()\n    print(loss)\n    step_size = 0.01\n    W.data = W.data - step_size * W.grad\n"})}),"\n",(0,r.jsx)(n.p,{children:"Last few lines of the printed loss looks like"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"tensor(2.7786, grad_fn=<NegBackward0>)\ntensor(2.7780, grad_fn=<NegBackward0>)\ntensor(2.7773, grad_fn=<NegBackward0>)\ntensor(2.7766, grad_fn=<NegBackward0>)\ntensor(2.7760, grad_fn=<NegBackward0>)\ntensor(2.7754, grad_fn=<NegBackward0>)\ntensor(2.7747, grad_fn=<NegBackward0>)\ntensor(2.7741, grad_fn=<NegBackward0>)\ntensor(2.7735, grad_fn=<NegBackward0>)\ntensor(2.7729, grad_fn=<NegBackward0>)\ntensor(2.7723, grad_fn=<NegBackward0>)\ntensor(2.7717, grad_fn=<NegBackward0>)\ntensor(2.7712, grad_fn=<NegBackward0>)\ntensor(2.7706, grad_fn=<NegBackward0>)\ntensor(2.7701, grad_fn=<NegBackward0>)\ntensor(2.7696, grad_fn=<NegBackward0>)\ntensor(2.7690, grad_fn=<NegBackward0>)\ntensor(2.7686, grad_fn=<NegBackward0>)\ntensor(2.7681, grad_fn=<NegBackward0>)\ntensor(2.7676, grad_fn=<NegBackward0>)\ntensor(2.7672, grad_fn=<NegBackward0>)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"23-step-3-verify-from-the-loss-in-step-1",children:"2.3 Step 3: Verify from the loss in step 1."}),"\n",(0,r.jsx)(n.p,{children:"The estimate from Step 1 is an analytical estimate. our new estimate should be very close to the analytical estimate."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'neg_log_likelihood = []\nfor name in names:\n    for c1, c2, c3 in zip(name, name[1:], name[2:]):\n        prob = third_prob_matrix[stoi[c1], stoi[c2], stoi[c3]]\n        neg_log_likelihood.append(- torch.log(prob))\nneg_log_likelihood = torch.tensor(neg_log_likelihood).mean()\n\nprint(neg_log_likelihood) \n"""\noutput:\ntensor(2.3313)\n"""\n'})}),"\n",(0,r.jsx)(n.h3,{id:"24-step-4-generate-names-based-on-the-probabilities",children:"2.4 Step 4: Generate names based on the probabilities"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'g = torch.Generator().manual_seed(123)\n\nfor _ in range(10):\n    first = 0\n    second = torch.multinomial(second_prob_matrix, replacement=True, generator=g, num_samples=1).item()\n    third = None\n    generated_sequence = [first, second]\n    while third != 0:\n        idx = stoi_729[(chars[first], chars[second])]\n        logits = W[idx]\n        counts = torch.exp(logits)\n        probs = counts/counts.sum()\n        third = torch.multinomial(probs, replacement=True, generator=g, num_samples=1).item()\n        generated_sequence.append(third)\n        first = second\n        second = third\n\n    generated_word = "".join(chars[i] for i in generated_sequence)\n    print(generated_word)\n\n"""\noutput:\n.ael.\n.ddisah.\n.taslmpkzyejpruouyra.\n.jabwourreyah.\n.tafwsxlfwper.\n.cymon.\n.zznpwm.\n.dapv.\n.caridgyiujge.\n.sanna.\n"""\n'})}),"\n",(0,r.jsx)(n.h3,{id:"25-step-5-smoothingregularization",children:"2.5 Step 5: Smoothing/Regularization"}),"\n",(0,r.jsxs)(n.p,{children:["Similar to when we did ",(0,r.jsx)(n.code,{children:"N = N + 1"})," in 1.2.2, we can do the same here to prevent certain parameter have very high negative log likelihood, which could cause overfit and affect the loss calculation."]}),"\n",(0,r.jsx)(n.p,{children:"Know that when a parameter in |W| is very extreme (e.g., 10, -10), it will then cause the negative log likelihood to be very high or very low, and this affects how we calculate the loss function and how we run gradient descent, and also caused the parameters we get to overfit to the training data."}),"\n",(0,r.jsx)(n.p,{children:"To run smoothing, on top of the step 2 model training,"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"W = torch.randn([730, 27], requires_grad=True)\n\nfor _ in range(1000):\n    logits = X @ W\n    counts = torch.exp(logits)\n    prob = counts / counts.sum(dim=1, keepdim=True)\n    prob = prob[torch.arange(X.size(0)), y]\n    loss = - torch.log(prob).mean()  # this is negative log likelihood loss\n    loss.backward()\n    print(loss)\n    step_size = 0.01\n    W.data = W.data - step_size * W.grad\n"})}),"\n",(0,r.jsxs)(n.p,{children:["we can change the function ",(0,r.jsx)(n.code,{children:"loss = - torch.log(prob).mean()"})]}),"\n",(0,r.jsx)(n.p,{children:"to be"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"loss = - torch.log(prob).mean() + 0.01 * (W**2).mean() \n"})}),"\n",(0,r.jsx)(n.p,{children:"essentially, this means the higher the absolute value of W is, the higher the loss. This adds a penalty when the the absolute value of W is too high."}),"\n",(0,r.jsx)(n.h1,{id:"appendix",children:"Appendix:"}),"\n",(0,r.jsx)(n.h2,{id:"appendix-1-broadcasting-rules",children:"Appendix 1. Broadcasting rules"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.code,{children:"Broadcasting rule: Two tensors are \u201cbroadcastable\u201d if the following rules hold:- Each tensor has at least one dimension.- When iterating over the dimension sizes, star ting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."})}),"\n",(0,r.jsx)(n.h3,{id:"trailing-dimension",children:"Trailing Dimension"}),"\n",(0,r.jsx)(n.p,{children:"The trailing dimension of an array is the last dimension in its shape. For instance:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["For a 1D array with shape\xa0",(0,r.jsx)(n.code,{children:"(5,)"}),", the trailing dimension is\xa0",(0,r.jsx)(n.code,{children:"5"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["For a 2D array with shape\xa0",(0,r.jsx)(n.code,{children:"(3, 4)"}),", the trailing dimension is\xa0",(0,r.jsx)(n.code,{children:"4"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["For a 3D array with shape\xa0",(0,r.jsx)(n.code,{children:"(2, 3, 4)"}),", the trailing dimension is\xa0",(0,r.jsx)(n.code,{children:"4"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"broadcasting",children:"Broadcasting"}),"\n",(0,r.jsx)(n.p,{children:'Broadcasting is the process of making arrays with different shapes compatible for arithmetic operations. It involves "stretching" the smaller array across the larger array so that they have compatible shapes.'}),"\n",(0,r.jsx)(n.h3,{id:"rules-for-broadcasting",children:"Rules for Broadcasting"}),"\n",(0,r.jsx)(n.p,{children:"The rules for broadcasting are:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Same Size"}),": The dimensions are equal., OR"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size of One"}),": One of the dimensions is 1., OR"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Nonexistent Dimension"}),": The dimension is nonexistent in the smaller array.,"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"the-statement-explained",children:"The Statement Explained"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:'"When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."'}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This means that, when aligning the dimensions of two arrays for broadcasting, you start comparing from the last dimension (the trailing dimension) and move towards the first dimension. For each pair of dimensions you compare:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"If they are the same size, they are compatible."}),"\n",(0,r.jsx)(n.li,{children:"If one of them is 1, it can be stretched to match the other size."}),"\n",(0,r.jsx)(n.li,{children:"If one of them does not exist (the smaller array has fewer dimensions), it is treated as if it has a size of 1 in that dimension."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(n.h3,{id:"example-1-compatible-arrays",children:"Example 1: Compatible Arrays"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Array A: Shape\xa0",(0,r.jsx)(n.code,{children:"(4, 3, 2)"})]}),"\n",(0,r.jsxs)(n.li,{children:["Array B: Shape\xa0",(0,r.jsx)(n.code,{children:"(3, 1)"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Step-by-step broadcasting:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trailing dimensions"}),"\xa0(last dimension):\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: 2"}),"\n",(0,r.jsx)(n.li,{children:"B: 1 (B can be stretched to size 2)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Next dimensions"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: 3"}),"\n",(0,r.jsx)(n.li,{children:"B: 3 (Same size, compatible)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Next dimensions"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: 4"}),"\n",(0,r.jsx)(n.li,{children:"B: Does not exist (Treated as 1, B can be stretched to size 4)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Result: B can be broadcasted to shape\xa0",(0,r.jsx)(n.code,{children:"(4, 3, 2)"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"example-2-incompatible-arrays",children:"Example 2: Incompatible Arrays"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Array A: Shape\xa0",(0,r.jsx)(n.code,{children:"(3, 2)"})]}),"\n",(0,r.jsxs)(n.li,{children:["Array B: Shape\xa0",(0,r.jsx)(n.code,{children:"(4, 3)"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Step-by-step broadcasting:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trailing dimensions"}),"\xa0(last dimension):\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: 2"}),"\n",(0,r.jsx)(n.li,{children:"B: 3 (Different sizes and neither is 1, not compatible)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Result: A and B cannot be broadcasted together."}),"\n",(0,r.jsx)(n.h3,{id:"visual-example",children:"Visual Example"}),"\n",(0,r.jsx)(n.p,{children:"Consider arrays:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Array A: Shape\xa0",(0,r.jsx)(n.code,{children:"(4, 1, 3)"})]}),"\n",(0,r.jsxs)(n.li,{children:["Array B: Shape\xa0",(0,r.jsx)(n.code,{children:"(1, 5, 3)"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Broadcasting steps:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trailing dimensions"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: 3"}),"\n",(0,r.jsx)(n.li,{children:"B: 3 (Same size, compatible)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Next dimensions"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: 1"}),"\n",(0,r.jsx)(n.li,{children:"B: 5 (A can be stretched to size 5)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Next dimensions"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A: 4"}),"\n",(0,r.jsx)(n.li,{children:"B: 1 (B can be stretched to size 4)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Final shapes after broadcasting:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Array A: Shape\xa0",(0,r.jsx)(n.code,{children:"(4, 5, 3)"})]}),"\n",(0,r.jsxs)(n.li,{children:["Array B: Shape\xa0",(0,r.jsx)(n.code,{children:"(4, 5, 3)"})]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"appendix-2-torchgenerator",children:"Appendix 2. Torch.Generator"}),"\n",(0,r.jsxs)(n.p,{children:["The line\xa0",(0,r.jsx)(n.code,{children:"g = torch.Generator().manual_seed(123)"}),"\xa0is used to create a random number generator with a specific seed. Here's what it does, why you might need it, and the differences if you don't use it:"]}),"\n",(0,r.jsx)(n.h3,{id:"what-it-does",children:"What It Does"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Creates a Random Number Generator"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"torch.Generator()"}),"\xa0creates a random number generator object. This generator can be used to control the random number generation process in PyTorch."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sets a Manual Seed"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"manual_seed(123)"}),"\xa0sets the seed for the random number generator to\xa0",(0,r.jsx)(n.code,{children:"123"}),". A seed is a starting point for the sequence of random numbers. By setting the seed, you ensure that the sequence of random numbers is reproducible."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"why-you-need-this",children:"Why You Need This"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reproducibility"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Setting a seed ensures that the results of your experiments are reproducible. This means that if you run the same code multiple times, you will get the same results each time. This is crucial for debugging, comparing models, and ensuring that your results are reliable."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Consistency Across Runs"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"When you are developing and testing machine learning models, you often need to ensure that your results are consistent across different runs. Setting a seed helps in achieving this consistency."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Experiment Tracking"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"In research and production environments, being able to reproduce the results is essential for tracking the progress of experiments and verifying results."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"difference-if-not-using-this",children:"Difference If Not Using This"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Non-Reproducible Results"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Without setting a seed, the random number generator will produce different sequences of random numbers each time you run your code. This can lead to variations in the results of your experiments or model training runs, making it hard to reproduce your results."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Difficulty in Debugging"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"If you encounter an issue in your code and you need to debug it, having reproducible results is extremely helpful. Without a set seed, the randomness can make it difficult to identify the root cause of issues."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Inconsistent Model Performance"}),":\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"In machine learning, the initial weights of a model, the order of data shuffling, and other stochastic processes can affect the performance of your model. Without a set seed, these factors can lead to different performance metrics across runs, making it harder to compare different models or hyperparameter settings."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,r.jsxs)(n.p,{children:["Here's an example to illustrate the use of\xa0",(0,r.jsx)(n.code,{children:"torch.Generator"}),"\xa0with a manual seed:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\n\n# Create a generator with a manual seed\ng = torch.Generator().manual_seed(123)\n\n# Generate a random tensor with the generator\nrandom_tensor = torch.randn(3, 3, generator=g)\nprint(random_tensor)\n\n# Generate another random tensor with the same generator\nrandom_tensor_2 = torch.randn(3, 3, generator=g)\nprint(random_tensor_2)\n\n# Reset the seed and generate the tensor again\ng.manual_seed(123)\nrandom_tensor_3 = torch.randn(3, 3, generator=g)\nprint(random_tensor_3)\n\n"""\noutput:\ntensor([[-0.1115,  0.1204, -0.3696],\n        [-0.2404, -1.1969,  0.2093],\n        [-0.9724, -0.7550,  0.3239]])\ntensor([[-0.1085,  0.2103, -0.3908],\n        [ 0.2350,  0.6653,  0.3528],\n        [ 0.9728, -0.0386, -0.8861]])\ntensor([[-0.1115,  0.1204, -0.3696],\n        [-0.2404, -1.1969,  0.2093],\n        [-0.9724, -0.7550,  0.3239]])\n"""\n'})}),"\n",(0,r.jsx)(n.p,{children:"In this example:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The first tensor (",(0,r.jsx)(n.code,{children:"random_tensor"}),") is generated using the generator with the seed set to\xa0",(0,r.jsx)(n.code,{children:"123"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["The second tensor (",(0,r.jsx)(n.code,{children:"random_tensor_2"}),") is generated using the same generator, continuing the random sequence."]}),"\n",(0,r.jsxs)(n.li,{children:["After resetting the seed to\xa0",(0,r.jsx)(n.code,{children:"123"}),", generating the tensor again (",(0,r.jsx)(n.code,{children:"random_tensor_3"}),") produces the same values as\xa0",(0,r.jsx)(n.code,{children:"random_tensor"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"summary",children:"Summary"}),"\n",(0,r.jsxs)(n.p,{children:["Using\xa0",(0,r.jsx)(n.code,{children:"torch.Generator(manual_seed=123)"}),"\xa0is crucial for ensuring reproducibility and consistency in your experiments. Without setting a seed, the random processes in your code can lead to non-reproducible and inconsistent results, which can complicate debugging and result verification."]}),"\n",(0,r.jsx)(n.h2,{id:"appendix-3-indexing-multiple-items-in-torch",children:"Appendix 3: Indexing multiple items in torch"}),"\n",(0,r.jsx)(n.h3,{id:"matrixr1-r2-r3-c1-c2-c3",children:(0,r.jsx)(n.code,{children:"matrix[[r1, r2, r3], [c1, c2, c3]]"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\nm = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])\n\n"""\nsuppose I want to get 3, 6, if I first write 3\'s index, then 6\'s index: \n"""\nelements = m[[0,2], [1,2]]\n"""\nthis will show an error, this is not possible:\nIndexError: index 2 is out of bounds for dimension 0 with size 2\n"""\n\n"""\nI will have to use the row index, then column index, \ni.e., matrix[[r1, r2, r3], [c1, c2, c3]]\ni.e., [0,1], [2,2] \n"""\nelements = m[[0,1], [2,2]]\nprint(elements)\n"""\noutput:\ntensor([3, 6])\n"""\n\n'})}),"\n",(0,r.jsxs)(n.h2,{id:"appendix-4-dim",children:["Appendix 4: ",(0,r.jsx)(n.code,{children:"dim"})]}),"\n",(0,r.jsx)(n.h3,{id:"key-idea",children:"Key Idea:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["The\xa0",(0,r.jsx)(n.code,{children:"dim"}),'\xa0argument specifies the axis that you\xa0"reduce"\xa0or\xa0"operate on".']})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["When you use operations like\xa0",(0,r.jsx)(n.code,{children:"sum"}),",\xa0",(0,r.jsx)(n.code,{children:"mean"}),",\xa0",(0,r.jsx)(n.code,{children:"max"}),", or\xa0",(0,r.jsx)(n.code,{children:"argmax"}),","]}),"\xa0specifying\xa0",(0,r.jsx)(n.code,{children:"dim=0"}),"\xa0tells PyTorch to reduce or operate along rows (axis 0), which means you collapse the rows and perform the operation across columns."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["When you use\xa0",(0,r.jsx)(n.code,{children:"dim=1"}),","]}),"\xa0you're telling PyTorch to operate along columns (axis 1), meaning you collapse the columns and perform the operation across rows."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"i.e.:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"if dim=0, shape=(2,5) \u4f1a\u53d8\u6210 shape=(5), \u4e5f\u5c31\u662fdim 0\u6d88\u5931\u4e86"}),"\n",(0,r.jsx)(n.li,{children:"if dim=1, shape=(2,5) \u4f1a\u53d8\u6210 shape=(2)\uff0c\u4e5f\u5c31\u662fdim 1\u6d88\u5931\u4e86"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"dim=-1:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["In PyTorch, specifying\xa0",(0,r.jsx)(n.code,{children:"dim=-1"}),"\xa0refers to the\xa0",(0,r.jsx)(n.strong,{children:"last"}),"\xa0axis of the tensor, regardless of its dimensionality. This is a convenient way to operate on the last dimension without explicitly knowing its index, which can be useful when working with tensors of varying shapes."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'x = torch.arange(10).view(2,5)\nprint(x)\n"""\ntensor([[0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9]])\n"""\nprint(x.sum(dim=0))\n"""\ntensor([ 5,  7,  9, 11, 13])\n"""\nprint(x.sum(dim=1))\n"""\ntensor([10, 35])\n"""\nprint(x.sum(dim=-1))\n"""\ntensor([10, 35])\n"""\n'})}),"\n",(0,r.jsx)(n.h2,{id:"appendix-5-torchcat",children:"Appendix 5: torch.cat"}),"\n",(0,r.jsxs)(n.p,{children:["In PyTorch, you can use\xa0",(0,r.jsx)(n.code,{children:"torch.cat"}),"\xa0to concatenate tensors along a specified dimension. Here\u2019s how you can do this with multi-dimensional tensors."]}),"\n",(0,r.jsx)(n.h3,{id:"example-1-concatenating-2d-tensors",children:"Example 1: Concatenating 2D Tensors"}),"\n",(0,r.jsx)(n.p,{children:"Let's start with a simple example of concatenating two 2D tensors along different dimensions."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'pythonCopy code\nimport torch\n\n# Create two 2D tensors\ntensor1 = torch.tensor([[1, 2, 3],\n                        [4, 5, 6]])\n\ntensor2 = torch.tensor([[7, 8, 9],\n                        [10, 11, 12]])\n\nprint("Tensor 1:")\nprint(tensor1)\n\nprint("Tensor 2:")\nprint(tensor2)\n\n'})}),"\n",(0,r.jsxs)(n.h3,{id:"concatenate-alongdim0rows---means-expanding-on-number-of-rows",children:[(0,r.jsxs)(n.strong,{children:["Concatenate along\xa0",(0,r.jsx)(n.code,{children:"dim=0"})]}),"\xa0(rows): - means expanding on number of rows."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'pythonCopy code\nresult_dim0 = torch.cat((tensor1, tensor2), dim=0)\nprint("Concatenate along dim=0:")\nprint(result_dim0)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"plaintextCopy code\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]])\n\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explanation"}),": Concatenating along\xa0",(0,r.jsx)(n.code,{children:"dim=0"}),"\xa0adds the rows of\xa0",(0,r.jsx)(n.code,{children:"tensor2"}),"\xa0below the rows of\xa0",(0,r.jsx)(n.code,{children:"tensor1"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.h3,{id:"concatenate-alongdim1columns",children:[(0,r.jsxs)(n.strong,{children:["Concatenate along\xa0",(0,r.jsx)(n.code,{children:"dim=1"})]}),"\xa0(columns):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'pythonCopy code\nresult_dim1 = torch.cat((tensor1, tensor2), dim=1)\nprint("Concatenate along dim=1:")\nprint(result_dim1)\n\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Output:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"plaintextCopy code\ntensor([[ 1,  2,  3,  7,  8,  9],\n        [ 4,  5,  6, 10, 11, 12]])\n\n"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explanation"}),": Concatenating along\xa0",(0,r.jsx)(n.code,{children:"dim=1"}),"\xa0adds the columns of\xa0",(0,r.jsx)(n.code,{children:"tensor2"}),"\xa0to the right of the columns of\xa0",(0,r.jsx)(n.code,{children:"tensor1"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);