"use strict";(globalThis.webpackChunknotes=globalThis.webpackChunknotes||[]).push([[6758],{28453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>s});var a=t(96540);const o={},r=a.createContext(o);function i(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:i(n.components),a.createElement(r.Provider,{value:e},n.children)}},80340:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"ML General/PyTorch - Convolutional Neural Network (CNN)","title":"PyTorch - Convolutional Neural Network (CNN)","description":"Definition:","source":"@site/docs/05. ML General/60.PyTorch - Convolutional Neural Network (CNN).md","sourceDirName":"05. ML General","slug":"/p/06ca38e7-caea-4560-a877-a72fa1e06183","permalink":"/notes/docs/p/06ca38e7-caea-4560-a877-a72fa1e06183","draft":false,"unlisted":false,"editUrl":"https://github.com/emmableu/notes/edit/main/docs/05. ML General/60.PyTorch - Convolutional Neural Network (CNN).md","tags":[],"version":"current","sidebarPosition":60,"frontMatter":{"created_at":"2025-11-02","page_link":"/p/06ca38e7-caea-4560-a877-a72fa1e06183","slug":"/p/06ca38e7-caea-4560-a877-a72fa1e06183"},"sidebar":"tutorialSidebar","previous":{"title":"Dropout","permalink":"/notes/docs/p/1c3d3e83-cba5-490e-adac-bbffd4d7c2c9"},"next":{"title":"Recurrent Neural Network - RNN","permalink":"/notes/docs/p/fca7bec5-0756-4298-ac4c-46b13422b565"}}');var o=t(74848),r=t(28453);const i={created_at:"2025-11-02",page_link:"/p/06ca38e7-caea-4560-a877-a72fa1e06183",slug:"/p/06ca38e7-caea-4560-a877-a72fa1e06183"},s=void 0,l={},c=[{value:"Definition:",id:"definition",level:3},{value:"Goal of Convolutional Neural Networks:",id:"goal-of-convolutional-neural-networks",level:2},{value:"\u89e3\u91ca\u5377\u79ef\u64cd\u4f5c\u4e2d\u7684\u7a00\u758f\u4ea4\u4e92\u548c\u53c2\u6570\u5171\u4eab\u53ca\u5176\u4f5c\u7528\u3002 223 \u2605\u2605\u2606\u2606\u2606",id:"\u89e3\u91ca\u5377\u79ef\u64cd\u4f5c\u4e2d\u7684\u7a00\u758f\u4ea4\u4e92\u548c\u53c2\u6570\u5171\u4eab\u53ca\u5176\u4f5c\u7528-223-",level:2},{value:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc CNN \u5982\u4f55\u7528\u4e8e\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff1f 227 \u2605\u2605\u2605\u2606\u2606",id:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc-cnn-\u5982\u4f55\u7528\u4e8e\u6587\u672c\u5206\u7c7b\u4efb\u52a1-227-",level:2},{value:"\u5e38\u7528\u7684\u6c60\u5316 pooling \u64cd\u4f5c\u6709\u54ea\u4e9b\uff1f\u6c60\u5316\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f 225 \u2605\u2605\u2605\u2606\u2606",id:"\u5e38\u7528\u7684\u6c60\u5316-pooling-\u64cd\u4f5c\u6709\u54ea\u4e9b\u6c60\u5316\u7684\u4f5c\u7528\u662f\u4ec0\u4e48-225-",level:2},{value:"Procedure of CNN",id:"procedure-of-cnn",level:2},{value:"1. Filter (<code>nn.Conv2d</code>)",id:"1-filter-nnconv2d",level:3},{value:"2. ReLU Acrivation Function",id:"2-relu-acrivation-function",level:3},{value:"3. MaxPooling:",id:"3-maxpooling",level:3},{value:"4. use the results from Max Pooling as the input nodes to a Neural Network",id:"4-use-the-results-from-max-pooling-as-the-input-nodes-to-a-neural-network",level:3},{value:"PyTorch Code For CNN - MNIST DataSet",id:"pytorch-code-for-cnn---mnist-dataset",level:2},{value:"import MNIST \u624b\u5199\u6570\u636e",id:"import-mnist-\u624b\u5199\u6570\u636e",level:3},{value:"CNN \u6a21\u578b",id:"cnn-\u6a21\u578b",level:3},{value:"\u8bad\u7ec3",id:"\u8bad\u7ec3",level:3}];function d(n){const e={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h3,{id:"definition",children:"Definition:"}),"\n",(0,o.jsx)(e.p,{children:"A neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:\n- convolutional layers\n- pooling layers\n- dense layers\n- Convolutional neural networks have had great success in certain kinds of problems, such as image recognition."}),"\n",(0,o.jsx)(e.h2,{id:"goal-of-convolutional-neural-networks",children:"Goal of Convolutional Neural Networks:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Reduce the number of input nodes (\u56fe\u7247\u6709\u5f88\u591a\u4e2apixel\uff0c\u6240\u4ee5\u76f4\u63a5\u53d8\u6210\u4e00\u4e2a\u6570\u7ec4\u4f1a\u5f88\u5927\uff0c\u8981\u60f3\u529e\u6cd5\u628a\u5b83\u53d8\u5c0f)\uff1b"}),"\n",(0,o.jsx)(e.li,{children:"Tolerate small shifts in where the pixels are in the image (\u56fe\u7247\u30100010\u3011 \u548c \u56fe\u7247 \u30100100\u3011\u901a\u5e38\u662f\u8868\u793a\u4e00\u6837\u7684\u4e1c\u897f\uff09"}),"\n",(0,o.jsx)(e.li,{children:"Take advantage of the correlation we observe in complex images: \uff08\u6bd4\u5982\u4e00\u4e2a\u718a\u7684\u7167\u7247\uff0c\u68d5\u8272\u70b9\u7684\u65c1\u8fb9\u5f80\u5f80\u4e5f\u662f\u68d5\u8272\u7684\u70b9\uff09"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"\u89e3\u91ca\u5377\u79ef\u64cd\u4f5c\u4e2d\u7684\u7a00\u758f\u4ea4\u4e92\u548c\u53c2\u6570\u5171\u4eab\u53ca\u5176\u4f5c\u7528-223-",children:"\u89e3\u91ca\u5377\u79ef\u64cd\u4f5c\u4e2d\u7684\u7a00\u758f\u4ea4\u4e92\u548c\u53c2\u6570\u5171\u4eab\u53ca\u5176\u4f5c\u7528\u3002 223 \u2605\u2605\u2606\u2606\u2606"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220045822.png",alt:""}),"\n",(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220046942.png",alt:""}),"\n",(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220046455.png",alt:""})]}),"\n",(0,o.jsx)(e.h2,{id:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc-cnn-\u5982\u4f55\u7528\u4e8e\u6587\u672c\u5206\u7c7b\u4efb\u52a1-227-",children:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc CNN \u5982\u4f55\u7528\u4e8e\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff1f 227 \u2605\u2605\u2605\u2606\u2606"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220049987.png",alt:""}),"\n",(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220050458.png",alt:""})]}),"\n",(0,o.jsx)(e.h2,{id:"\u5e38\u7528\u7684\u6c60\u5316-pooling-\u64cd\u4f5c\u6709\u54ea\u4e9b\u6c60\u5316\u7684\u4f5c\u7528\u662f\u4ec0\u4e48-225-",children:"\u5e38\u7528\u7684\u6c60\u5316 pooling \u64cd\u4f5c\u6709\u54ea\u4e9b\uff1f\u6c60\u5316\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f 225 \u2605\u2605\u2605\u2606\u2606"}),"\n",(0,o.jsx)(e.p,{children:"\u5e73\u79fb\uff08translation\uff09\u3001\u65cb\u8f6c\uff08rotation\uff09\u3001\u7f29\u653e\uff08scaling\uff09"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220048800.png",alt:""}),"\n",(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202209220049614.png",alt:""})]}),"\n",(0,o.jsx)(e.h2,{id:"procedure-of-cnn",children:"Procedure of CNN"}),"\n",(0,o.jsx)(e.p,{children:"\u4ee5 \u533a\u5206 \u30100\u3011\u548c\u3010x\u3011\u4e3a\u4f8b\uff1a"}),"\n",(0,o.jsxs)(e.h3,{id:"1-filter-nnconv2d",children:["1. Filter (",(0,o.jsx)(e.code,{children:"nn.Conv2d"}),")"]}),"\n",(0,o.jsxs)(e.p,{children:["\u8ba1\u7b97\u6bcf\u4e00\u90e8\u5206\u7684\u56fe\u50cf\u548c\u4e00\u4e2afilter \uff08aka kernel\uff09\u7684\u70b9\u4e58\u7684\u79ef, \u5e76\u52a0\u4e00\u4e2abias\uff0c \u5f97\u5230\u4e00\u4e2afeature map\n",(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202204181257092.png",alt:""}),"\n**\u600e\u6837\u5f97\u5230filter\u7684matrix:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["filter\u7684matrix\u6700\u7ec8\u662f\u7531",(0,o.jsx)(e.strong,{children:"backpropagation"}),"\u5f97\u5230\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0cstart with random pixel values, then after training with ",(0,o.jsx)(e.strong,{children:"Backpropagation"}),", we end up with something more useful."]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Because each cell in the feature map corresponds to a group of neighboring pixels, the feature map takes advantage of the correlations between neighboring pixels in an image."}),"\n",(0,o.jsx)(e.p,{children:"\u8fd9\u91cc\u7684\u5377\u79ef\u5c42\u5bf9\u5e94\u7684\u53c2\u6570\u662f\uff1a"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:" nn.Conv2d(\n                in_channels=1,    # \u56fe\u7247\u672c\u8eab\u6709\u51e0\u5c42\uff0crgb\u56fe\u7247\u67093\u5c42\uff0c\u672c\u9898\u662f\u9ed1\u767d\u56fe\u7247\uff0c\u6240\u4ee5\u53ea\u67091\u5c42\n                out_channels=1,  # n_filters\uff0c\u8f93\u51fa\u9ad8\u5ea6\uff0c\u4e5f\u5c31\u662ffilter\u7684\u4e2a\u6570\uff0c\n                # \u672c\u9898\u53ea\u6709\u4e00\u4e2afilter\uff0c\u4f46\u662f\u53ef\u4ee5\u6709\u591a\u4e2afilter\uff0c\n                # \u4e0d\u540c\u7684filter\u6700\u540e\u901a\u8fc7back propagation \u8ba1\u7b97\u51fa\u6765\u7684 matrix\u4e5f\u4e0d\u4e00\u6837\u3002\n                kernel_size=3,      # filter \u7684\u8fb9\u957f\uff0c3\u8868\u793a3\u4e2a\u50cf\u7d20\u70b9\n                stride=1,           # filter movement/step\n                padding=0,      # filter\u5f80\u540e\u626b\u7684\u65f6\u5019\u53ef\u80fd\u4f1a\u591a\u51fa\u6765\u4e00\u622a\uff0c\n                # padding\u5c31\u662f\u5728\u56fe\u7247\u7684\u4e0a\uff0c\u4e0b\uff0c\u5de6\uff0c\u53f3\uff0c\u5305\u4e00\u57080\uff0c\u8fd9\u57080\u7684\u5bbd\u5ea6\u5c31\u662fpadding\n                # \u5982\u679c\u60f3\u8981 con2d \u51fa\u6765\u7684\u56fe\u7247\u957f\u5bbd\u6ca1\u6709\u53d8\u5316, \u5982\u679c stride = 1\uff0c\n                # \u5219 padding=(kernel_size-1)/2\uff0c \u4e5f\u5c31\u662f\u8bf4\uff0c\u8fd9\u91cc\u7684\u957f\u5bbd\u5df2\u7ecf\u53d8\u5c0f\u4e86\uff0c\u4f46\u662f\u5982\u679c\n                # \u5e0c\u671b\u5b83\u4e0d\u53d8\u5c0f\uff08\u4fdd\u6301input\u7684\u8fb9\u957f6\uff09\uff0c\u5c31\u8981\u628a\u5b83\u5728\u5916\u9762\u90fd\u5305\u4e00\u5708\u5bbd\u5ea6\u4e3a1\u76840\uff0c\n                # \u628a\u539f\u56fe\u7247\u53d8\u6210\u4e00\u4e2a 8*8 \u7684\u56fe\u7247\u3002\n            ),\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-relu-acrivation-function",children:"2. ReLU Acrivation Function"}),"\n",(0,o.jsxs)(e.p,{children:["\u628afeature map\u4e0a\u6240\u6709<0\u7684\u70b9\u90fd\u53d8\u6210 0\n",(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202204181300836.png",alt:""}),"\n\u7528\u4ee3\u7801\u8868\u793a\u5c31\u662f",(0,o.jsx)(e.code,{children:"nn.ReLU()"})]}),"\n",(0,o.jsx)(e.h3,{id:"3-maxpooling",children:"3. MaxPooling:"}),"\n",(0,o.jsxs)(e.p,{children:["calculate the maximum value of each region on the Feature Map, Post ReLU\n\u548cfilter \u957f\u5f97\u6700\u63a5\u8fd1\u7684region\u7684\u503c\u6700\u540e\u4f1a\u88abmax pooling\u7ed9\u4fdd\u5b58\u4e0b\u6765\n",(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202204181303865.png",alt:""}),"\n\u4ee3\u7801\uff1a",(0,o.jsx)(e.code,{children:"nn.MaxPool2d(kernel_size=2)"}),", \u56e0\u4e3a\u8fd9\u4e2amax polling\u7684kernel \u8fb9\u957f\u662f2"]}),"\n",(0,o.jsx)(e.h3,{id:"4-use-the-results-from-max-pooling-as-the-input-nodes-to-a-neural-network",children:"4. use the results from Max Pooling as the input nodes to a Neural Network"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.img,{src:"https://raw.githubusercontent.com/emmableu/image/master/202204181304906.png",alt:""})}),"\n",(0,o.jsx)(e.h2,{id:"pytorch-code-for-cnn---mnist-dataset",children:"PyTorch Code For CNN - MNIST DataSet"}),"\n",(0,o.jsx)(e.h3,{id:"import-mnist-\u624b\u5199\u6570\u636e",children:"import MNIST \u624b\u5199\u6570\u636e"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.utils.data as Data\nimport torchvision      # \u6570\u636e\u5e93\u6a21\u5757\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(1)    # reproducible\n\n# Hyper Parameters\nEPOCH = 1           # \u8bad\u7ec3\u6574\u6279\u6570\u636e\u591a\u5c11\u6b21, \u4e3a\u4e86\u8282\u7ea6\u65f6\u95f4, \u6211\u4eec\u53ea\u8bad\u7ec3\u4e00\u6b21\nBATCH_SIZE = 50\nLR = 0.001          # \u5b66\u4e60\u7387\nDOWNLOAD_MNIST = True  # \u5982\u679c\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u597d\u4e86mnist\u6570\u636e\u5c31\u5199\u4e0a False\n\n\n# Mnist \u624b\u5199\u6570\u5b57\ntrain_data = torchvision.datasets.MNIST(\n    root='./mnist/',    # \u4fdd\u5b58\u6216\u8005\u63d0\u53d6\u4f4d\u7f6e\n    train=True,  # this is training data\n    transform=torchvision.transforms.ToTensor(),    # \u8f6c\u6362 PIL.Image or numpy.ndarray \u6210\n                                                    # torch.FloatTensor (C x H x W), \u8bad\u7ec3\u7684\u65f6\u5019 normalize \u6210 [0.0, 1.0] \u533a\u95f4\n    download=DOWNLOAD_MNIST,          # \u6ca1\u4e0b\u8f7d\u5c31\u4e0b\u8f7d, \u4e0b\u8f7d\u4e86\u5c31\u4e0d\u7528\u518d\u4e0b\u4e86\n)\n"})}),"\n",(0,o.jsx)(e.p,{children:"\u540c\u6837, \u6211\u4eec\u9664\u4e86\u8bad\u7ec3\u6570\u636e, \u8fd8\u7ed9\u4e00\u4e9b\u6d4b\u8bd5\u6570\u636e, \u6d4b\u8bd5\u770b\u770b\u5b83\u6709\u6ca1\u6709\u8bad\u7ec3\u597d."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"test_data = torchvision.datasets.MNIST(root='./mnist/', train=False)\n\n# \u6279\u8bad\u7ec3 50samples, 1 channel, 28x28 (50, 1, 28, 28)\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n\n# \u4e3a\u4e86\u8282\u7ea6\u65f6\u95f4, \u6211\u4eec\u6d4b\u8bd5\u65f6\u53ea\u6d4b\u8bd5\u524d2000\u4e2a\ntest_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   \n# shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\n# \u4e3a\u4ec0\u4e48\u8981/255: \u538b\u7f29\u52300-1\u7684\u533a\u95f4\ntest_y = test_data.test_labels[:2000]\n"})}),"\n",(0,o.jsx)(e.h3,{id:"cnn-\u6a21\u578b",children:"CNN \u6a21\u578b"}),"\n",(0,o.jsxs)(e.p,{children:["\u548c\u4ee5\u524d\u4e00\u6837, \u6211\u4eec\u7528\u4e00\u4e2a class \u6765\u5efa\u7acb CNN \u6a21\u578b. \u8fd9\u4e2a CNN \u6574\u4f53\u6d41\u7a0b\u662f \u5377\u79ef(",(0,o.jsx)(e.code,{children:"Conv2d"}),") -> \u6fc0\u52b1\u51fd\u6570(",(0,o.jsx)(e.code,{children:"ReLU"}),") -> \u6c60\u5316, \u5411\u4e0b\u91c7\u6837 (",(0,o.jsx)(e.code,{children:"MaxPooling"}),") -> \u518d\u6765\u4e00\u904d -> \u5c55\u5e73\u591a\u7ef4\u7684\u5377\u79ef\u6210\u7684\u7279\u5f81\u56fe -> \u63a5\u5165\u5168\u8fde\u63a5\u5c42 (",(0,o.jsx)(e.code,{children:"Linear"}),") -> \u8f93\u51fa"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Sequential(  # input shape (1, 28, 28), 1 \u4ee3\u8868\u4e00\u4e2adata channel\uff0c\n\t\t\t\t\t\t\t\t    # \u56e0\u4e3a\u56fe\u7247\u53ea\u67091\u5c42 \uff08\u9ed1\u767d\uff09\uff0c\u6240\u4ee5\u662f1\u4e2adata channel\n            nn.Conv2d(\n                in_channels=1,      # input height\n                out_channels=16,    # n_filters\n                kernel_size=5,      # filter size\n                stride=1,           # filter movement/step\n                padding=2,      # \u5982\u679c\u60f3\u8981 con2d \u51fa\u6765\u7684\u56fe\u7247\u957f\u5bbd\u6ca1\u6709\u53d8\u5316, padding=(kernel_size-1)/2 \u5f53 stride=1\n            ),      # output shape (16, 28, 28)\n            nn.ReLU(),    # activation\n            nn.MaxPool2d(kernel_size=2),    # \u5728 2x2 \u7a7a\u95f4\u91cc\u5411\u4e0b\u91c7\u6837, output shape (16, 14, 14)\n        )\n        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)\n            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)\n            nn.ReLU(),  # activation\n            nn.MaxPool2d(2),  # output shape (32, 7, 7)\n        )\n        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)   # (batch, 32, 7, 7)\n        x = x.view(x.size(0), -1)   # \u5c55\u5e73\u591a\u7ef4\u7684\u5377\u79ef\u56fe\u6210 (batch_size, 32 * 7 * 7)\n        output = self.out(x)\n        return output\n\ncnn = CNN()\nprint(cnn)  # net architecture\n"""\nCNN (\n  (conv1): Sequential (\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU ()\n    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  )\n  (conv2): Sequential (\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU ()\n    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  )\n  (out): Linear (1568 -> 10)\n)\n"""\n\n'})}),"\n",(0,o.jsx)(e.h3,{id:"\u8bad\u7ec3",children:"\u8bad\u7ec3"}),"\n",(0,o.jsxs)(e.p,{children:["\u4e0b\u9762\u6211\u4eec\u5f00\u59cb\u8bad\u7ec3, \u5c06 ",(0,o.jsx)(e.code,{children:"x"})," ",(0,o.jsx)(e.code,{children:"y"})," \u90fd\u7528 ",(0,o.jsx)(e.code,{children:"Variable"})," \u5305\u8d77\u6765, \u7136\u540e\u653e\u5165 ",(0,o.jsx)(e.code,{children:"cnn"})," \u4e2d\u8ba1\u7b97 ",(0,o.jsx)(e.code,{children:"output"}),", \u6700\u540e\u518d\u8ba1\u7b97\u8bef\u5dee. \u4e0b\u9762\u4ee3\u7801\u7701\u7565\u4e86\u8ba1\u7b97\u7cbe\u786e\u5ea6 ",(0,o.jsx)(e.code,{children:"accuracy"})," \u7684\u90e8\u5206, \u7ec6\u770b ",(0,o.jsx)(e.code,{children:"accuracy"})," \u89c1",(0,o.jsx)(e.a,{href:"https://github.com/MorvanZhou/Tensorflow-Tutorial/blob/master/tutorial-contents/401_CNN.py",children:"github"})," \u770b\u5168\u90e8\u4ee3\u7801."]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\nloss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted\n\n# training and testing\nfor epoch in range(EPOCH):\n    for step, (b_x, b_y) in enumerate(train_loader):   # \u5206\u914d batch data, normalize x when iterate train_loader\n        output = cnn(b_x)               # cnn output\n        loss = loss_func(output, b_y)   # cross entropy loss\n        optimizer.zero_grad()           # clear gradients for this training step\n        loss.backward()                 # backpropagation, compute gradients\n        optimizer.step()                # apply gradients\n\n"""\n...\nEpoch:  0 | train loss: 0.0306 | test accuracy: 0.97\nEpoch:  0 | train loss: 0.0147 | test accuracy: 0.98\nEpoch:  0 | train loss: 0.0427 | test accuracy: 0.98\nEpoch:  0 | train loss: 0.0078 | test accuracy: 0.98\n"""\n'})}),"\n",(0,o.jsx)(e.p,{children:"\u6700\u540e\u6211\u4eec\u518d\u6765\u53d610\u4e2a\u6570\u636e, \u770b\u770b\u9884\u6d4b\u7684\u503c\u5230\u5e95\u5bf9\u4e0d\u5bf9:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'test_output = cnn(test_x[:10])\npred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\nprint(pred_y, \'prediction number\')\nprint(test_y[:10].numpy(), \'real number\')\n\n"""\n[7 2 1 0 4 1 4 9 5 9] prediction number\n[7 2 1 0 4 1 4 9 5 9] real number\n"""\n'})})]})}function u(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}}}]);