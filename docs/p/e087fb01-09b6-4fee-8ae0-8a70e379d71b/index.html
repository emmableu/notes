<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-ML General/Definition Glossary" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Definition Glossary | My Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://emmableu.github.io/notes/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://emmableu.github.io/notes/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://emmableu.github.io/notes/docs/p/e087fb01-09b6-4fee-8ae0-8a70e379d71b"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Definition Glossary | My Site"><meta data-rh="true" name="description" content="source"><meta data-rh="true" property="og:description" content="source"><link data-rh="true" rel="icon" href="/notes/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://emmableu.github.io/notes/docs/p/e087fb01-09b6-4fee-8ae0-8a70e379d71b"><link data-rh="true" rel="alternate" href="https://emmableu.github.io/notes/docs/p/e087fb01-09b6-4fee-8ae0-8a70e379d71b" hreflang="en"><link data-rh="true" rel="alternate" href="https://emmableu.github.io/notes/docs/p/e087fb01-09b6-4fee-8ae0-8a70e379d71b" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"05. ML General","item":"https://emmableu.github.io/notes/docs/ml-general"},{"@type":"ListItem","position":2,"name":"Definition Glossary","item":"https://emmableu.github.io/notes/docs/p/e087fb01-09b6-4fee-8ae0-8a70e379d71b"}]}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous"><link rel="stylesheet" href="/notes/assets/css/styles.607fdee8.css">
<script src="/notes/assets/js/runtime~main.c02a6fe4.js" defer="defer"></script>
<script src="/notes/assets/js/main.02311e79.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/notes/img/logo.svg"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/notes/"><div class="navbar__logo"><img src="/notes/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/notes/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">My Site</b></a><a class="navbar__item navbar__link" href="/notes/docs/ml100">01. ML Theory 100</a><a class="navbar__item navbar__link" href="/notes/docs/dl100">02. DL Theory 100</a><a class="navbar__item navbar__link" href="/notes/docs/leetcode">03. Leetcode</a><a class="navbar__item navbar__link" href="/notes/docs/zero2hero">04. Zero To Hero</a><a class="navbar__item navbar__link" href="/notes/docs/ml-general">05. ML General</a><a class="navbar__item navbar__link" href="/notes/docs/reinforcement-learning">Reinforcement Learning</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/notes/docs/ml100">01. ML Theory 100</a><button aria-label="Expand sidebar category &#x27;01. ML Theory 100&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/notes/docs/ml-general">05. ML General</a><button aria-label="Collapse sidebar category &#x27;05. ML General&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/cc9e9b72-f558-45fb-b323-8c982cd21785">ML资源</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/6a1a76f4-1db9-459d-a3ff-6d9fb9bd3c8e">Likelihood Basics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/33322e19-551a-47dd-ba82-e1a9d5d30479">Bias and Variance</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/40c27ac6-55a8-4fa7-94a2-9e0ea4b0976d">Classification Metric</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/780d6a08-9052-48b8-be65-8e0aa46e3ffb">Feature Selection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/74afc8cf-1b32-4848-b8ca-8a61fa8f137b">Regularization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/ab96da11-1590-44aa-801a-2848affaa604">Distributions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/e66fc711-ed04-4037-9657-70f91790966b">Mutual Information</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/85996f78-63d2-4da1-b968-385104020c3c">Linear Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/b026b72a-d550-4301-8b1d-c443c79d9e0c">Logistic Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/78d7a94f-b5a8-4b20-a94f-643f772a6ee2">Decision Tree</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/e7292d95-c582-4cb3-85ef-1adc73b3a172">Ensemble Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/2784b330-5a5e-449f-9a0e-27da46d5c017">Naive Bayes and Generative Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/d7874b72-4d46-49c8-9996-4b65598d8ed8">Linear Discrimitive Analysis (LDA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/e4b13962-3bfd-4179-9f05-8389360f76b6">Support Vector Machine (SVM)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/bc2360e2-3cde-44dd-ad53-b5cddfd713e7">Hierarchical Clustering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/c8f4884e-488f-4566-8b4d-690d0e062362">EM Algorithm</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/bdee33c5-59c5-4656-ae24-de6a44a339c6">KNN - K Nearest Neighbor</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/83bf409d-c62d-436f-8814-d1bcd2c739ff">SoftMax and Cross-Entropy Loss Function</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/75f6131c-e9cb-4073-b06a-407ca559b982">Principle Component Analysis - PCA</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/3950ea5b-4dcb-4513-804d-55d0162005c8">Learning to Rank</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/75d0bf4d-fcc2-4e72-b273-4e862d8898f8">GMM - Gaussian Mixed Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/1f533186-531c-44b3-96e0-eaa311ac7ff8">Gradient Boost 细节</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/a5f44b92-03c1-475e-a773-76c53542b1ad">T-SNE or tSNE or tsne</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/5e97f5b9-0ff7-4097-9053-526db6d75222">Convexity, local, global minima</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/9568038f-e88e-4df3-a806-5532f48cef35">Neural Network Intro</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/6a021dfe-ba24-4fd3-8e3d-548e660e5a08">Gradient Descent (Best Explanation)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/af31081f-3337-4f01-9762-7eb2d9c9001d">Backpropagation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/39005b9b-0dc7-477a-b12b-ab99967780db">Multiple Inputs and Outputs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/b3253a65-5a0a-4717-8a41-a7b6dd377834">Loss Functions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/6a1c56e8-94cf-4eff-9f46-a60b6a34f55e">Batch Normalization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/1c3d3e83-cba5-490e-adac-bbffd4d7c2c9">Dropout</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/06ca38e7-caea-4560-a877-a72fa1e06183">PyTorch - Convolutional Neural Network (CNN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/fca7bec5-0756-4298-ac4c-46b13422b565">Recurrent Neural Network - RNN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/cdf7bcb9-c32f-4124-a4c9-f65f76c1a69d">LSTM</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/fbab20b7-b47f-4dcd-a022-b9926965e147">BERT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/016d741d-2e5f-4c42-89e2-3b5732ccaae8">GAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/6b0d6c51-2d88-4110-ba81-9b78d4a2ce9c">ResNet 残差网络</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/2e942169-193e-4a5a-838f-61ff2df5f9b4">Encoder-Decoder 编码器-解码器</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/a8b31f8a-3b38-4a19-a18b-5625ba34d1b6">百面机器学习 - 答案 Bai Mian Baimian Solution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/notes/docs/p/e087fb01-09b6-4fee-8ae0-8a70e379d71b">Definition Glossary</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3">评价模型 Model Review</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/56af319b-39c9-41e1-959a-0e470b51ffc0">Model Compare 比较模型好坏</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/notes/docs/p/97e0f4a6-4e81-49c1-8216-28e411710e65">DBSCAN</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/notes/docs/dl100">02. DL Theory 100</a><button aria-label="Expand sidebar category &#x27;02. DL Theory 100&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/notes/docs/leetcode">03. Leetcode</a><button aria-label="Expand sidebar category &#x27;03. Leetcode&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/notes/docs/zero2hero">04. Zero to Hero</a><button aria-label="Expand sidebar category &#x27;04. Zero to Hero&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/notes/docs/reinforcement-learning">Reinforcement Learning</a><button aria-label="Expand sidebar category &#x27;Reinforcement Learning&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes/docs/docs">Docs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes/docs/docs/intro">Welcome / Placeholder</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/notes/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link" href="/notes/docs/ml-general"><span>05. ML General</span></a></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Definition Glossary</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Definition Glossary</h1></header><p><a href="https://developers.google.com/machine-learning/glossary#a" target="_blank" rel="noopener noreferrer">source</a></p>
<p>webwhiteboard website: <a href="https://webwhiteboard.com/" target="_blank" rel="noopener noreferrer">https://webwhiteboard.com/</a></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="a">a<a href="#a" class="hash-link" aria-label="Direct link to a" title="Direct link to a">​</a></h2>
<ul>
<li>accuracy: The fraction of predictions that a classification model got right</li>
<li>activation function: A function (for example, ReLU or sigmoid) that takes in the weighted sum of all of the inputs from the previous layer and then generates and passes an output value (typically nonlinear) to the next layer.</li>
<li>adaboost: <a href="/notes/pages/63f233/#%E4%BE%8B%E5%AD%90-adaboost-adaptive-boost">local link</a></li>
<li>attention: Any of a wide range of <a href="https://developers.google.com/machine-learning/glossary#neural_network" target="_blank" rel="noopener noreferrer"><strong>neural network</strong></a> architecture mechanisms that aggregate information from a set of inputs in a data-dependent manner. A typical attention mechanism might consist of a weighted sum over a set of inputs, where the <a href="https://developers.google.com/machine-learning/glossary#weight" target="_blank" rel="noopener noreferrer"><strong>weight</strong></a> for each input is computed by another part of the neural network.</li>
<li>AUC (Area under the ROC Curve): An evaluation metric that considers all possible classification thresholds. The Area Under the ROC curve is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="b">b<a href="#b" class="hash-link" aria-label="Direct link to b" title="Direct link to b">​</a></h2>
<ul>
<li>backpropagation: <a href="/notes/pages/2a7490/">local link</a></li>
<li>bagging: <a href="/notes/pages/63f233/#bagging-bootstrap-aggregating-concept/">local link</a></li>
<li>bag of words:
<ul>
<li>A representation of the words in a phrase or passage, irrespective of order. For example, bag of words represents the following three phrases identically:
<ul>
<li>the dog jumps</li>
<li>jumps the dog</li>
<li>dog jumps the</li>
</ul>
</li>
<li>Each word is mapped to an index in a sparse vector, where the vector has an index for every word in the vocabulary. For example, the phrase the dog jumps is mapped into a feature vector with non-zero values at the three indices corresponding to the words the, dog, and jumps. The non-zero value can be any of the following:
<ul>
<li>A 1 to indicate the presence of a word.</li>
<li>A count of the number of times a word appears in the bag.</li>
<li>Some other value, such as the logarithm of the count of the number of times a word appears in the bag.</li>
</ul>
</li>
</ul>
</li>
<li>batch: The set of examples used in one iteration (that is, one gradient update) of model training.</li>
<li>batch normalization: <a href="/notes/pages/f405d4/">local link</a></li>
<li>batch size: The number of examples in a batch. For example, the batch size of SGD is 1, while the batch size of a mini-batch is usually between 10 and 1000.</li>
<li>boosting: <a href="/notes/pages/63f233/#boosting-concept">local link</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="c">c<a href="#c" class="hash-link" aria-label="Direct link to c" title="Direct link to c">​</a></h2>
<ul>
<li>centroid: The center of a cluster as determined by a k-means or k-median algorithm. For instance, if k is 3, then the k-means or k-median algorithm finds 3 centroids.</li>
<li>centroid-based clustering: A category of clustering algorithms that organizes data into nonhierarchical clusters. k-means is the most widely used centroid-based clustering algorithm. Contrast with hierarchical clustering algorithms.</li>
<li>collaborative filtering: Making predictions about the interests of one user based on the interests of many other users. Collaborative filtering is often used in recommendation systems.</li>
<li>confusion matrix: An NxN table that aggregates a classification model&#x27;s correct and incorrect guesses. One axis of a confusion matrix is the label that the model predicted, and the other axis is the ground truth. N represents the number of classes. For example, N=2 for a binary classification model.</li>
<li>convolutional neural network: <a href="/notes/pages/f75af9/">local link</a></li>
<li>cross-entropy: <a href="/notes/pages/710eb0/#cross-entropy-loss">local link</a></li>
<li>cross-validation: A mechanism for estimating how well a model would generalize to new data by testing the model against one or more non-overlapping data subsets withheld from the training set.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="d">d<a href="#d" class="hash-link" aria-label="Direct link to d" title="Direct link to d">​</a></h2>
<ul>
<li>decision tree: <a href="/notes/pages/05b850/">local link</a></li>
<li>discriminative model: local link</li>
<li>dropout: <a href="/notes/pages/9e1504/">local link</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="e">e<a href="#e" class="hash-link" aria-label="Direct link to e" title="Direct link to e">​</a></h2>
<ul>
<li>early stopping: A method for reducing overfit that involves ending model training before training loss finishes decreasing.</li>
<li>eigendecomposition: <a href="/notes/pages/0d8e27/#eigenvalue-decomposition">local link</a></li>
<li>embeddings:
<ul>
<li>A categorical feature represented as a continuous-valued feature. Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space. For example, you can represent the words in an English sentence in either of the following two ways:
<ul>
<li>As a million-element (high-dimensional) sparse vector in which all elements are integers.
<ul>
<li>Each cell in the vector represents a separate English word;</li>
<li>the value in a cell represents the number of times that word appears in a sentence.</li>
<li>Since a single English sentence is unlikely to contain more than 50 words, nearly every cell in the vector will contain a 0.</li>
<li>The few cells that aren&#x27;t 0 will contain a low integer (usually 1) representing the number of times that word appeared in the sentence.</li>
</ul>
</li>
<li>As a several-hundred-element (low-dimensional) dense vector in which each element holds a floating-point value between 0 and 1. This is an embedding.</li>
</ul>
</li>
<li>embeddings are trained by backpropagating loss just like any other parameter in a neural network.</li>
<li>The <a href="https://wikipedia.org/wiki/Dot_product" target="_blank" rel="noopener noreferrer">dot product</a> of two embeddings is a measure of their similarity.</li>
</ul>
</li>
<li>ensemble: A collection of models trained independently whose predictions are averaged or aggregated. In many cases, an ensemble produces better predictions than a single model. For example, a random forest is an ensemble built from multiple decision trees. Note that not all decision forests are ensembles.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="f">f<a href="#f" class="hash-link" aria-label="Direct link to f" title="Direct link to f">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="g">g<a href="#g" class="hash-link" aria-label="Direct link to g" title="Direct link to g">​</a></h2>
<ul>
<li>Gaussian Mixed Model (GMM): <a href="/notes/pages/38f1b5/">local link</a></li>
<li>generalization curve: A loss curve showing both the training set and the validation set. A generalization curve can help you detect possible overfitting.</li>
<li>generative model:
<ul>
<li>a model that does either of the following:
<ul>
<li>Creates (generates) new examples from the training dataset. e.g., create poetry after training on a dataset of poems. (e.g., GAN)</li>
<li>Determines the probability that a new example comes from the training set, or was created from the same mechanism that created the training set. e.g., after training on a dataset consisting of English sentences, a generative model could determine the probability that new input is a valid English sentence.(e.g., Naive Bayes)</li>
</ul>
</li>
<li>A generative model can understand the distribution of examples or particular features in a dataset.</li>
</ul>
</li>
<li>gradient: The vector of partial derivatives with respect to all of the independent variables. In machine learning, the gradient is the vector of partial derivatives of the model function. The gradient points in the direction of steepest ascent.</li>
<li>gradient boost: <a href="/notes/pages/049472/">local link</a></li>
<li>gradient descent: A technique to minimize loss by computing the gradients of loss with respect to the model&#x27;s parameters, conditioned on training data. In another word, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="h">h<a href="#h" class="hash-link" aria-label="Direct link to h" title="Direct link to h">​</a></h2>
<ul>
<li>hierarchical clustering: <a href="/notes/pages/4d184b/">local link</a></li>
<li>hinge loss: A family of loss functions for classification designed to find the decision boundary as distant as possible from each training example, thus maximizing the margin between examples and the boundary. SVMs use hinge loss (or a related function, such as squared hinge loss).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="i">i<a href="#i" class="hash-link" aria-label="Direct link to i" title="Direct link to i">​</a></h2>
<ul>
<li>Independent and identically distributed random variables (iid): we say a bunch of random variables is independent and identically distributed if each random variable has the same probability distribution as the others, and, between any two of them, these variables are mutually independent.
<ul>
<li>Identically Distributed means that there are no overall trends–the distribution doesn’t fluctuate and all items in the sample are taken from the same probability distribution.</li>
<li>Independent means that the sample items are all independent events. In other words, they aren’t connected to each other in any way.[2] In other words, knowledge of the value of one variable gives no information about the value of the other and vice versa.</li>
</ul>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="j">j<a href="#j" class="hash-link" aria-label="Direct link to j" title="Direct link to j">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="k">k<a href="#k" class="hash-link" aria-label="Direct link to k" title="Direct link to k">​</a></h2>
<ul>
<li>k-means:
<ul>
<li>A clustering algorithm that clusters samples by:
<ol>
<li>Iteratively determines the best k center points.</li>
<li>Assigns each sample to the closest center points. Those samples nearest the same center points belong to the same group.</li>
</ol>
</li>
<li>So, k-means algorithm picks center points to minimize the cumulative square of the distances from each sample to its closest center points</li>
</ul>
</li>
<li>knn: a classification algorithm, which classifies the new data points based on the similarity measure of the earlier stored data points.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="l">l<a href="#l" class="hash-link" aria-label="Direct link to l" title="Direct link to l">​</a></h2>
<ul>
<li>L1 regularization: A type of regularization that penalizes weights in proportion to the sum of the absolute values of the weights. In models relying on sparse features, L1 regularization helps drive the weights of irrelevant or barely relevant features to exactly 0, which removes those features from the model.</li>
<li>L2 regularization: A type of regularization that penalizes weights in proportion to the sum of the squares of the weights. L2 regularization helps drive outlier weights (those with high positive or low negative values) closer to 0 but not quite to 0. (Contrast with L1 regularization.) L2 regularization always improves generalization in linear models.</li>
<li>learning rate: A scalar used to train a model via gradient descent. During each iteration, the gradient descent algorithm multiplies the learning rate by the gradient. The resulting product is called the gradient step.</li>
<li>logistic regression: A classification model that uses a sigmoid function to convert a linear model&#x27;s raw prediction (y&#x27;) into a value between 0 and 1.</li>
<li>Long Short-Term Memory (LSTM): A type of cell in a recurrent neural network used to process sequences of data in applications such as handwriting recognition, machine translation, and image captioning. LSTMs address the vanishing gradient problem that occurs when training RNNs due to long data sequences by maintaining history in an internal memory state based on new input and context from previous cells in the RNN.</li>
<li>loss: A measure of how far a model&#x27;s predictions are from its label.</li>
<li>loss curve: A graph of loss as a function of training iterations.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="m">m<a href="#m" class="hash-link" aria-label="Direct link to m" title="Direct link to m">​</a></h2>
<ul>
<li>mini-batch: A small, randomly selected subset of the entire batch of examples run together in a single iteration of training or inference. The batch size of a mini-batch is usually between 10 and 1,000. It is much more efficient to calculate the loss on a mini-batch than on the full training data.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="n">n<a href="#n" class="hash-link" aria-label="Direct link to n" title="Direct link to n">​</a></h2>
<ul>
<li>naive bayes: <a href="/notes/pages/ee42b0/#naive-bayes-%E7%AE%97%E6%B3%95">local link</a></li>
<li>normalization: The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="o">o<a href="#o" class="hash-link" aria-label="Direct link to o" title="Direct link to o">​</a></h2>
<ul>
<li>overfitting: Creating a model that matches the training data so closely that the model fails to make correct predictions on new data.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="p">p<a href="#p" class="hash-link" aria-label="Direct link to p" title="Direct link to p">​</a></h2>
<ul>
<li>Principle Component Analysis (PCA): <a href="/notes/pages/0d8e27/">local link</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="q">q<a href="#q" class="hash-link" aria-label="Direct link to q" title="Direct link to q">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="r">r<a href="#r" class="hash-link" aria-label="Direct link to r" title="Direct link to r">​</a></h2>
<ul>
<li>random forest: <a href="/notes/pages/63f233/#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95-random-forest">local link</a></li>
<li>Rectified Linear Unit (ReLU): An activation function with the following rules:
<ul>
<li>If input is negative or zero, output is 0.</li>
<li>If input is positive, output is equal to input.</li>
</ul>
</li>
<li>recurrent neural network (RNN): A neural network that is intentionally run multiple times, where parts of each run feed into the next run. Specifically, hidden layers from the previous run provide part of the input to the same hidden layer in the next run. Recurrent neural networks are particularly useful for evaluating sequences, so that the hidden layers can learn from previous runs of the neural network on earlier parts of the sequence.</li>
<li>regularization: The penalty on a model&#x27;s complexity. Regularization helps prevent overfitting. Different kinds of regularization include:
<ul>
<li>L1 regularization</li>
<li>L2 regularization</li>
<li>dropout regularization</li>
<li>early stopping (this is not a formal regularization method, but can effectively limit overfitting)</li>
</ul>
</li>
<li>regularization rate: A scalar value, represented as lambda, specifying the relative importance of the regularization function. The following simplified loss equation shows the regularization rate&#x27;s influence:
<img decoding="async" loading="lazy" src="https://raw.githubusercontent.com/emmableu/image/master/202209292333236.png" alt="" class="img_ev3q"></li>
<li>ROC (receiver operating characteristic) Curve: A curve of true positive rate vs. false positive rate at different classification thresholds.</li>
<li>Root Mean Squared Error (RMSE): The square root of the Mean Squared Error.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="s">s<a href="#s" class="hash-link" aria-label="Direct link to s" title="Direct link to s">​</a></h2>
<ul>
<li>scaling: A commonly used practice in feature engineering to tame a feature&#x27;s range of values to match the range of other features in the dataset.</li>
<li>softmax: A function that provides probabilities for each possible class in a multi-class classification model. The probabilities add up to exactly 1.0.</li>
<li>Squared loss: The loss function used in linear regression. This function calculates the squares of the difference between a model&#x27;s predicted value for a labeled example and the actual value of the label.</li>
<li>Support Vector Machines (SVMs): A classification algorithm that seeks to maximize the margin between positive and negative classes by mapping input data vectors to a higher dimensional space.</li>
<li>sigmoid function: A function that maps logistic or multinomial regression output (log odds) to probabilities, returning a value between 0 and 1.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="t">t<a href="#t" class="hash-link" aria-label="Direct link to t" title="Direct link to t">​</a></h2>
<p>t-SNE: [local link]</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="u">u<a href="#u" class="hash-link" aria-label="Direct link to u" title="Direct link to u">​</a></h2>
<ul>
<li>underfitting: Producing a model with poor predictive ability because the model hasn&#x27;t captured the complexity of the training data.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="v">v<a href="#v" class="hash-link" aria-label="Direct link to v" title="Direct link to v">​</a></h2>
<ul>
<li>vanishing gradient problem: The tendency for the gradients of early hidden layers of some deep neural networks to become surprisingly flat (low).</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="w">w<a href="#w" class="hash-link" aria-label="Direct link to w" title="Direct link to w">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="x">x<a href="#x" class="hash-link" aria-label="Direct link to x" title="Direct link to x">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="y">y<a href="#y" class="hash-link" aria-label="Direct link to y" title="Direct link to y">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="z">z<a href="#z" class="hash-link" aria-label="Direct link to z" title="Direct link to z">​</a></h2></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/emmableu/notes/edit/main/docs/05. ML General/1001.Definition Glossary.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/notes/docs/p/a8b31f8a-3b38-4a19-a18b-5625ba34d1b6"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">百面机器学习 - 答案 Bai Mian Baimian Solution</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/notes/docs/p/70a87f0a-d0f2-4e2d-a3a5-c7eb19daa4b3"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">评价模型 Model Review</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#a" class="table-of-contents__link toc-highlight">a</a></li><li><a href="#b" class="table-of-contents__link toc-highlight">b</a></li><li><a href="#c" class="table-of-contents__link toc-highlight">c</a></li><li><a href="#d" class="table-of-contents__link toc-highlight">d</a></li><li><a href="#e" class="table-of-contents__link toc-highlight">e</a></li><li><a href="#f" class="table-of-contents__link toc-highlight">f</a></li><li><a href="#g" class="table-of-contents__link toc-highlight">g</a></li><li><a href="#h" class="table-of-contents__link toc-highlight">h</a></li><li><a href="#i" class="table-of-contents__link toc-highlight">i</a></li><li><a href="#j" class="table-of-contents__link toc-highlight">j</a></li><li><a href="#k" class="table-of-contents__link toc-highlight">k</a></li><li><a href="#l" class="table-of-contents__link toc-highlight">l</a></li><li><a href="#m" class="table-of-contents__link toc-highlight">m</a></li><li><a href="#n" class="table-of-contents__link toc-highlight">n</a></li><li><a href="#o" class="table-of-contents__link toc-highlight">o</a></li><li><a href="#p" class="table-of-contents__link toc-highlight">p</a></li><li><a href="#q" class="table-of-contents__link toc-highlight">q</a></li><li><a href="#r" class="table-of-contents__link toc-highlight">r</a></li><li><a href="#s" class="table-of-contents__link toc-highlight">s</a></li><li><a href="#t" class="table-of-contents__link toc-highlight">t</a></li><li><a href="#u" class="table-of-contents__link toc-highlight">u</a></li><li><a href="#v" class="table-of-contents__link toc-highlight">v</a></li><li><a href="#w" class="table-of-contents__link toc-highlight">w</a></li><li><a href="#x" class="table-of-contents__link toc-highlight">x</a></li><li><a href="#y" class="table-of-contents__link toc-highlight">y</a></li><li><a href="#z" class="table-of-contents__link toc-highlight">z</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/notes/docs/intro">Tutorial</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://x.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">X<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/notes/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/emmableu/notes" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>